{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"jax2onnx \ud83c\udf1f","text":"<p><code>jax2onnx</code> converts your JAX, Flax NNX, Flax Linen, Equinox functions directly into the ONNX format.</p> <p></p>"},{"location":"#key-features","title":"\u2728 Key Features","text":"<ul> <li>Simple API \u2013 Convert JAX callables using <code>to_onnx(...)</code>.</li> <li>Model structure preserved \u2013 With <code>@onnx_function</code>, submodules appear as named functions in the ONNX graph.</li> <li>Dynamic input support \u2013 Use abstract dimensions like <code>'B'</code> or pass scalars as runtime inputs.</li> <li>Plugin-based extensibility \u2013 Add support for new primitives by writing small, local plugins.</li> <li>onnx-ir native pipeline \u2013 Conversion, optimization, and post-processing all run on the typed <code>onnx_ir</code> toolkit.</li> <li>Netron-friendly outputs \u2013 Generated graphs carry shape/type annotations and a clean hierarchy.</li> </ul>"},{"location":"#getting-started","title":"\ud83d\ude80 Getting Started","text":"<p>Check out the Getting Started guide to install and export your first model in minutes.</p>"},{"location":"#current-version","title":"\ud83d\udce6 Current Version","text":"<p>v0.11.1 \u2013 MaxText example stack covering DeepSeek, Gemma, GPT-3, Kimi, Llama, Mistral, and Qwen families.</p> <p>See Roadmap for planned features and Past Versions for the full release archive.</p> <p>Happy converting! \ud83c\udf89</p>"},{"location":"about/acknowledgements/","title":"Acknowledgements","text":""},{"location":"about/acknowledgements/#special-thanks","title":"Special Thanks","text":"<p>\u2728 @clementpoiret for initiating Equinox support and for Equimo, which brings modern vision models\u2014such as DINOv3\u2014to JAX/Equinox.</p> <p>\u2728 @justinchuby for introducing onnx-ir as a scalable and more efficient way to handle ONNX model construction.  </p> <p>\u2728 @atveit for introducing us to gpt-oss-jax-vs-torch-numerical-comparison.</p> <p>\u2728 @benmacadam64 for championing the complex-number handling initiative.</p> <p>\u2728 @lutzroeder for making shapes internal to ONNX function visible in his great Netron viewer. - ONNX: Function value_info support #1447</p> <p>\u2728 tumaer/JAXFLUIDS for contributing valuable insights rooted in physics simulation use cases.</p> <p>\u2728 @limarta for the elegant jaxpr-to-ONNX demonstration that inspired this project.</p>"},{"location":"about/acknowledgements/#contributors","title":"Contributors","text":"<p>Example contributions: @burakssen, @Cadynum, @clementpoiret, @PVirie</p> <p>Plugin contributions: @burakssen, @clementpoiret, @Clouder0, @rakadam, @benmacadam64</p>"},{"location":"about/acknowledgements/#community","title":"Community","text":"<p>Thanks to the community members involved in:</p> <ul> <li>Flax Feature Request #4430</li> <li>JAX Feature Request #26430</li> </ul>"},{"location":"about/contributing/","title":"Contributing","text":"<p>We warmly welcome contributions!</p>"},{"location":"about/contributing/#how-you-can-help","title":"How You Can Help","text":"<ul> <li> <p>Add a plugin: Extend <code>jax2onnx</code> by writing a simple Python file in <code>jax2onnx/plugins</code>:   a primitive or an example. The Plugin Quickstart walks through the process step-by-step.</p> </li> <li> <p>Bug fixes &amp; improvements: PRs and issues are always welcome on GitHub.</p> </li> </ul>"},{"location":"about/contributing/#getting-started","title":"Getting Started","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch</li> <li>Make your changes</li> <li>Run tests: <code>poetry run pytest</code></li> <li>Submit a pull request</li> </ol> <p>See the Plugin System documentation for details on adding new operators.</p>"},{"location":"about/dependencies/","title":"Dependencies","text":"<p>Latest supported versions of major dependencies:</p> Library Version <code>JAX</code> 0.8.2 <code>Flax</code> 0.12.2 <code>Equinox</code> 0.13.2 <code>onnx-ir</code> 0.1.13 <code>onnx</code> 1.20.0 <code>onnxruntime</code> 1.23.2 <p>For exact pins and extras, see <code>pyproject.toml</code>.</p>"},{"location":"about/license/","title":"License","text":"<p>This project is licensed under the Apache License, Version 2.0.</p> <p>See the full license text at <code>LICENSE</code>.</p>"},{"location":"about/past_versions/","title":"Past Versions","text":"<ul> <li>0.11.0: Initial Flax Linen support: core layers, activation coverage, attention stack, recurrent stack, and Linen examples (MLP/CNN/Sequential); modernized IR optimization pipeline: standard <code>onnx_ir</code> CSE pass adoption, removed legacy helpers/getattr patterns, and simplified tests with direct graph iteration.</li> <li>0.10.4: Fixed <code>vmap</code> batching for <code>jax.numpy.reshape</code>/<code>transpose</code> and other <code>jax.numpy</code> primitives; refactored the IR optimizer to use <code>onnx-ir</code> public APIs (<code>value.consumers()</code>, <code>graph.remove()</code>); added Common Subexpression Elimination (CSE) and Constant Lifting passes to <code>ir_optimizations.py</code>; added GitHub Actions CI for automated testing.</li> <li>0.10.3: Added a Flax/NNX DINOv3 VisionTransformer example stack (<code>plugins/examples/nnx/dinov3.py</code>) with deterministic rng helpers, rotary cache capture, and expect_graph coverage across ViT variants; introduced Equinox\u2192NNX parity testing for DINOv3 (weight copy + forward check) to keep the Equinox and Flax paths aligned; example registry keys are now context-aware (<code>context::component</code>) with override warnings to avoid collisions between example stacks; documented NNX DINO exports (static/dynamic batch) and kept generated ONNX artifacts out of git via a dedicated <code>.gitignore</code>.</li> <li>0.10.2: Added the GPT-OSS export stack (Equinox + Flax/NNX reference modules, parity harnesses, exporter scripts, docs); new primitive coverage for <code>lax.top_k</code>, <code>lax.rsqrt</code>, and Equinox <code>RMSNorm</code>; masked softmax lowers <code>where</code>-masked calls to Softmax + Where while zeroing masked positions; scatter ops in <code>cond</code>/<code>scan</code> now preserve ONNX-compliant initializers/types via refreshed index helpers (fixing Issue #139); strengthened symbolic-dimension handling via DimExpr/shape-polynomial helpers to stabilize broadcast/loop/gather shapes; IR return-mode/input_param materialization fixed and legacy <code>serde_onnx</code> removed for deterministic IR-only outputs; tightened typing with shared <code>typing_support</code> protocols and helper scripts (<code>check_typing.sh</code>, <code>report_rng_traces.py</code>); dependency stack bumped to JAX 0.8.1 / Flax 0.12.1 with matching NNX plugin updates.</li> <li>0.10.1: Introduced complex-number support with a unified packed layout (<code>[..., 2]</code>) and broad plugin coverage (elementwise, conjugation, dot_general/matmul/conv, FFT via DFT); added stacktrace metadata toggles (<code>pkg.jax2onnx.callsite</code> / <code>pkg.jax2onnx.plugin</code>); enabled an Einsum fallback for <code>lax.dot_general</code>; tightened <code>lax.broadcast_in_dim</code> determinism (always emit Expand, preserve loop extent metadata); and rebuilt <code>lax.reduce_window_sum</code> using a Conv-based lowering covering strides, dilation, integer operands, and static base dilation.</li> <li>0.10.0: Expanded Equinox DINO exporter (new <code>equinox/eqx/nn</code> plugins and example), introduced shared lowering helpers (<code>_axis0_utils</code>, <code>_loop_extent_meta</code>, <code>jax/lax/gather_compile</code>, <code>jax/lax/gather_helpers</code>, <code>jax/image/resize</code>, <code>jax/numpy/outer</code>), refreshed control-flow and scatter/gather implementations, added <code>@onnx_function(unique=True)</code>, refactored the IR builder (<code>clone_graph</code>, live proxies), and bumped dependencies to JAX 0.8.0 / onnx-ir 0.1.11.</li> <li>0.9.0 migrates from the ONNX proto builder to <code>onnx_ir</code>, adds a <code>return_mode</code> (<code>proto</code> / <code>ir</code> / <code>file</code>), and updates dependencies to JAX 0.7.2, Flax 0.12.0, Equinox 0.13.2, onnx-ir 0.1.10, and onnx 1.19.1.</li> <li>0.8.1 adds N-D <code>lax.dynamic_update_slice</code> with negative-index handling, sharpens grad/VJP paths for <code>jnp.cumsum</code>, introduces <code>lax.add_any</code> and the <code>lax.pow</code>/<code>jnp.power</code>/<code>jnp.pow</code> family with improved <code>lax.scan</code> dtype propagation, and supports <code>eqx.nn.Linear(use_bias=False)</code>.</li> <li>0.8.0 adds initial Equinox support (<code>eqx.dropout</code>, <code>eqx.layer_norm</code>, <code>eqx.linear</code>, plus an <code>MlpExample</code>), stabilizes SSA/shape handling across <code>lax.scan</code> and <code>lax.fori_loop</code> to prevent dtype leaks, improves dtype propagation in <code>lax.gather</code> and <code>lax.concatenate</code>, and adds plugin support for <code>lax.pad</code>.</li> <li>0.7.5 fixes tests for functions without arguments, adds support for <code>lax.bitwise_not</code>, <code>lax.clamp</code>, <code>lax.ge</code>, <code>jnp.clip</code>, <code>lax.rev</code>, and enhances support for <code>nnx.dot_product_attention</code>, <code>nnx.conv</code>, <code>nnx.batch_norm</code>, <code>lax.mul</code>, <code>lax.reduce_max</code>, <code>lax.scan</code>, <code>lax.slice</code>, <code>lax.while_loop</code>, <code>nn.gelu</code>, <code>jnp.arange</code>, <code>jnp.cumsum</code>, <code>jnp.select</code>, <code>jnp.where</code>, and <code>jnp.concatenate</code>.</li> <li>0.7.4 adds support for <code>lax.cumsum</code> and <code>jnp.cumsum</code>, and improves <code>lax.scatter</code>.</li> <li>0.7.3 improves polymorphism handling for transformers.</li> <li>0.7.2 adds support for <code>jnp.split</code>, <code>lax.split</code>, <code>lax.logistic</code>, includes an example for <code>nnx.GRUCell</code>, and improves <code>lax.scatter</code> and <code>lax.while_loop</code>.</li> <li>0.7.1 fixes a numeric equivalence bug in the test system, and adds support for <code>core.custom_jvp_generic</code>, <code>eqx.identity</code>, <code>jnp.select</code>, <code>jnp.stack</code>, <code>jnp.unstack</code>, <code>lax.select</code>, plus multiple <code>nn.*</code> activations (<code>identity</code>, <code>celu</code>, <code>elu</code>, <code>gelu</code>, <code>relu</code>, <code>leaky_relu</code>, <code>mish</code>, <code>selu</code>, <code>sigmoid</code>, <code>soft-sign</code>, <code>softmax</code>, <code>truncated_normal</code>).</li> <li>0.7.0 introduces a GPT-2 example based on nanoGPT with ONNX function support and attention masking, adds support for <code>jnp.concatenate</code>, <code>jnp.take</code>, <code>nnx.Embed</code>, and starts hosting ONNX models on Hugging Face.</li> <li>0.6.5 improves support for <code>nnx.batch_norm</code>, <code>nnx.group_norm</code>, <code>nnx.layer_norm</code>, <code>nnx.rms_norm</code>, <code>lax.broadcast_in_dim</code>, <code>lax.cond</code>, <code>lax.fori_loop</code>, <code>lax.integer_pow</code>, <code>lax.scan</code>, <code>lax.scatter</code>, <code>lax.scatter_add</code>, <code>lax.scatter_mul</code>, and <code>lax.while_loop</code>; and adds support for <code>lax.and</code>, <code>lax.rem</code>, and <code>lax.remat2</code>.</li> <li>0.6.4 improves support for <code>lax.scatter_mul</code>.</li> <li>0.6.3 applies double-precision fixes for <code>lax.fori_loop</code> and <code>lax.while_loop</code>, and fixes bugs in <code>lax.scan</code> and <code>jnp.where</code>.</li> <li>0.6.2 fixes bugs in <code>nnx.conv</code> and <code>lax.reshape</code>, and adds the <code>jnp.prod</code> primitive.</li> <li>0.6.1 improves support for <code>lax.cond</code> and <code>lax.select_n</code>, introduces new primitives (<code>lax.reduce_and</code>, <code>lax.reduce_or</code>, <code>lax.reduce_prod</code>, <code>lax.reduce_xor</code>), and adds examples for <code>jnp.select</code> and <code>jnp.sort</code>.</li> <li>0.6.0 introduces the <code>enable_double_precision</code> parameter (default <code>False</code>) to support physics simulations and enhances <code>lax.scatter</code> handling.</li> <li>0.5.2 adds support for <code>jnp.where</code>, <code>jnp.arange</code>, and <code>jnp.linspace</code>.</li> <li>0.5.1 expands subgraph coverage for <code>lax.while_loop</code>, <code>lax.cond</code>, <code>lax.fori_loop</code>, and <code>lax.scan</code>.</li> <li>0.5.0 improves dynamic batch handling via shape polymorphism and adds <code>jnp.sign</code>, <code>jnp.abs</code>, and <code>jnp.iota</code>.</li> <li>0.4.4 adds <code>lax.cos</code>, <code>lax.cosh</code>, <code>lax.sin</code>, <code>lax.sinh</code>, and <code>lax.scatter</code> support.</li> <li>0.4.3 fixes ONNX validation for JAX callable outputs and cleans up newly exposed tests.</li> <li>0.4.2 cleans up and fixes the initial ONNX function release.</li> <li>0.4.1 introduces ONNX functions via the <code>@onnx_function</code> decorator, creating function instances directly in the call graph.</li> <li>0.3.2 relaxes the minimum Python version to 3.10.</li> <li>0.3.0 streamlines plugin registration and custom primitive integration.</li> <li>0.2.0 (first PyPI release) rebases on <code>jaxpr</code>, improving usability and adding low-level <code>lax</code> components.</li> <li>0.1.0 (initial approach, not released to PyPI) exports early <code>nnx</code> components and examples including a vision transformer.</li> </ul>"},{"location":"about/roadmap/","title":"Roadmap","text":""},{"location":"about/roadmap/#planned","title":"Planned","text":"<ul> <li>Broaden coverage of JAX, Flax NNX/Linen, and Equinox components.</li> <li>Expand SotA example support for vision and language models.</li> <li>Improve support for physics-based simulations.</li> <li>Support for MaxDiffusion.</li> </ul>"},{"location":"about/roadmap/#current-version","title":"Current Version","text":""},{"location":"about/roadmap/#jax2onnx-0111-maxtext-model-family-coverage-cleaner-exported-graphs","title":"jax2onnx 0.11.1 \u2013 MaxText model family coverage &amp; cleaner exported graphs","text":"<ul> <li> <p>Comprehensive MaxText example stack:   Added a fully comprehensive MaxText example + test suite covering exports for DeepSeek, Gemma, GPT-3, Kimi, Llama, Mistral, and Qwen model families.</p> </li> <li> <p>MaxText stubs &amp; new primitive coverage:   Introduced MaxText dependency stubs and implemented new primitive support required to enable those exports end-to-end.</p> </li> <li> <p>Cleaner ONNX graphs via stricter subgraph cleanup:   Tightened subgraph cleanup to produce cleaner, more minimal ONNX graphs (less leftover/unused substructure after export).</p> </li> </ul>"},{"location":"about/roadmap/#past-versions","title":"Past Versions","text":"<p>See Past Versions for the full release archive.</p>"},{"location":"developer_guide/architecture/","title":"Architecture Overview","text":"<p>jax2onnx is a library for converting JAX functions and Flax modules to the ONNX format. It enables deployment of JAX-based models to any runtime that supports ONNX, such as ONNX Runtime, TensorRT, or CoreML.</p> <p>The library is designed around two core principles:</p> <ol> <li>Plugin-based extensibility: All operator-specific logic lives in plugins. The core converter knows nothing about specific operations like convolutions or attention\u2014it only orchestrates the conversion process.</li> <li>Minimal core, maximal flexibility: The converter is a thin, generic engine that traces JAX programs and delegates lowering to plugins, keeping the architecture clean and maintainable.</li> </ol>"},{"location":"developer_guide/architecture/#big-idea","title":"Big idea","text":"<p>The converter is a tiny, generic JAXPR \u2192 IR engine. It knows nothing about NNX, Conv, Pool, or any specific op. Its only job is:</p> <ol> <li>Discover plugins (inversion of control),</li> <li>Activate whatever they declare (monkey-patching to produce crisp primitives),</li> <li>Trace your function to a ClosedJaxpr,</li> <li>Lower each equation by handing it to a plugin that claimed that primitive,</li> <li>Assemble an IR graph,</li> <li>Optimize the IR graph with a small, safe, plugin-agnostic pass,</li> <li>Finalize a valid ONNX model (stamp shapes/dtypes, prune, serialize).</li> </ol> <p>Everything op-specific \u2014 layouts, padding math, attribute shapes, NHWC\u2194NCHW, etc. \u2014 stays in plugins.</p>"},{"location":"developer_guide/architecture/#related-documentation","title":"Related documentation","text":"<ul> <li><code>advanced_topics/onnx_ir_builder.md</code> \u2013 canonical builder guardrails and examples.</li> <li><code>advanced_topics/expect_graph_reference.md</code> \u2013 structural test patterns for <code>expect_graph</code>.</li> <li><code>advanced_topics/subgraph_input_handling.md</code> \u2013 control-flow body wiring (If/Loop/Scan).</li> <li><code>../user_guide/supported_components.md</code> \u2013 autogenerated support matrices for primitives/examples.</li> <li><code>../about/past_versions.md</code> \u2013 changelog snapshots for each jax2onnx release.</li> </ul>"},{"location":"developer_guide/architecture/#roles-responsibilities","title":"Roles &amp; responsibilities","text":""},{"location":"developer_guide/architecture/#core-plugin-agnostic","title":"Core (plugin-agnostic)","text":"<ul> <li>Plugin discovery. Recursively import <code>plugins/*</code>. Plugins self-register into a registry keyed by primitive name (string). The core never sees concrete classes like <code>nnx.Conv</code>.</li> <li>Activation window. Core enters a context that applies whatever patches plugins declare. This context wraps tracing so patched high-level calls (e.g., <code>nnx.Conv.__call__</code>) emit the right primitive names. No allowlists; no special-cases.</li> <li>Tracing. <code>make_jaxpr(fn)(*shape_specs)</code> yields a ClosedJaxpr: <code>(constvars, invars, eqns, outvars)</code>.</li> <li> <p>IR assembly. Walk equations in order; for each equation:</p> </li> <li> <p>Look up <code>PLUGIN_REGISTRY[eqn.primitive.name]</code>.</p> </li> <li>Give it the equation and a lowering context; it emits IR nodes/values.</li> <li>Assert that every <code>eqn.outvars[i]</code> is bound to an IR value before moving on (generic guardrail).</li> <li>Converters/plugins emit new ops through <code>ctx.builder</code> so constants and <code>_outputs</code> stay consistent across ONNX IR variants (see ONNX IR Builder Guide).</li> <li>IR optimization (safe, structure-only). Run small, local rewrites that don\u2019t encode op semantics, e.g. folding redundant layout ping-pongs (see below).</li> <li>Finalize. Add model inputs/outputs, stamp symbolic dim labels (e.g. <code>\"B\"</code>), prune dead nodes/initializers, serialize <code>ModelProto</code>.</li> </ul>"},{"location":"developer_guide/architecture/#plugin-op-specific","title":"Plugin (op-specific)","text":"<p>Each plugin describes one primitive (or one high-level function). It has three standard pieces:</p> <ul> <li> <p>Binding specs (monkey-patching). \u201cWhen a user calls X, bind primitive named P.\u201d   Example: patch <code>flax.nnx.Conv.__call__</code> so the traced program contains <code>primitive.name == \"nnx.conv\"</code>. If NNX exposes multiple symbols, the plugin lists them all. The core just applies what\u2019s declared.</p> </li> <li> <p>Abstract eval (shape/dtype). Given JAX abstract values (<code>ShapedArray</code>), return the result\u2019s abstract value (or tuple). No real compute; just shape math (use <code>lax.*</code> if helpful). This is used by JAX during tracing.</p> </li> <li> <p>Lowering (IR emission). Given a <code>LoweringContext</code> and the equation:</p> </li> <li> <p>Pull IR inputs via <code>ctx.get_value_for_var(eqn.invars[i])</code>.</p> </li> <li>Create IR nodes (Conv, Transpose, Reshape, \u2026).</li> <li>Produce IR outputs and bind them to <code>eqn.outvars[i]</code> via <code>ctx.bind_value_for_var(...)</code>.</li> <li>Return nothing (binding suffices) or return the produced values (the core will bind any unbound outvars generically).</li> </ul> <p>That\u2019s it. The contract is tiny and uniform across all primitives.</p>"},{"location":"developer_guide/architecture/#plugin-guardrails-must-follow","title":"Plugin guardrails (must-follow)","text":"<p>To keep conversions portable across <code>onnx_ir</code> variants, every plugin lowering must observe these project-wide rules:</p> <ul> <li>Builder-first: emit ops via <code>ctx.builder</code> (see ONNX IR Builder Guide). <code>_outputs</code> must always be a sequence; constants come from <code>builder.initializer(...)</code> or <code>ctx.bind_const_for_var(...)</code>.</li> <li>Metadata stamping: after every builder call, stamp dtype/shape on the produced value and run <code>_ensure_value_metadata(...)</code> to normalize the <code>ir.Value</code> metadata (there is no separate <code>value_info</code> registry).   Legacy reminder: the converter removed <code>builder.value_info</code>; all shape/type metadata must travel with the values themselves.</li> <li>Single-use RNG / module construction: never seed at import time. Expose stochastic callables with <code>construct_and_call(...).with_rng_seed(...)</code> / <code>.with_requested_dtype(...)</code> so the test harness can rebuild modules for both f32/f64 variants without clashes (see <code>AGENTS.md</code>).</li> <li>No protobuf in converters/plugins: only the top-level adapters touch <code>onnx</code> (protobuf types). Policy tests under <code>tests/extra_tests</code> enforce this.</li> </ul>"},{"location":"developer_guide/architecture/#data-flow-end-to-end","title":"Data flow end-to-end","text":"<pre><code>User fn + shape specs\n       \u2502\n       \u25bc\n[Activation Context]  \u2190\u2014 plugins declare patches; core applies them (no names)\n       \u2502\n       \u25bc\nClosedJaxpr = make_jaxpr(fn)(*specs)\n       \u2502\n       \u251c\u2500\u2500 constvars  \u2192 ctx.bind_const_for_var(...)\n       \u251c\u2500\u2500 invars     \u2192 ctx.add_input_for_invar(...)\n       \u2502\n       \u2514\u2500\u2500 eqns:  e\u2080, e\u2081, \u2026, e\u2099\n                  \u2502\n                  \u251c\u2500 core reads e\u1d62.primitive.name (string)\n                  \u251c\u2500 plugin = REGISTRY[name]\n                  \u251c\u2500 plugin.lower(ctx, e\u1d62)   \u2190 emits IR nodes\n                  \u2514\u2500 core asserts e\u1d62.outvars all bound\n       \u2502\n       \u2514\u2500\u2500 outvars    \u2192 ctx.add_outputs_from_vars(...)\n       \u2502\n       \u25bc\nIR graph \u2192 **IR optimizer** \u2192 stamp shapes/symbols \u2192 prune \u2192 ONNX ModelProto\n</code></pre> <p>No step above references \u201cConv\u201d, \u201cTanh\u201d, or any specific op in the core. All knowledge sits behind the primitive name string chosen by the plugin.</p>"},{"location":"developer_guide/architecture/#conversion-pipeline-detailed","title":"Conversion pipeline (detailed)","text":"<p>The full conversion pipeline spans two modules: <code>conversion_api.py</code> (core conversion) and <code>user_interface.py</code> (export facade). The following table shows the exact order:</p> Step Module Location Purpose 1 <code>conversion_api</code> <code>to_onnx</code> Build raw IR: trace JAXPR, lower equations to nodes 2 <code>conversion_api</code> <code>optimize_graph</code> Structural optimization: dead node removal, CSE, constant lifting, reshape folding 3 <code>conversion_api</code> Late overrides Apply user attribute patches to surviving nodes; fix Concat axis 4 <code>conversion_api</code> <code>run_optional_shape_inference</code> (Reserved for future shape inference; currently no-op) 5 <code>conversion_api</code> <code>_finalize_model_value_shapes</code> Normalize symbolic dims to <code>ir.SymbolicDim</code> objects 6 <code>conversion_api</code> Return Model has precise shapes preserved 7 <code>user_interface</code> <code>postprocess_ir_model</code> Shape loosening: replace intermediate value shapes with dynamic dims for ORT flexibility 8 <code>user_interface</code> <code>_materialize_input_params_on_ir</code> Expose <code>input_params</code> as explicit graph inputs 9 <code>user_interface</code> Serialize Convert to proto / save to file"},{"location":"developer_guide/architecture/#why-this-order","title":"Why this order?","text":"<ol> <li>Optimize before patching: Dead node removal runs first so we don't waste time patching nodes that will be deleted.</li> <li>Finalize before loosening: <code>conversion_api</code> normalizes shapes while they are precise. Loosening (Step 7) is intentionally AFTER to preserve accuracy for shape inference and finalization.</li> <li>Loosening is export-only: <code>postprocess_ir_model</code> is called only by the user-facing <code>to_onnx</code> function, not by internal pipelines.</li> </ol>"},{"location":"developer_guide/architecture/#module-responsibilities","title":"Module responsibilities","text":"Module Responsibility <code>ir_optimizations.py</code> Pure optimization passes (DCE, CSE, constant lifting, reshape folding) <code>conversion_api.py</code> Core conversion + optimization + finalization (returns precise-shape model) <code>ir_postprocess.py</code> Export preparation: shape loosening for runtime flexibility <code>user_interface.py</code> Public API facade: orchestrates conversion \u2192 postprocess \u2192 serialize"},{"location":"developer_guide/architecture/#the-lowering-context-what-plugins-see","title":"The lowering context (what plugins see)","text":"<p>A small, stable API:</p> <ul> <li> <p><code>get_value_for_var(var, *, name_hint=None) -&gt; IRValue</code>   Materialize (or retrieve) the IR value corresponding to a JAX var (const/invar/intermediate). Handles literals by creating constant initializers.</p> </li> <li> <p><code>bind_value_for_var(var, value: IRValue)</code>   Declare that this IR value is the output of <code>var</code> (an equation outvar). This is the only binding contract the core depends on.</p> </li> <li> <p>Minimal utilities the plugin can rely on (implemented once in the core):</p> </li> <li> <p>Name generator (<code>fresh_name</code>),</p> </li> <li>Helpers for constants and attributes,</li> <li>Optionally a couple of generic IR helpers (<code>emit_node</code>, tiny wrappers for Shape/Gather/Unsqueeze where dynamic dims are needed).</li> </ul>"},{"location":"developer_guide/architecture/#output-binding-pattern-must-follow","title":"Output binding pattern (must follow)","text":"<p>When lowering, always reuse the IRValue pre-allocated for each equation outvar. The canonical flow is:</p> <ol> <li>Fetch inputs via <code>ctx.get_value_for_var(eqn.invars[i])</code>.</li> <li>Pre-allocate outputs with <code>ctx.get_value_for_var(eqn.outvars[i])</code>.</li> <li>Emit nodes whose <code>outputs=[...]</code> point to those pre-allocated values (write final results directly into them).</li> <li>Optionally stamp dtype/shape metadata on the same values.</li> </ol> <p>Avoid producing temporary outputs and \u201cattaching\u201d them afterwards; that pattern bypasses the var\u2192value map and leads to orphaned tensors. Keeping the contract tight here means downstream equations always receive the correct tensor without extra bookkeeping.</p>"},{"location":"developer_guide/architecture/#builder-vs-tape","title":"Builder vs. Tape","text":"<p>Every lowering runs against the same IR backend but plugins are expected to work through <code>ctx.builder</code> whenever possible:</p> <ul> <li>The builder records nodes/initializers in one place and mirrors <code>_tape.Builder</code>   semantics across converters, plugins, and tests. The policy suite (<code>tests/extra_tests/framework/test_ir_builder_contracts.py</code>   and <code>scripts/check_ir_builder_usage.py</code>) assumes <code>_outputs</code>/initializer calls flow through   the builder helpers.</li> <li>Drop down to <code>onnx_ir.tape.Tape</code> (or construct <code>ir.Node</code> manually) only when you need features   the builder cannot express\u2014overload selection, custom output reuse, or metadata props. When you do,   restore dtype/shape metadata manually and write back to all graph mirrors (<code>graph.nodes</code>, <code>_nodes</code>, etc.).</li> <li>The ONNX IR Builder Guide collects the concrete guardrails and   example snippets. Treat it as the source of truth for <code>_outputs</code>, initializer naming, and RNG/dtype   conventions referenced by the policy tests.</li> </ul>"},{"location":"developer_guide/architecture/#functions-converter-owned-call-boundaries","title":"Functions (converter-owned call boundaries)","text":"<p>Decorators such as <code>@onnx_function</code> register a plugin that lowers the call into a FunctionScope. At runtime the handler:</p> <ol> <li>Builds a <code>FunctionKey</code> from the qualified target name, input aval signature,    and capture signature (class instance id/config).</li> <li>Reuses an existing definition if the key is cached; otherwise it opens a    <code>FunctionScope</code>, maps parent inputs to fresh function inputs, and recursively    lowers the original callable inside the child IRContext (constants are emitted    as <code>Constant</code> nodes because FunctionProto bodies cannot own initializers).</li> <li>Seals the scope into an <code>onnx_ir.Function</code>, records any attribute overrides,    and caches the result.</li> <li>Emits a call-site node in the parent graph with <code>domain/op_type</code> matching the    function and wires the original in/out values to that node.</li> </ol> <p>Because the definition is keyed on avals and capture signature, identical calls share a single function body, while shape/config changes trigger new entries.</p>"},{"location":"developer_guide/architecture/#shapes-symbolic-dims","title":"Shapes &amp; symbolic dims","text":"<ul> <li>Inputs. If the user gives symbolic strings (e.g., <code>\"B\"</code>), the core creates JAX symbolic dims so the jaxpr records symbols instead of numbers.</li> <li>Abstract eval. Preserve <code>_DimExpr</code> symbols\u2014call <code>jax.eval_shape</code> on the   original callable (with <code>ShapeDtypeStruct</code> inputs) instead of doing manual   shape math. Never cast symbolic dims to ints.</li> <li> <p>Dynamic shapes in IR. When an IR op needs runtime sizes (e.g., a flatten), plugins use:</p> </li> <li> <p><code>Shape(x)</code> \u2192 shape vector,</p> </li> <li><code>Gather(shape, axis=i)</code> \u2192 ith dimension,</li> <li><code>Unsqueeze/Concat</code> \u2192 assemble a runtime shape tensor,</li> <li><code>Reshape(x, shape_tensor)</code>.</li> <li>Output stamping. After lowering, the core restamps inputs/outputs so symbolic labels survive through ONNX <code>ValueInfo</code> (only where no concrete size is present). <code>IRContext</code> tracks the origin tensor/axis for each symbol so helpers like <code>dim_as_value</code> can materialize runtime shapes via <code>Shape \u2192 Gather \u2192 Squeeze</code>.</li> </ul>"},{"location":"developer_guide/architecture/#determinism-graph-hygiene","title":"Determinism &amp; graph hygiene","text":"<ul> <li>Deterministic names. The core\u2019s <code>fresh_name</code> yields deterministic per-graph names; initializers keep stable names based on plugin hints (when feasible).</li> <li>Single-node policy. If a plugin needs <code>Reshape</code>, it emits one <code>Reshape</code> and a single constant shape initializer if static; it avoids const-only <code>Concat</code>.</li> <li>Pruning. A simple backwards mark from graph outputs removes dead nodes and unused initializers. This keeps <code>match=\"exact\"</code> tests strict.</li> <li>No dangling inputs. The core asserts every outvar is bound; graph is built in jaxpr order so edges are naturally well-formed.</li> </ul>"},{"location":"developer_guide/architecture/#graph-structure-specs","title":"Graph structure specs","text":"<ul> <li>Post-conversion graph checks live directly in metadata. Use <code>expect_graph([...], \u2026)</code> so the contract is visible next to each testcase.</li> <li>When the lowered structure changes, run <code>poetry run python scripts/emit_expect_graph.py &lt;testcase&gt;</code> to regenerate a canonical snippet; paste the result into the plugin metadata.</li> <li>The helper relies on <code>auto_expect_graph_spec</code> internally, so it always reflects the current ONNX graph without persisting extra fixtures.</li> </ul>"},{"location":"developer_guide/architecture/#ir-optimizer-plugin-agnostic","title":"IR optimizer (plugin-agnostic)","text":"<p>Before serialization we run a tiny, structure-only optimization sweep. The canonical rules and implementation notes live in <code>advanced_topics/ir_optimizer.md</code>. Today the only pass folds redundant <code>Transpose \u2192 [elementwise]* \u2192 Transpose</code> pairs when their permutations compose to identity; future passes must follow the same IR-only, backend-agnostic constraints.</p>"},{"location":"developer_guide/architecture/#testing-expectations-how-exact-works","title":"Testing expectations (how \u201cexact\u201d works)","text":"<ul> <li>Anchored path checks. Tests can say:   <code>Transpose \u2192 Conv \u2192 Relu \u2192 AveragePool \u2192 \u2026</code>   With <code>match=\"exact\"</code>, the test fails if required ops are missing or extra ops are present between anchors.</li> <li>CNN sentinel. The CNN static test is a canary: if Conv doesn\u2019t lower, flatten will see <code>{B,14,14,1}</code> and fail a later <code>Reshape(B,3136)</code>. With the optimizer, extra <code>Transpose\u2026Transpose</code> pairs around Relu are eliminated; pairs around AveragePool remain (by design).</li> </ul> <p>See <code>advanced_topics/expect_graph_reference.md</code> for a focused reference on writing <code>expect_graph</code> checks (shapes, counts, symbols, and helper flags).</p>"},{"location":"developer_guide/architecture/#typical-plugin-lifecycle-concrete-but-generic","title":"Typical plugin lifecycle (concrete but generic)","text":"<ol> <li> <p>Register <code>@register_primitive(jaxpr_primitive=\"\u2026\")</code> puts an instance in the registry under that string key. The core will later match that key with <code>eqn.primitive.name</code>.</p> </li> <li> <p>Patch <code>binding_specs()</code> returns <code>MonkeyPatchSpec</code>s: \u201creplace <code>module.symbol</code> with <code>prim.bind(\u2026)</code> shim\u201d. If there are multiple aliases, the plugin lists them. The core just applies them all.</p> </li> <li> <p>Abstract eval <code>def abstract_eval(*avals, **params):</code> returns a <code>ShapedArray</code> (or tuple) describing the outputs. Use <code>jax.eval_shape</code> on <code>lax.*</code> helpers if that\u2019s easier, but never call the patched function (it would recurse).</p> </li> <li> <p>Lower <code>def lower(ctx, eqn):</code></p> </li> <li> <p><code>x = ctx.get_value_for_var(eqn.invars[0])</code></p> </li> <li>Create IR nodes (e.g., Transpose, Conv, CastLike, \u2026),</li> <li><code>ctx.bind_value_for_var(eqn.outvars[0], y)</code></li> </ol> <p>That\u2019s the whole contract.</p>"},{"location":"developer_guide/architecture/#failure-modes-how-the-architecture-contains-them","title":"Failure modes &amp; how the architecture contains them","text":"<ul> <li> <p>Patch activation window too late. If activation doesn\u2019t wrap tracing, the jaxpr will never contain the plugin\u2019s primitive names. The core still doesn\u2019t special-case anything; you just see \u201cno plugin for primitive \u2018foo\u2019\u201d. Fix = activate around <code>make_jaxpr</code>.</p> </li> <li> <p>Plugin forgets to bind the output. Then the core\u2019s generic guardrail catches it and fails the build at the exact primitive, without central knowledge of op names.</p> </li> <li> <p>Multiple symbols for the same high-level op. Plugins add multiple patch specs. The core applies them all \u2014 still no names or allow-lists in the core.</p> </li> <li> <p>An unfinished plugin gets imported. If it also patches a runtime path it can trip tracing/lowering. Fix in the plugin: either complete it or don\u2019t patch until ready. The core does not and should not maintain an allow/deny list.</p> </li> </ul>"},{"location":"developer_guide/architecture/#architectural-guarantees","title":"Architectural Guarantees","text":"<ul> <li> <p>Inversion of control. The only dynamic choice the core makes is:   \u201cGiven <code>eqn.primitive.name</code> (a string), ask the registry for a handler.\u201d   There is zero knowledge of concrete ops or frameworks.</p> </li> <li> <p>Uniform contracts. Every plugin implements the same three hooks. The core only provides generic services (var\u2192value map, name generator, constant creation, and a place to put nodes).</p> </li> <li> <p>No central policy on which plugins are \u2018on\u2019. Activation applies whatever plugins declare. If a plugin shouldn\u2019t change tracing yet, it shouldn\u2019t publish a monkey-patch \u2014 that decision is local to the plugin, not the core.</p> </li> </ul>"},{"location":"developer_guide/architecture/#tldr-blueprint-for-maintainers","title":"TL;DR blueprint (for maintainers)","text":"<ol> <li> <p>Core = small, generic: discover, activate, trace, loop eqns, call <code>plugin.lower</code>, assert outputs bound, run IR optimizer, finalize ONNX.    No plugin names. Ever.</p> </li> <li> <p>Plugins = specific: declare patches (all aliases), implement <code>abstract_eval</code>, implement <code>lower</code> (bind outvars), own all op semantics.</p> </li> <li> <p>Context = minimal API for plugins: <code>get_value_for_var</code>, <code>bind_value_for_var</code>, <code>fresh_name</code>, plus a couple IR conveniences; no framework knowledge.</p> </li> <li> <p>Optimizer = tiny, safe, and IR-only: fold layout ping-pongs across pure elementwise ops; match by name or object; never mutate <code>Node.inputs</code> directly \u2014 use <code>replace_input_with</code>.</p> </li> </ol>"},{"location":"developer_guide/plugin_system/","title":"Plugin Quickstart","text":"<p>This walkthrough shows how to add a new plugin to <code>jax2onnx</code>. There are three common flavours:</p> Plugin flavour Purpose Canonical example Low-level primitive Wrap an existing JAX primitive such as <code>jax.lax.abs</code>. Lowerings generally emit a straight ONNX op. <code>jax2onnx/plugins/jax/lax/abs.py</code> High-level primitive / function Provide a composed op (e.g. <code>jax.nn.dot_product_attention</code>, <code>MultiHeadAttention</code>) or a custom <code>@onnx_function</code>. Often manages RNG helpers, symbol binding, or multiple ONNX ops. <code>jax2onnx/plugins/jax/nn/dot_product_attention.py</code> Example plugin Expose an end-to-end regression example for docs/tests; lives under the <code>examples.*</code> namespace. <code>jax2onnx/plugins/examples/jnp/select.py</code> <p>Whichever flavour you choose, the contract is identical:</p> <ol> <li>register metadata so the test generator knows how to rebuild the callable,</li> <li>implement a lowering that emits ONNX IR via the shared builder helpers,</li> <li>register a batching rule so <code>vmap</code> tracing succeeds before we ever reach ONNX,</li> <li>add an <code>expect_graph</code> snippet so structural regressions stay locked down.</li> </ol> <p>The walkthrough below uses a low-level primitive (<code>abs</code>) because it is the smallest template. For high-level or example plugins, follow the same steps and refer back to the table above for richer real-world samples.</p>"},{"location":"developer_guide/plugin_system/#1-pick-a-template","title":"1. Pick a Template","text":"<p>Start from a plugin that matches the flavour you need:</p> <ul> <li>Low-level primitive \u2013 copy <code>lax/abs.py</code>.</li> <li>High-level primitive/function \u2013 look at <code>jax/nn/dot_product_attention.py</code> for a larger pattern with RNG helpers and multiple ONNX ops.</li> <li>Example plugin \u2013 mimic a lightweight regression such as <code>examples/jnp/select.py</code>.</li> </ul> <p>Create a new file under <code>jax2onnx/plugins/&lt;namespace&gt;/...</code> and rename the class and metadata appropriately.</p> <pre><code># jax2onnx/plugins/jax/lax/my_primitive.py\nimport jax\n\nfrom jax2onnx.plugins._post_check_onnx_graph import expect_graph as EG\nfrom jax2onnx.plugins.plugin_system import PrimitiveLeafPlugin, register_primitive\n</code></pre> <p>Tip: keep the namespace consistent (<code>primitives.lax</code>, <code>primitives.nn</code>, etc.). It drives the autogenerated docs and test layout.</p>"},{"location":"developer_guide/plugin_system/#2-register-the-primitive","title":"2. Register the Primitive","text":"<p>Fill in the <code>@register_primitive</code> decorator:</p> <ul> <li><code>jaxpr_primitive</code>: the name JAX uses in the traced <code>ClosedJaxpr</code></li> <li><code>onnx</code>: docs for the ONNX op(s) the lowering will emit</li> <li><code>testcases</code>: at least one entry for the test generator</li> </ul> <pre><code>@register_primitive(\n    jaxpr_primitive=jax.lax.abs_p.name,\n    context=\"primitives.lax\",\n    component=\"abs\",\n    onnx=[{\"component\": \"Abs\", \"doc\": \"https://onnx.ai/...\"}],\n    testcases=[\n        {\n            \"testcase\": \"abs\",\n            \"callable\": lambda x: jax.lax.abs(x),\n            \"input_shapes\": [(3,)],\n            \"post_check_onnx_graph\": EG([\"Abs:3\"], no_unused_inputs=True),\n        },\n    ],\n)\n</code></pre> <p>Use <code>construct_and_call(...).with_requested_dtype(...).with_rng_seed(...)</code> when the primitive needs deterministic module construction or RNG split helpers. See <code>jax2onnx/plugins/jax/nn/dot_product_attention.py</code> for a larger example.</p>"},{"location":"developer_guide/plugin_system/#3-register-a-batching-rule","title":"3. Register a Batching Rule","text":"<p>Every primitive plugin must register a batching rule\u2014JAX errors out during <code>vmap</code> tracing long before the converter runs its lowering hooks otherwise.</p> <ul> <li>For unary, elementwise activations (ReLU, sigmoid, CELU, etc.) call   <code>register_unary_elementwise_batch_rule(&lt;Primitive&gt;)</code> once the class is defined.   The helper lives in <code>jax2onnx/plugins/jax/nn/_builder_utils.py</code> and mirrors the   legacy batching behaviour JAX ships internally.</li> <li>For more complex primitives, add an explicit rule to   <code>jax.interpreters.batching.primitive_batchers[...]</code> near the plugin definition.   Look at <code>jax2onnx/plugins/jax/numpy/stack.py</code> for a pattern that remaps batch   axes before delegating to pure JAX ops.</li> </ul> <p>Only after batching is in place should you run the converter tests\u2014the helper keeps day-to-day plugins concise while still enforcing the guardrail.</p>"},{"location":"developer_guide/plugin_system/#function-plugin-naming-invariants","title":"Function plugin naming invariants","text":"<p>ONNX function plugins now keep the original callable/class name as the node <code>op_type</code>. Uniqueness lives in the call-site metadata instead:</p> <ul> <li>Call nodes are named <code>&lt;Callable&gt;_N</code> (1-indexed) so graphs remain human   readable.</li> <li>Each specialization gets a unique domain, e.g. <code>custom</code> for the first   instance and <code>custom.Callable_2</code> for the second. The pair <code>(op_type, domain)</code>   stays stable across exports and test runs.</li> <li>The converter mirrors every inner domain into the generated <code>Function</code>   opset imports automatically, so ONNX Runtime receives the same domain the   call-site advertises\u2014no manual opset bookkeeping required.</li> </ul> <p>Update structural expectations (<code>expect_graph</code>, ORT checks, etc.) to key off the <code>op_type</code> when you want to match all instances, and fall back to the full <code>node.name</code> only when a specific call-site matters. Older expectations that referenced <code>Callable_1</code> continue to pass because the checker strips numeric suffixes when comparing <code>op_type</code>.</p>"},{"location":"developer_guide/plugin_system/#4-implement-lower","title":"4. Implement <code>lower</code>","text":"<p>Fetch inputs and pre-allocated outputs via the lowering context, then emit the ONNX op through <code>ctx.builder</code>:</p> <pre><code>class AbsPlugin(PrimitiveLeafPlugin):\n    def lower(self, ctx, eqn):\n        x_var = eqn.invars[0]\n        out_var = eqn.outvars[0]\n\n        x_val = ctx.get_value_for_var(x_var, name_hint=ctx.fresh_name(\"abs_in\"))\n        out_val = ctx.get_value_for_var(out_var, name_hint=ctx.fresh_name(\"abs_out\"))\n\n        result = ctx.builder.Abs(x_val, _outputs=[out_val.name or ctx.fresh_name(\"abs_out\")])\n\n        # Stamp metadata if the pre-allocated value already knows its type/shape\n        if getattr(out_val, \"type\", None) is not None:\n            result.type = out_val.type\n        if getattr(out_val, \"shape\", None) is not None:\n            result.shape = out_val.shape\n\n        ctx.bind_value_for_var(out_var, result)\n</code></pre> <p>Key guardrails:</p> <ul> <li>Emit ops via the builder (<code>ctx.builder.Abs(...)</code>). Avoid constructing   <code>ir.Node</code> directly unless you need advanced features.</li> <li><code>_outputs</code> must be a sequence (<code>[\"_name\"]</code>, not <code>\"name\"</code>).</li> <li>Bind every <code>eqn.outvars[i]</code> using <code>ctx.bind_value_for_var(...)</code>.</li> <li>If you allocate constants, use helpers such as <code>_const_i64</code> that route through   the builder so initializers stay registered.</li> </ul> <p>The ONNX IR Builder Guide lists every policy enforced by the automated checks.</p>"},{"location":"developer_guide/plugin_system/#5-add-a-structural-assertion","title":"5. Add a Structural Assertion","text":"<p>The <code>post_check_onnx_graph</code> entry in the testcase calls the structural checker (<code>expect_graph</code>). Use <code>scripts/emit_expect_graph.py</code> to capture the snippet:</p> <pre><code>poetry run python scripts/emit_expect_graph.py abs\n</code></pre> <p>Slot the output into the metadata and rerun the command whenever the lowering changes shape.</p> <p>For more involved graphs, consult <code>advanced_topics/expect_graph_reference.md</code> for matching tips.</p>"},{"location":"developer_guide/plugin_system/#6-run-the-tests","title":"6. Run the Tests","text":"<p>At minimum:</p> <pre><code>poetry run python scripts/check_ir_builder_usage.py --diff\npoetry run pytest -q tests/primitives/test_lax.py -k abs\npoetry run pytest -q\n</code></pre> <p>The pre-commit hooks execute the same checks, but running them locally keeps the feedback loop tight.</p>"},{"location":"developer_guide/plugin_system/#7-submit-the-pr","title":"7. Submit the PR","text":"<p>Include the plugin file and the updated tests. Reference the example you copied from in your PR description so reviewers know the baseline.</p> <p>For deeper dives:</p> <ul> <li>ONNX IR Builder Guide</li> <li>Expect Graph Reference</li> <li><code>jax2onnx/plugins/jax/lax/abs.py</code> \u2013   full, working example</li> <li><code>jax2onnx/plugins/jax/nn/dot_product_attention.py</code> \u2013 high-level primitive with RNG + multi-op lowering</li> <li><code>jax2onnx/plugins/examples/jnp/select.py</code> \u2013 minimal example plugin</li> <li><code>docs/design.md</code> \u2013 architecture overview and plugin roles</li> </ul> <p>Happy lowering!</p>"},{"location":"developer_guide/advanced_topics/complex_numbers/","title":"Complex Numbers in jax2onnx","text":"<p>This guide explains how <code>jax2onnx</code> handles complex tensors while staying within the ONNX specification, and how plugin authors should interact with the shared helper utilities.</p>"},{"location":"developer_guide/advanced_topics/complex_numbers/#why-we-need-a-strategy","title":"Why we need a strategy","text":"<p>ONNX (up to opset 21) does not provide native <code>tensor(complex*)</code> types for most operators. Arithmetic, shape, and control-flow primitives all expect real-valued tensors. The only complex-aware operator we rely on is <code>DFT</code>, which represents complex inputs and outputs as real tensors whose trailing dimension packs the real and imaginary channels (<code>[..., 2]</code>).</p> <p>To stay portable across runtimes we represent every complex tensor as a real tensor with that trailing size-2 channel. Conversion never emits <code>Real</code>, <code>Imag</code>, or other custom operators\u2014everything is expressed in terms of standard ONNX ops on real tensors.</p>"},{"location":"developer_guide/advanced_topics/complex_numbers/#helper-surface-plugins_complex_utilspy","title":"Helper surface (<code>plugins/_complex_utils.py</code>)","text":"Helper Purpose <code>pack_native_complex(ctx, tensor)</code> Reinterpret a native <code>complex64/complex128</code> value as a packed real tensor (<code>[..., 2]</code>). Handles double-precision upgrades automatically when <code>enable_double_precision=True</code>. <code>is_packed_complex_tensor(value)</code> Detect whether a value already uses the packed representation. <code>ensure_packed_real_pair(ctx, value)</code> Return <code>(packed_tensor, base_dtype)</code> for both native complex inputs and already-packed tensors. Raises if the value is neither. <code>cast_real_tensor(ctx, value, target_dtype)</code> Insert a <code>Cast</code> when the packed tensor must move between <code>FLOAT</code> and <code>DOUBLE</code> representations. <code>resolve_common_real_dtype(lhs, rhs)</code> Pick the shared real dtype (<code>FLOAT</code> or <code>DOUBLE</code>) for binary complex operations. <code>split_packed_real_imag(ctx, value, base_dtype)</code> Gather the trailing real and imaginary channels from a packed tensor, returning two real tensors. <code>pack_real_imag_pair(ctx, real, imag, base_dtype)</code> Unsqueeze matching real/imag tensors and concatenate them back into the packed <code>[... , 2]</code> representation. <code>conjugate_packed_tensor(ctx, value, base_dtype)</code> Flip the sign of the imaginary channel while preserving shape metadata, producing the complex conjugate of a packed tensor. <code>coerce_dim_values(dims)</code> Normalise shape metadata so <code>onnx_ir</code> can stamp symbolic dimensions and integers consistently. <code>unpack_to_native_complex(...)</code> Convert a packed tensor back to a native complex value (used rarely, e.g. when handing results back to JAX in test harnesses). <p>These helpers take care of dtype metadata, <code>IRBuilder</code> stamping, and axis bookkeeping so individual plugins only need to express the real-valued arithmetic. New complex-aware plugins should rely on them instead of ad hoc <code>Gather</code> / <code>Reshape</code> sequences so every lowering shares the same representation.</p>"},{"location":"developer_guide/advanced_topics/complex_numbers/#supported-operations","title":"Supported operations","text":"<ul> <li>Elementwise arithmetic (<code>lax.add</code>, <code>lax.sub</code>, <code>lax.mul</code>, <code>lax.div</code>):</li> <li>Detection logic looks at the JAX avals and value metadata. When a complex value is involved we normalise operands through <code>ensure_packed_real_pair</code>, align their base dtype (<code>FLOAT</code> \u2194 <code>DOUBLE</code>) via <code>resolve_common_real_dtype</code> / <code>cast_real_tensor</code>, run the real-valued formulas, and use <code>pack_real_imag_pair</code> to rebuild the packed output.</li> <li> <p>Outputs inherit the packed representation and expose real metadata (<code>tensor(float)</code> / <code>tensor(double)</code> with trailing <code>2</code>).</p> </li> <li> <p>FFT pipeline (<code>lax.fft</code>, <code>jnp.fft</code> for FFT/IFFT/RFFT):</p> </li> <li>Complex inputs (FFT/IFFT) are packed, reshaped if needed, and lowered to ONNX <code>DFT</code> with <code>inverse</code> / <code>onesided</code> flags. Real inputs (RFFT) receive the trailing channel before invoking <code>DFT</code>.</li> <li><code>IRFFT</code> currently requires explicit <code>fft_lengths</code>. The implementation reconstructs the missing half of the spectrum, flips the imaginary channel, and runs a forward packed <code>DFT</code> before gathering the real component.</li> <li> <p>For <code>jnp.fft</code>, we register metadata-only primitives that reuse the same lowering when the call matches the canonical 1-D form (<code>axis=-1</code>, optional length). <code>irfft</code> keeps the original NumPy behaviour until we finish integrating the dtype-safe reconstruction path.</p> </li> <li> <p>MatMul / Einsum family (<code>jax.lax.dot_general</code>, <code>jnp.matmul</code>):</p> </li> <li>Operands are normalised via <code>ensure_packed_real_pair</code> and cast to a shared real dtype. The real/imag channels are split with <code>split_packed_real_imag</code>, the real-valued contraction (<code>Einsum</code> or <code>MatMul</code>) runs four times, and <code>pack_real_imag_pair</code> stitches the results back together.</li> <li>For <code>dot_general</code>, both the batched MatMul fast-path and general <code>Einsum</code> lowering share the same helper plumbing so the trailing complex channel is never part of the contraction labels.</li> <li> <p>For <code>jnp.matmul</code>, the four-real flow lowers to four ONNX <code>MatMul</code> nodes before recombining; broadcasting and vector/matrix promotion match the real path.</p> </li> <li> <p>Convolutions (<code>jax.lax.conv_general_dilated</code>):</p> </li> <li>Inputs and kernels flow through <code>ensure_packed_real_pair</code>, are cast to a shared dtype, and have the complex channel split before any layout transposes.</li> <li> <p>Each of the four real-valued paths runs through the existing Conv lowering (after layout canonicalisation). Outputs are optionally transposed back to the requested layout and re-packed with <code>pack_real_imag_pair</code>.</p> </li> <li> <p>Conjugation (<code>jax.lax.conj</code>, <code>jnp.conj</code>):</p> </li> <li> <p>Normalise packed/native complex inputs with <code>ensure_packed_real_pair</code>, call <code>conjugate_packed_tensor</code> to negate the imaginary channel, and return the packed output. Real inputs bypass through an <code>Identity</code>.</p> </li> <li> <p>Tests: regression coverage lives under <code>tests/primitives/test_lax.py::Test_fft</code>, <code>Test_add</code>, <code>Test_sub</code>, <code>Test_mul</code>, <code>Test_div</code>, and <code>tests/primitives/test_jnp.py::Test_fft/ifft/rfft</code>.</p> </li> </ul>"},{"location":"developer_guide/advanced_topics/complex_numbers/#authoring-new-plugins-with-complex-inputs","title":"Authoring new plugins with complex inputs","text":"<ol> <li>Detect complex flows early. Inspect JAX avals (<code>var.aval.dtype</code>) or existing value metadata. If the operand is complex, call <code>ensure_packed_real_pair(...)</code> to normalise it.</li> <li>Work in real space. Once packed, treat the tensors as real arrays. Use <code>resolve_common_real_dtype</code> and <code>cast_real_tensor</code> to reconcile dtypes before running arithmetic.</li> <li>Stamp shapes and metadata. Most helpers already stamp values, but if you build new tensors (e.g., concatenations) remember to call <code>_stamp_type_and_shape</code> with <code>coerce_dim_values(...)</code> so the ONNX graph carries explicit metadata.</li> <li>Return packed outputs. Results should remain in <code>[... , 2]</code> form. Do not attempt to reintroduce native complex ONNX tensors\u2014runtimes will reject them.</li> <li>Tests + docs. Add <code>expect_graph</code> snippets alongside the plugin metadata and cover complex variants in the autogenerated test suites.</li> </ol>"},{"location":"developer_guide/advanced_topics/complex_numbers/#current-limitations","title":"Current limitations","text":"<ul> <li><code>jnp.fft.irfft</code> still delegates to the upstream implementation. The packed-real helpers need a dtype-clean reconstruction path before we can reuse the ONNX <code>DFT</code> lowering safely; track this separately if IRFFT metadata is required.</li> <li>When new primitives handle complex data (e.g., transcendental ops), follow the same recipe outlined above: convert to packed real tensors, run the pure-real arithmetic, and emit <code>[... , 2]</code> outputs.</li> <li>Convolution transpose / deconvolution paths are not yet implemented in <code>jax2onnx</code>; once a plugin lands it should reuse the same four-real structure (split, canonicalise layout, regroup, repack).</li> <li>Additional regression coverage (broadcasted shapes, reduced-precision dtypes such as <code>bfloat16</code>, and multi-group convolutions) is staged in <code>work_notes_complex.md</code> and will be brought online incrementally.</li> </ul>"},{"location":"developer_guide/advanced_topics/complex_numbers/#potential-optimizations","title":"Potential optimizations","text":"<ul> <li>Gaussian 3-multiply strategy: at the moment we always lower via the straightforward four-real expansion. The <code>complex_strategy</code> knob isn\u2019t wired yet; it could be exposed as (<code>four_real</code> by default, <code>gauss</code> as an opt-in). The Gauss variant would replace the four real multiplies <code>[(a_r b_r), (a_i b_i), (a_r b_i), (a_i b_r)]</code> with three multiplies plus a few adds:  </li> <li><code>p1 = a_r * b_r</code> </li> <li><code>p2 = a_i * b_i</code> </li> <li><code>p3 = (a_r + a_i) * (b_r + b_i)</code>      then reconstruct <code>real = p1 - p2</code> and <code>imag = p3 - p1 - p2</code>.   We\u2019ll evaluate this once we can guarantee backend support and have regression coverage for the numerical trade-offs.</li> </ul>"},{"location":"developer_guide/advanced_topics/expect_graph_reference/","title":"<code>expect_graph</code> checklist for plugins","text":"<p><code>expect_graph</code> (from <code>jax2onnx.plugins._post_check_onnx_graph</code>) is the lightweight structural assertion helper used by plugin tests and examples. It lets a test express the operators, ordering, and shapes that should appear in a converted IR/ONNX graph without dumping the full model. This document captures the conventions we rely on when writing or reviewing <code>post_check_onnx_graph</code> expectations.</p> <p>Test metadata reminder: when wiring new examples/tests, construct callables via <code>construct_and_call(...).with_requested_dtype(...).with_rng_seed(...)</code> so the harness can rebuild deterministic f32/f64 variants. See the builder guide for the full randomness and dtype rules.</p>"},{"location":"developer_guide/advanced_topics/expect_graph_reference/#import","title":"Import","text":"<pre><code>from jax2onnx.plugins._post_check_onnx_graph import expect_graph as EG\n</code></pre> <p>Alias it to <code>EG</code> inside tests to keep callsites short.</p> <p>Builder reminder: structural tests assume plugins emitted nodes via <code>ctx.builder</code>. Review the ONNX IR Builder Guide if <code>_outputs</code> naming or initializer wiring looks suspicious; policy tests now enforce those contracts.</p>"},{"location":"developer_guide/advanced_topics/expect_graph_reference/#basic-usage","title":"Basic usage","text":"<p>Pass a list of patterns to <code>expect_graph</code>. Each pattern is either a string or a <code>(string, options)</code> tuple. Nodes are written in evaluation order with <code>-&gt;</code> separating them.</p> <pre><code>EG([\n    \"Transpose -&gt; Conv -&gt; Relu -&gt; AveragePool\",\n])\n</code></pre> <p>The pattern above requires the graph to contain that exact operator chain. Failing to find it raises an assertion with a summarized diff of the graph.</p>"},{"location":"developer_guide/advanced_topics/expect_graph_reference/#encoding-shapes","title":"Encoding shapes","text":"<p>Append <code>:shape</code> to a node name to assert the output shape of that node. Use <code>x</code> separators (e.g. <code>Bx32x28x28</code>). Leave dimensions symbolic by reusing the string symbol that the test harness passed as an input shape (for example <code>\"B\"</code>).</p> <pre><code>EG([\n    \"Gemm:Bx256 -&gt; Relu:Bx256 -&gt; Gemm:Bx10\",\n])\n</code></pre> <p>Write concrete integers for known static sizes (<code>3x1x28x28</code>). Symbols and integers can be mixed (<code>B?x256</code> is not supported; prefer <code>symbols={\"B\": None}</code> if you need to unify multiple strings).</p>"},{"location":"developer_guide/advanced_topics/expect_graph_reference/#additional-match-options","title":"Additional match options","text":"<p>Attach an options dictionary to require counts, forbid nodes, or tweak the search.</p> <pre><code>EG([\n    (\n        \"Transpose:3x1x28x28 -&gt; Conv:3x32x28x28 -&gt; Relu:3x32x28x28 -&gt; Gemm:3x256\",\n        {\n            \"counts\": {\"Transpose\": 1, \"Conv\": 1, \"Relu\": 1, \"Gemm\": 1},\n        },\n    ),\n],\nno_unused_inputs=True,\nmode=\"all\",\nmust_absent=[\"Not\"],\n)\n</code></pre> <p>Common fields:</p> <ul> <li><code>counts</code>: map of op type to the exact number of occurrences expected.</li> <li><code>must_absent</code>: list of operator names that must not appear anywhere.</li> <li><code>symbols</code>: dictionary mapping symbolic dim labels to <code>None</code> (any value) or an   integer (specific size). Use it when multiple patterns should share the same   symbolic dimension.</li> <li><code>mode</code>: one of <code>\"all\"</code> (default; all patterns must match), <code>\"any\"</code> (at least   one matches), or <code>\"exact\"</code> (the entire graph must equal the pattern).</li> <li><code>no_unused_inputs</code>: when <code>True</code>, fail if the graph retains dangling inputs   after conversion. Combine with <code>no_unused_function_inputs=True</code> to extend the   check to every imported ONNX function body (requires <code>search_functions=True</code>).</li> <li><code>search_functions</code>: include function bodies (control-flow subgraphs) in the   search.</li> </ul> <p>The matcher automatically walks through helper nodes that frequently sit on the main data edge (by default we skip <code>Reshape</code>, <code>Identity</code>, <code>Cast</code>, <code>CastLike</code>, <code>Squeeze</code>, <code>Unsqueeze</code>, <code>Flatten</code>, <code>Shape</code>, <code>Gather</code>, <code>Concat</code>, <code>Add</code>, and <code>Where</code>). This lets a single pattern cover sequential graphs where tensors fan out into shape-building side chains, such as the CNN dynamic example where the <code>Transpose</code> output feeds both <code>Reshape</code> and the shape-construction subgraph.</p>"},{"location":"developer_guide/advanced_topics/expect_graph_reference/#function-naming-compatibility","title":"Function naming compatibility","text":"<p>Function exports now keep the original callable name as the node <code>op_type</code> (<code>TransformerBlock</code>, <code>MLPBlock</code>, \u2026) and move the numeric suffix into <code>node.name</code>/<code>domain</code> (<code>TransformerBlock_2</code>, <code>custom.TransformerBlock_2</code>, \u2026). To keep older expectations valid, <code>expect_graph</code> automatically strips trailing <code>_123</code> suffixes when comparing <code>op_type</code> and normalises graph filters such as <code>fn:custom.TransformerBlock_2</code>. Prefer matching on the base <code>op_type</code> unless a specific call-site needs to be distinguished by name.</p>"},{"location":"developer_guide/advanced_topics/expect_graph_reference/#practical-tips","title":"Practical tips","text":"<ul> <li>Prefer a single path that covers the interesting operators rather than every   node in the graph. Keep counts strict if extra occurrences would signal a   regression.</li> <li>Include shapes for layers where layout or dimension handling is important   (transposes, pooling, reshapes). Shape assertions catch missing   <code>_stamp_type_and_shape</code> calls and layout errors quickly.</li> <li>Keep expectations small for dynamic tests; the static counterpart usually   asserts shapes, while a dynamic test covers symbolic behaviour or flags.</li> <li>Use <code>mode=\"all\"</code> with multiple patterns to check disjoint subgraphs, or   <code>mode=\"exact\"</code> when the entire graph must be anchored (rare; harder to   maintain).</li> <li>If the graph contains fused or optimizer-inserted elementwise ops, anchor the   pattern on the surrounding operators and rely on <code>counts</code> to constrain the   totals.</li> </ul>"},{"location":"developer_guide/advanced_topics/expect_graph_reference/#maintaining-coverage","title":"Maintaining Coverage","text":"<ul> <li>Every plugin/example should ship an <code>expect_graph(...)</code> snippet alongside tests; rerun <code>python scripts/emit_expect_graph.py &lt;testcase&gt;</code> whenever behaviour changes.</li> <li>Regenerate and update this guide after each sweep so metadata and documentation stay in sync.</li> <li>When new fixtures land, add them to the coverage snapshot and verify the relevant pytest target before updating this doc.</li> </ul>"},{"location":"developer_guide/advanced_topics/expect_graph_reference/#workflow-checklist","title":"Workflow Checklist","text":"<ol> <li>Identify the next uncovered component (scan for missing <code>register_example</code> / <code>register_primitive</code> entries if needed).</li> <li>Capture the snippet via <code>poetry run python scripts/emit_expect_graph.py &lt;testcase&gt;</code>.</li> <li>Update metadata/tests with the snippet, run the focused pytest target, then expand to the broader suite if applicable.</li> <li>Refresh this guide (and coverage tables) with the new snippet.</li> <li>Before wrapping, ensure everything is documented and guardrails (RNG helpers, ONNX-IR boundaries, attention normalisation) are respected.</li> </ol>"},{"location":"developer_guide/advanced_topics/expect_graph_reference/#guardrails","title":"Guardrails","text":"<ul> <li>Converter/plugins must remain ONNX-IR only (no protobuf imports).</li> <li>Use <code>construct_and_call(...).with_requested_dtype()</code> and <code>with_rng_seed(...)</code>; split PRNG keys before reuse.</li> <li>Attention plugins must retain masked-weight normalisation; expect_graph snippets should reflect the normalised path.</li> <li>Run core tooling (<code>poetry run pytest -q</code>, <code>poetry run ruff check .</code>, <code>poetry run mypy src</code>) for larger sweeps.</li> </ul>"},{"location":"developer_guide/advanced_topics/expect_graph_reference/#where-to-use-it","title":"Where to use it","text":"<p><code>post_check_onnx_graph</code> entries appear inside example/plugin test metadata (see <code>jax2onnx/plugins/examples/nnx/cnn.py</code> for a reference). The helper works with any object that produces an ONNX IR graph compatible with <code>onnx_ir.GraphProto</code>. The same API is shared by policy tests under <code>tests/extra_tests</code>.</p> <p>When adding new metadata entries, seed them with a minimal structural check, run the example once to capture the intended op sequence, and then layer on shape assertions and counts to guard against regressions.</p>"},{"location":"developer_guide/advanced_topics/expect_graph_reference/#reference-snippets-oct-2025-refresh","title":"Reference snippets (Oct\u00a02025 refresh)","text":"<p>NNX reminder: follow the Oct\u00a02025 AGENTS note\u2014seed nnx fixtures via <code>with_rng_seed(...)</code> (no inline lambdas) so callables stay hashable under JAX 0.7. Attention plugins must keep masked-weight normalisation enabled; retain the helper path when updating metadata or docs.</p>"},{"location":"developer_guide/advanced_topics/expect_graph_reference/#scatter-add-sweep-primitiveslaxscatter_add","title":"Scatter add sweep (<code>primitives.lax.scatter_add</code>)","text":"<p>The converter now anchors the full regression matrix on <code>ScatterND</code>. These snippets were regenerated with <code>JAX_ENABLE_X64=1</code> to keep f64 parity.</p> <pre><code>EG(['ScatterND:4'], no_unused_inputs=True)  # scatter_add_vector\nEG([{'path': 'ScatterND:6', 'inputs': {2: {'const': 5.0}}}], no_unused_inputs=True)  # scatter_add_scalar\nEG(['ScatterND:5'], no_unused_inputs=True)  # scatter_add_simple_1d / scatter_add_batch_updates_1d_operand\nEG(['ScatterND:2x3'], no_unused_inputs=True)  # scatter_add_window_2d_operand_1d_indices\nEG(['ScatterND:5x208x1x1'], no_unused_inputs=True)  # scatter_add_mismatched_window_dims_from_user_report\n</code></pre> <p>Additional user-report variants (<code>report2</code>, <code>report3</code>, fluids pattern, depth helpers) share the same <code>ScatterND:&lt;shape&gt;</code> signature and reuse the wrapper helpers documented in <code>jax2onnx/plugins/jax/lax/scatter_add.py</code>.</p>"},{"location":"developer_guide/advanced_topics/expect_graph_reference/#issue-18-loop-fixtures-examplesjnpissue18","title":"Issue 18 loop fixtures (<code>examples.jnp.issue18</code>)","text":"<p>Regenerated loop traces now expose the control-flow helpers and the loop-carried symbol. Remember to pass <code>search_functions=True</code> when validating subgraph bodies.</p> <pre><code>EG([{'path': 'Loop', 'inputs': {0: {'const': 5.0}, 1: {'const_bool': True}}}],\n   search_functions=True, no_unused_inputs=True)  # fori_loop_fn\nEG([{'path': 'Less -&gt; Loop', 'inputs': {0: {'const': 9.223372036854776e18}, 3: {'const': 0.0}}}],\n   no_unused_inputs=True)  # while_loop_fn\nEG(['Loop:B'], symbols={'B': None}, search_functions=True, no_unused_inputs=True)  # scan_fn\nEG(['Greater:3 -&gt; Where:3'], no_unused_inputs=True)  # where_fn\n</code></pre>"},{"location":"developer_guide/advanced_topics/expect_graph_reference/#flaxnnx-gru-cell-examplesnnxgru_cell_basic","title":"Flax/NNX GRU cell (<code>examples.nnx.gru_cell_basic</code>)","text":"<p>The ONNX lowering now fuses the <code>Tanh</code> stage, resulting in twin <code>Add</code> paths off the Sigmoid gate outputs. Regenerate the snippet after adjusting module wiring, and keep the RNG helpers in place so the sample stays deterministic.</p> <pre><code>EG(\n    [\n        \"Gemm:2x12 -&gt; Split:2x4 -&gt; Add:2x4 -&gt; Sigmoid:2x4 -&gt; Sub:2x4 -&gt; Mul:2x4 -&gt; Add:2x4\",\n        \"Gemm:2x12 -&gt; Split:2x4 -&gt; Add:2x4 -&gt; Sigmoid:2x4 -&gt; Sub:2x4 -&gt; Mul:2x4 -&gt; Add:2x4 -&gt; Add:2x4\",\n    ],\n    no_unused_inputs=True,\n)\n</code></pre>"},{"location":"developer_guide/advanced_topics/expect_graph_reference/#nnx-multi-head-attention-examplesnnxmulti_head_attention","title":"NNX Multi-head attention (<code>examples.nnx.multi_head_attention</code>)","text":"<p>Both the pure JAX (<code>multihead_attention_nn</code>) and nnx-driven (<code>multihead_attention_nnx</code>, <code>multihead_attention_2_nnx</code>) variants emit the same core chain after reshaping queries/keys/values. Symbols capture the batch size; <code>search_functions=True</code> keeps subgraph rewrites under the function export.</p> <pre><code>EG(\n    [\n        \"Reshape:?x256 -&gt; Gemm:?x256 -&gt; Reshape:Bx4x8x32 -&gt; \"\n        \"Transpose:Bx8x4x32 -&gt; MatMul:Bx8x4x4 -&gt; Mul:Bx8x4x4 -&gt; \"\n        \"Softmax:Bx8x4x4 -&gt; MatMul:Bx8x4x32 -&gt; Transpose:Bx4x8x32 -&gt; \"\n        \"Reshape:?x256 -&gt; Gemm:?x256 -&gt; Reshape:Bx4x256\"\n    ],\n    symbols={\"B\": None},\n    search_functions=True,\n    no_unused_inputs=True,\n)\n</code></pre>"},{"location":"developer_guide/advanced_topics/expect_graph_reference/#equinox-dinov3-vision-transformer-exampleseqx_dino","title":"Equinox DINOv3 vision transformer (<code>examples.eqx_dino</code>)","text":"<p>Use the bundled helper to emit snippets for each variant (<code>eqx_dinov3_vit_Ti14</code>, <code>_S14</code>, <code>_B14</code>, <code>_S16</code>). All of them collapse into a single <code>VisionTransformer</code> node with the expected patch/token layout. Ensure <code>no_unused_inputs=True</code> stays set so cached weights or mask inputs do not linger.</p> <pre><code>EG(['VisionTransformer:Bx257x192'], symbols={'B': None}, no_unused_inputs=True)\n# S16/S14/B14 variants only differ in the trailing dimension (384/768) and the token count.\n</code></pre>"},{"location":"developer_guide/advanced_topics/expect_graph_reference/#equinox-dino-building-blocks","title":"Equinox DINO building blocks","text":"<p>While covering the DINO stack, keep these helpers in sync:</p> <pre><code>EG(['PatchEmbed_1:1x256x384'], no_unused_inputs=True)  # PatchEmbed\nEG([{'path': 'Gemm', 'counts': {'Gemm': 4}},\n    {'path': 'MatMul', 'counts': {'MatMul': 2}},\n    {'path': 'Softmax', 'counts': {'Softmax': 1}}],\n   symbols={'B': None}, search_functions=True, no_unused_inputs=True)  # AttentionCore\nEG([{'path': 'MatMul', 'counts': {'MatMul': 2}}, 'Softmax'],\n   symbols={'B': None}, search_functions=True)  # Attention\nEG(['Block_1:Bx257x384'], symbols={'B': None}, must_absent=['Identity'])  # Block\n</code></pre>"},{"location":"developer_guide/advanced_topics/expect_graph_reference/#gpt-components-examplesgpt","title":"GPT components (<code>examples.gpt</code>)","text":"<p><code>GPT_Attention</code> and <code>GPT_CausalSelfAttention</code> rely on the shared <code>_no_cast_where</code> helper, which fails the test if any <code>Cast -&gt; Where</code> chain appears in the exported graph (and reruns with diagnostics to surface the offending path). The rest of the GPT stack leans on compact structural checks:</p> <pre><code>EG(['MLP_1:Bx1024x768'], symbols={'B': None}, no_unused_inputs=True)  # GPT_MLP\nEG(['Block_1:Bx1024x768'], symbols={'B': None}, no_unused_inputs=True)  # GPT_TransformerBlock\nEG(['TokenEmbedding_1:Bx1024x768'], symbols={'B': None}, no_unused_inputs=True)  # GPT_TokenEmbedding\nEG(['PositionEmbedding_1:1x1024x768'], no_unused_inputs=True)  # GPT_PositionEmbedding\nEG(['GPTTransformerStack_1:Bx1024x768'], symbols={'B': None}, no_unused_inputs=True)  # GPT_TransformerStack\nEG(['GPTEmbeddings_1:Bx1024x768'], symbols={'B': None}, no_unused_inputs=True)  # GPT_Embeddings\nEG(['GPTHead_1:Bx1024x3144'], symbols={'B': None}, no_unused_inputs=True)  # GPT_Head\nEG(['Add:Bx4x5'], symbols={'B': None}, no_unused_inputs=True)  # GPT_broadcast_add\nEG(\n    [{'graph': 'custom.PositionEmbedding.1:PositionEmbedding',\n      'path': 'Range -&gt; Unsqueeze -&gt; Expand -&gt; Gather',\n      'must_absent': ['Cast']}],\n    no_unused_inputs=True,\n    no_unused_function_inputs=True,\n    search_functions=True,\n)  # GPT\n</code></pre>"},{"location":"developer_guide/advanced_topics/expect_graph_reference/#vision-transformer-components-examplesvit-examplesvit_flat","title":"Vision Transformer components (<code>examples.vit</code>, <code>examples.vit_flat</code>)","text":"<p>Patch/conv embeddings and the final classifier keep the ViT snippets compact. When working on the flattened variants, keep the reshape/transposes anchored so the sequence length stays guarded.</p> <pre><code>EG(['PatchEmbedding_1:Bx49x256'], symbols={'B': None}, no_unused_inputs=True)\nEG(['ConvEmbedding_1:Bx49x128'], symbols={'B': None}, no_unused_inputs=True)\nEG(['FeedForward_1:Bx10x256'], symbols={'B': None}, no_unused_inputs=True)\nEG(['TransformerBlock_1:Bx10x256'], symbols={'B': None}, no_unused_inputs=True)\nEG(['TransformerStack_1:Bx10x256'], symbols={'B': None}, no_unused_inputs=True)\nEG(['ConcatClsToken_1:Bx50x256'], symbols={'B': None}, no_unused_inputs=True)\nEG(['PositionalEmbedding_1:Bx50x256'], symbols={'B': None}, no_unused_inputs=True)\nEG(['VisionTransformer_1:Bx10'], symbols={'B': None}, no_unused_inputs=True)  # conv embedding model\nEG(['VisionTransformer_1:2x10'], no_unused_inputs=True)  # patch embedding model\nEG(['LayerNormalization -&gt; Gemm -&gt; LogSoftmax'], symbols={'B': None}, no_unused_inputs=True)  # flattened ViT heads\nEG(\n    [\n        \"Reshape:Bx7x4x7x4x1 -&gt; Transpose:Bx7x7x4x4x1 -&gt; \"\n        \"Reshape:Bx49x16 -&gt; Reshape:?x16 -&gt; Gemm:?x256 -&gt; Reshape:Bx49x256\"\n    ],\n    symbols={'B': None},\n    no_unused_inputs=True,\n)  # PatchEmbeddingFlatten\nEG(\n    ['Slice -&gt; Squeeze', {'path': 'Transpose:50xBx256 -&gt; Gather:Bx256', 'inputs': {1: {'const': 0.0}}}],\n    mode='any',\n    symbols={'B': None},\n    no_unused_inputs=True,\n)  # GetToken\nEG(['ClassificationHead_1:Bx10'], symbols={'B': None}, no_unused_inputs=True)\n</code></pre>"},{"location":"developer_guide/advanced_topics/expect_graph_reference/#transformer-decoder-variants-examplesnnxtransformer_decoder_","title":"Transformer decoder variants (<code>examples.nnx.transformer_decoder_*</code>)","text":"<p>Both decoder flavours share the same residual-add/LayerNorm cadence; the sequential version also has a dynamic-shape testcase.</p> <pre><code>EG(\n    [\"Add:2x8x16 -&gt; LayerNormalization:2x8x16 -&gt; Add:2x8x16 -&gt; LayerNormalization:2x8x16 -&gt; Add:2x8x16 -&gt; LayerNormalization:2x8x16\"],\n    search_functions=True,\n    no_unused_inputs=True,\n)  # TransformerDecoderWithSequential (static shapes)\nEG(\n    [\"Add:BxHx16 -&gt; LayerNormalization:BxHx16 -&gt; Add:BxHx16 -&gt; LayerNormalization:BxHx16 -&gt; Add:BxHx16 -&gt; LayerNormalization:BxHx16\"],\n    search_functions=True,\n    symbols={'B': None, 'H': None},\n    no_unused_inputs=True,\n)  # TransformerDecoderWithSequential (dynamic shapes)\nEG(\n    [\"Add:Bx8x16 -&gt; LayerNormalization:Bx8x16 -&gt; Add:Bx8x16 -&gt; LayerNormalization:Bx8x16 -&gt; Add:Bx8x16 -&gt; LayerNormalization:Bx8x16\"],\n    symbols={'B': 2},\n    no_unused_inputs=True,\n)  # TransformerDecoderWithoutSequential\n</code></pre>"},{"location":"developer_guide/advanced_topics/ir_optimizer/","title":"IR Optimizer Passes","text":"<p>The converter runs a lightweight, IR-only optimization sweep after lowering and before serialization. Passes must be structure-only (no op-specific math) and safe across <code>onnx_ir</code> variants. This guide documents the current canon and the invariants each pass must respect.</p>"},{"location":"developer_guide/advanced_topics/ir_optimizer/#pipeline-placement","title":"Pipeline Placement","text":"<p>The optimizer runs as Step 2 in the conversion pipeline (see architecture.md):</p> <ol> <li>Build raw IR (<code>to_onnx</code>)</li> <li><code>optimize_graph</code> \u2190 runs here</li> <li>Late attribute overrides</li> <li>Shape inference (no-op currently)</li> <li>Finalize shapes</li> <li>Return from <code>conversion_api</code></li> <li>Post-process (shape loosening, export prep)</li> </ol> <p>This placement ensures: - Optimization sees the raw, unpatched graph for maximum benefit. - Late overrides only patch nodes that survived optimization. - Shape finalization operates on an already-optimized graph.</p>"},{"location":"developer_guide/advanced_topics/ir_optimizer/#transpose-pair-folding","title":"Transpose Pair Folding","text":"<p>Pattern <code>Transpose \u2192 [pure elementwise]* \u2192 Transpose</code></p> <p>Condition The composed permutation of the Transpose nodes equals identity.</p> <p>Allowed middle ops Elementwise operators that do not reorder elements, including: <code>Relu</code>, <code>Gelu</code>, <code>Elu</code>, <code>Sigmoid</code>, <code>Tanh</code>, <code>LeakyRelu</code>, <code>Dropout</code>, <code>Cast</code>, <code>CastLike</code>, <code>Identity</code>, etc.</p> <p>Not folded Anything that crosses non-elementwise operators such as <code>AveragePool</code>, <code>Conv</code>, or similar layout-sensitive ops.</p>"},{"location":"developer_guide/advanced_topics/ir_optimizer/#matching-heuristics","title":"Matching heuristics","text":"<ul> <li>Follow the true consumer chain by name or object identity (some <code>onnx_ir</code> builds wrap/rename <code>Value</code> objects).</li> <li>Skip helper nodes on side branches (<code>Const</code>, <code>Shape</code>, etc.) that do not consume the current tensor.</li> <li>Require single consumer at each hop (no branching rewires).</li> <li>Read permutations from the <code>perm</code> attribute when available.</li> <li>When <code>perm</code> is missing, treat the pair as cancellable only if the input and output shapes match and the middle segment is strictly elementwise.</li> </ul>"},{"location":"developer_guide/advanced_topics/ir_optimizer/#rewiring-and-deletion","title":"Rewiring and deletion","text":"<ul> <li><code>onnx_ir.Node.inputs</code> may be immutable; use <code>Node.replace_input_with(index: int, value: Value)</code> when provided by the backend.</li> <li>Rewire all consumers of the second transpose\u2019s output (by name or object) to the kept tensor.</li> <li>Update graph/model outputs and the var\u2192value map so no reference points at removed nodes.</li> <li>Delete nodes in reverse order (second transpose first), maintaining any live list mirrors (<code>graph.nodes</code>, <code>graph._nodes</code>, etc.).</li> </ul> <p>This pass is intentionally conservative, portable across <code>onnx_ir</code> variants, and oblivious to specific operator semantics.</p>"},{"location":"developer_guide/advanced_topics/ir_optimizer/#identity-reshape-removal","title":"Identity Reshape Removal","text":"<p>Pattern <code>Reshape(x, shape)</code> where <code>shape</code> is a constant that exactly matches <code>x</code>\u2019s known dimensions.</p> <p>Condition - The <code>shape</code> input is a constant tensor with no <code>-1</code> or <code>0</code> entries. - Every dimension of <code>x</code> is statically known and equal to the requested target. - Output metadata (if present) already reflects the same shape.</p> <p>Effect Rewire consumers of the Reshape output directly to the input and drop the node. Any now-unused shape initializers are left for later dead-code removal passes.</p> <p>This trims redundant layout annotations generated by higher-level conversions (e.g., Equinox attention blocks) without touching dynamic reshape cases.</p>"},{"location":"developer_guide/advanced_topics/ir_optimizer/#authoring-new-passes","title":"Authoring new passes","text":"<ul> <li>Keep logic IR-only\u2014never import ONNX protobuf utilities.</li> <li>Verify mutations persist across in the graph (<code>graph</code>).</li> <li>Use <code>graph</code> directly as a node container.</li> <li>Avoid creating unnecessary helper functions. Prefer builtin IR methods instead.</li> <li>Add focused regression tests under <code>tests/extra_tests/framework/</code>.</li> <li>Document the new rule here and reference the guide from <code>architecture.md</code>.</li> </ul>"},{"location":"developer_guide/advanced_topics/ir_reflection_guidelines/","title":"IR Reflection &amp; Typed API Guidelines","text":"<p>These notes capture the durable lessons from the reflection cleanup completed in PR 96. Use them when extending the converter, optimizer, or plugin helpers so we stay aligned with the typed <code>onnx_ir</code> APIs and keep future audits small.</p>"},{"location":"developer_guide/advanced_topics/ir_reflection_guidelines/#cleanup-workflow","title":"Cleanup Workflow","text":"<ol> <li>Introduce or reuse typed utility helpers (for example <code>_maybe_aval</code>, <code>_maybe_dtype</code>) and migrate one module at a time instead of broad rewrites.  </li> <li>Run <code>scripts/audit_ir_dynamic_access.py</code> after each sweep. Classify remaining hits as either expected (document why) or needs follow-up.  </li> <li>Re-run <code>mypy</code> together with a focused <code>pytest</code> slice before marking the sweep complete.</li> </ol> <p>Document ONNX proto shims that still require reflection (<code>plugins/_post_check_onnx_graph.py</code>, <code>plugins/_patching.py</code>) so the audit trail stays actionable.</p>"},{"location":"developer_guide/advanced_topics/ir_reflection_guidelines/#core-rules","title":"Core Rules","text":"<ul> <li>Typed APIs first \u2014 Treat <code>ir.Attr</code>, <code>ir.Value</code>, and <code>ir.Graph</code> as immutable, typed objects. Avoid <code>getattr</code>, private field mutation, or redundant <code>None</code> guards; rely on the documented accessors instead.</li> <li>Use built-in graph iterators \u2014 Iterate via <code>graph.nodes</code>, <code>graph.all_nodes()</code>, <code>value.consumers()</code>, <code>value.producer()</code>, and <code>ir.convenience.replace_all_uses_with</code> instead of constructing custom maps.</li> <li>When working with <code>onnx_ir</code> graphs, prefer <code>graph.all_nodes()</code> so nested functions and control-flow subgraphs are traversed with the typed iterator. Only fall back to the ONNX proto mirrors (<code>function.node</code>) when you truly need the raw proto objects.</li> <li>Reuse shape &amp; dtype helpers \u2014 Use <code>onnx_ir.Shape.is_unknown_dim</code>, <code>ir.Shape(...)</code>, and <code>value.dtype</code> rather than cloning dtype/shape logic manually.</li> <li>Prefer existing passes \u2014 Before adding bespoke optimizations, check the available ONNX Script passes (e.g., <code>fold_constants.FoldConstantsPass</code>, <code>LiftConstantsToInitializersPass</code>) and the IR optimizer docs.</li> <li>Keep helpers focused &amp; typed \u2014 Provide clear function signatures (e.g., <code>_attribute_iter(node: ir.Node) -&gt; Iterable[ir.Attr]</code>) and avoid over-engineered utilities. Document non-obvious helpers.</li> <li>Lean on <code>onnx_ir</code> containers \u2014 Use live <code>Attributes</code>, <code>Functions</code>, and other containers instead of proto mirrors; drop shims now that typed APIs are universal.</li> <li>Graph rewrites \u2014 When eliminating nodes (e.g., identity removal), check <code>Value.is_graph_input/output</code>, rename the surviving value, update <code>graph.outputs</code> with <code>Value</code> objects, call <code>ir.convenience.replace_all_uses_with</code>, and remove the node via <code>graph.remove(..., safe=True)</code>.</li> <li>Annotate aggressively \u2014 Keep explicit type annotations in converter and plugin code so mypy enforces alignment with IR interfaces.</li> </ul>"},{"location":"developer_guide/advanced_topics/linen_support/","title":"Enabling Flax Linen Support in jax2onnx","text":"<p>This document details the work done to enable support for Flax Linen models in <code>jax2onnx</code>.</p>"},{"location":"developer_guide/advanced_topics/linen_support/#1-the-challenge","title":"1. The Challenge","text":"<p>Initial attempts to convert Flax Linen models using <code>jax2onnx</code> failed with a <code>TypeError</code> related to JAX tracing:</p> <pre><code>TypeError: Shapes must be 1D sequences of concrete values of integer type, got (JitTracer(int32[]), 16)\n</code></pre> <p>This error occurred because <code>jax2onnx</code> activates a set of \"plugins\" that monkey-patch JAX primitives to capture the ONNX graph. One specific plugin, <code>JnpShapePlugin</code>, was monkey-patching <code>jnp.shape</code>. This interfered with Flax Linen's internal initialization tracing (specifically within <code>flax.linen.Dense</code>), where it relies on <code>jnp.shape</code> returning concrete values to determine kernel shapes. The monkey patch caused it to return a <code>JitTracer</code> instead, leading to the failure.</p>"},{"location":"developer_guide/advanced_topics/linen_support/#2-the-fix","title":"2. The Fix","text":"<p>The solution was to disable the monkey-patching of <code>jnp.shape</code> within the <code>JnpShapePlugin</code>.</p> <p>File: <code>jax2onnx/plugins/jax/numpy/shape.py</code></p> <pre><code>class JnpShapePlugin(PrimitiveLeafPlugin):\n    # ...\n    def get_patch_params(self):\n        # We generally DO NOT want to patch jnp.shape logic because it\n        # breaks internal helper logic in libraries like Flax that rely\n        # on shape inspection during init.\n        # Returning an empty list disables the monkey patch.\n        return [] \n</code></pre> <p>By ensuring <code>jnp.shape</code> behaves natively during tracing, Flax Linen can correctly resolve shapes during its initialization phase, allowing the rest of the <code>jax2onnx</code> conversion process to proceed normally using JAXPR tracing.</p>"},{"location":"developer_guide/advanced_topics/linen_support/#3-nnx-bridge-and-rng-handling","title":"3. NNX Bridge and RNG Handling","text":"<p>When Linen models are bridged into NNX during tracing, <code>nnx.Rngs</code> state must not mutate across trace levels. We handle this in <code>jax2onnx/plugins/flax/test_utils.py</code> by converting <code>nnx.Rngs</code> to a raw PRNG key (e.g., <code>rngs[\"params\"].key.value</code>) and wrapping the model call to pass that key explicitly. This avoids the <code>TraceContextError</code> raised by <code>RngCount</code> mutations during <code>jax.make_jaxpr</code>.</p> <p>If you see RNG-related trace errors, use the <code>linen_to_nnx</code> helper rather than instantiating <code>bridge.ToNNX</code> directly.</p>"},{"location":"developer_guide/advanced_topics/linen_support/#4-graph-optimization-gemm-bias-fusion","title":"4. Graph Optimization: Gemm Bias Fusion","text":"<p>Flax Linen's <code>Dense</code> layer decomposes into a dot product followed by a bias addition. In the raw ONNX export, this appeared as: 1.  <code>Gemm</code> (Input * Kernel) 2.  <code>Reshape</code> (Reshaping bias vector) 3.  <code>Add</code> (Adding reshaped bias to Gemm output)</p> <p>To produce a cleaner and more efficient ONNX model, we implemented a custom IR optimization pass, <code>fuse_gemm_bias_ir</code>, to fuse this pattern into a single <code>Gemm</code> node.</p> <p>Optimization Logic: -   Pattern Matching: Identifies <code>Add</code> nodes where one input comes from a <code>Gemm</code> node and the other is a constant bias (potentially fed through a <code>Reshape</code>). -   Fusion:     -   Extracts the bias constant.     -   Modifies the <code>Gemm</code> node to accept this bias as its 3rd input (<code>C</code>).     -   Important: Explicitly removes the redundant <code>Add</code> node AND the intermediate <code>Reshape</code> node to prevent disconnected nodes from lingering in the graph. -   Result: A single <code>Gemm(A, B, C)</code> node.</p> <p>File: <code>jax2onnx/converter/ir_optimizations.py</code></p>"},{"location":"developer_guide/advanced_topics/linen_support/#5-testing-infrastructure","title":"5. Testing Infrastructure","text":"<p>To ensure ongoing support and prevent regressions, we integrated Linen examples into the <code>jax2onnx</code> test suite.</p> <p>Location: <code>jax2onnx/plugins/examples/linen/</code></p> <p>We introduced a pattern for testing stateful Linen modules within the stateless <code>jax2onnx</code> test harness:</p>"},{"location":"developer_guide/advanced_topics/linen_support/#the-tonnx-bridge-pattern","title":"The <code>ToNNX</code> Bridge Pattern","text":"<p>Since <code>jax2onnx</code> tests typically expect a simple stateless callable, we bridge Linen modules into NNX and run a one-time lazy init with a dummy input.</p> <pre><code>import jax.numpy as jnp\nfrom flax import nnx\nfrom flax.nnx import bridge\n\ndef linen_to_nnx(module_cls, input_shape=(1, 32), dtype=jnp.float32, rngs=None, **kwargs):\n    module = module_cls(**kwargs)\n    model = bridge.ToNNX(module, rngs=rngs)\n    dummy_x = jnp.zeros(input_shape, dtype=dtype)\n    if isinstance(rngs, nnx.Rngs):\n        if \"params\" in rngs:\n            rngs = rngs[\"params\"].key.value\n        elif \"default\" in rngs:\n            rngs = rngs[\"default\"].key.value\n        else:\n            raise ValueError(\"NNX RNGs must define a 'params' or 'default' stream.\")\n    if rngs is None:\n        model.lazy_init(dummy_x)\n        return model\n    model.lazy_init(dummy_x, rngs=rngs)\n    return lambda *args, **kwargs: model(*args, rngs=rngs, **kwargs)\n</code></pre>"},{"location":"developer_guide/advanced_topics/linen_support/#registration","title":"Registration","text":"<p>We use <code>register_example</code> to expose the test case to the test generator (<code>tests/t_generator.py</code>).</p> <pre><code>register_example(\n    component=\"LinenDense\",\n    context=\"examples.linen\",\n    testcases=[{\n        \"testcase\": \"simple_linen_dense\",\n        \"callable\": construct_and_call(\n            linen_to_nnx,\n            module_cls=SimpleDense,\n            input_shape=(1, 32),\n            dtype=with_requested_dtype(),\n            rngs=with_rng_seed(0),\n            features=16,\n        ),\n        \"input_shapes\": [(1, 32)],\n    }]\n)\n</code></pre>"},{"location":"developer_guide/advanced_topics/linen_support/#6-usage-example","title":"6. Usage Example","text":"<p>Here is a minimal script to convert a Linen model to ONNX:</p> <pre><code>import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\nfrom jax2onnx import to_onnx\n\nclass MyModel(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        x = nn.Dense(16)(x)\n        return nn.relu(x)\n\n# 1. Initialize Model\nmodel = MyModel()\nkey = jax.random.PRNGKey(0)\ndummy_input = jnp.ones((1, 32))\nvariables = model.init(key, dummy_input)\n\n# 2. Define functional wrapper\ndef apply_fn(x):\n    return model.apply(variables, x)\n\n# 3. Convert to ONNX\nonnx_model = to_onnx(\n    apply_fn,\n    inputs=[dummy_input], # or [jax.ShapeDtypeStruct((1,32), jnp.float32)]\n    model_name=\"my_linen_model\"\n)\n\n# 4. Save\nwith open(\"my_model.onnx\", \"wb\") as f:\n    f.write(onnx_model.SerializeToString())\n</code></pre>"},{"location":"developer_guide/advanced_topics/onnx_function_usage/","title":"ONNX Function Decorator Guidelines","text":"<p>This guide summarizes the current behaviour and guardrails for the <code>@onnx_function</code> decorator. It complements the plugin quickstart by detailing how function reuse, namespacing, and testing conventions work in practice.</p>"},{"location":"developer_guide/advanced_topics/onnx_function_usage/#goals-and-defaults","title":"Goals and Defaults","text":"<ul> <li><code>@onnx_function</code> marks a callable as an ONNX function boundary. Each invocation   emits a <code>FunctionProto</code> in the exported model.</li> <li>By default, every call-site receives its own function instance; the node\u2019s   <code>op_type</code> mirrors the callable name, and the domain uses the <code>\"custom\"</code> namespace.</li> <li>Optional flags allow you to reuse function bodies (<code>unique=True</code>) and control the   domain prefix (<code>namespace=...</code>), without sacrificing readability in tools like   Netron.</li> </ul>"},{"location":"developer_guide/advanced_topics/onnx_function_usage/#flags","title":"Flags","text":"<pre><code>@onnx_function(unique=False, namespace=\"custom\", type=\"MyFn\")\ndef my_fn(...):\n    ...\n</code></pre> <ul> <li><code>unique=True</code> de-duplicates call-sites that share the same capture signature.</li> <li><code>namespace</code> overrides the domain prefix. When omitted, it defaults to <code>\"custom\"</code>.</li> <li><code>type</code> (preferred) or <code>name</code> (alias) overrides the human-readable base   name/op_type used for the function. When omitted, the callable\u2019s Python name   is used.</li> </ul>"},{"location":"developer_guide/advanced_topics/onnx_function_usage/#domain-naming-rules","title":"Domain Naming Rules","text":"<ul> <li>Non-unique functions use <code>{namespace}.{base}.{counter}</code>.</li> <li>Unique functions use <code>{namespace}.{base}.unique</code> for the first instance and   <code>{namespace}.{base}.unique.{N}</code> for additional variants when the capture differs.</li> <li><code>op_type</code> equals the sanitized base name (<code>type</code>/<code>name</code> when provided,   otherwise the callable name) so node \u201ctypes\u201d stay human-friendly.</li> </ul> <p>Examples (default namespace):</p> Setting First FunctionProto Domain Second Variant unique=False <code>custom.MyBlock.1</code> <code>custom.MyBlock.2</code> unique=True <code>custom.MyBlock.unique</code> <code>custom.MyBlock.unique.2</code> <p>Custom namespace:</p> <pre><code>@onnx_function(unique=True, namespace=\"my.model\")\ndef square(...):\n    ...\n</code></pre> <p>Produces <code>domain=\"my.model.square.unique\"</code> for all reused call-sites.</p>"},{"location":"developer_guide/advanced_topics/onnx_function_usage/#reuse-mechanics","title":"Reuse Mechanics","text":"<ul> <li>Function identity considers:</li> <li>Qualified target name.</li> <li>Input shapes/dtypes from the parent equation.</li> <li>Captured parameters. For classes, we fingerprint the module state via     <code>jax.tree_util.tree_flatten</code>.</li> <li>When <code>unique=True</code>, call-sites with identical captures share the same   <code>FunctionProto</code>; otherwise each cooperative invocation gets its own domain suffix.</li> </ul>"},{"location":"developer_guide/advanced_topics/onnx_function_usage/#testing-examples","title":"Testing &amp; Examples","text":"<ul> <li>The regression suite lives at   <code>tests/extra_tests/converter/test_onnx_function_unique.py</code>.   It covers duplicate reuse, distinct captures, and custom namespaces.</li> <li>Example <code>onnx_functions_017</code> demonstrates two call-sites sharing a unique function.</li> <li>Regenerate fixtures after behaviour changes:   <code>bash   poetry run python scripts/generate_tests.py</code></li> </ul>"},{"location":"developer_guide/advanced_topics/onnx_function_usage/#best-practices","title":"Best Practices","text":"<ul> <li>Use <code>construct_and_call(...).with_requested_dtype(...).with_rng_seed(...)</code> in   metadata so tests can rebuild deterministic f32/f64 variants.</li> <li>Keep <code>post_check_onnx_graph</code> expectations focused on meaningful structural   checks. Function selectors accept either the exact <code>domain:name</code> or a domain   prefix (e.g. <code>\"custom.MyFn.1\"</code> matches <code>\"custom.MyFn.1:MyFn\"</code>).</li> <li>When migrating existing decorators, ensure no conflicting namespace choices are   applied to the same target; the decorator raises if mixed namespacing is detected.</li> </ul>"},{"location":"developer_guide/advanced_topics/onnx_ir_builder/","title":"ONNX IR Builder Guide","text":"<p>This guide distills the guardrails we enforce around <code>onnx_ir._tape.Builder</code>: how to wire values, record initializers, and keep tests green now that the IR pipeline is builder-first.</p>"},{"location":"developer_guide/advanced_topics/onnx_ir_builder/#policy-checklist","title":"Policy Checklist","text":"<ul> <li>Always pass <code>name=</code> when calling <code>builder.initializer(...)</code> or <code>ctx.builder.add_initializer_from_*</code>. <code>tests/extra_tests/framework/test_no_onnx_in_converter_plugins.py</code> verifies this.</li> <li><code>_outputs</code> must be a list/tuple (or alias that resolves to one); string literals are rejected by <code>tests/extra_tests/framework/test_ir_builder_contracts.py</code> and <code>scripts/check_ir_builder_usage.py</code>.</li> <li>Keep converter/plugins IR-only\u2014no <code>onnx</code> protobuf helpers\u2014per the same policy suite.</li> <li>Run <code>scripts/check_ir_builder_usage.py</code> before sending patches (it is also wired into the pre-commit stack).</li> </ul>"},{"location":"developer_guide/advanced_topics/onnx_ir_builder/#quick-checklist","title":"Quick Checklist","text":"<ul> <li>Emit ops through <code>ctx.builder</code> (or <code>_tape.Builder</code>) rather than constructing <code>ir.Node</code> manually. Fall back only in function-mode/legacy paths where the builder cannot express the behaviour.</li> <li>After every builder call, stamp dtype and shape with <code>_stamp_type_and_shape(...)</code> and run <code>_ensure_value_metadata(...)</code> so the <code>ir.Value</code> carries normalized shape/type metadata (no separate <code>value_info</code> bucket).</li> <li>Register constants via <code>builder.initializer(...)</code> / <code>ctx.bind_const_for_var(...)</code>; never smuggle tensors through ad-hoc <code>ir.Value(const_value=...)</code> without keeping the initializer list in sync.</li> <li>When defining plugin metadata, use <code>construct_and_call(...).with_requested_dtype(...).with_rng_seed(...)</code> to honour the single-use RNG policy and keep tests deterministic across f32/f64 runs.</li> <li>Run the validation hooks listed below (Ruff, builder usage checker, pytest) before landing a change; the pre-commit stack invokes them automatically.</li> </ul>"},{"location":"developer_guide/advanced_topics/onnx_ir_builder/#plugin-metadata-requirements","title":"Plugin Metadata Requirements","text":"<ul> <li>Construct callable metadata with <code>construct_and_call(...)</code> so the test harness can rebuild modules for each dtype. Pair it with <code>with_requested_dtype()</code> and <code>with_rng_seed(...)</code>/<code>with_prng_key(...)</code> helpers instead of inlining lambdas or seeding at import time.</li> <li>Avoid <code>callable_factory</code>. The test generator now raises if metadata still relies on factories\u2014<code>callable</code> entries must be concrete <code>construct_and_call(...)</code> results.</li> <li>When you need constants inside plugin lowers, prefer shared helpers (for example, <code>_const_i64</code>) that delegate to <code>ctx.builder</code> so they participate in initializer bookkeeping.</li> <li>Respect the single-use RNG rule: split keys per consumer and never cache module instances inside traced calls\u2014<code>construct_and_call(...).with_dtype(...)</code> already handles per-dtype reuse.</li> </ul>"},{"location":"developer_guide/advanced_topics/onnx_ir_builder/#validation-hooks","title":"Validation Hooks","text":"<ul> <li><code>tests/extra_tests/framework/test_no_onnx_in_converter_plugins.py</code> enforces both the \"no protobuf\" policy and initializer naming for every builder call.</li> <li><code>tests/extra_tests/framework/test_ir_builder_contracts.py</code> walks the AST to guarantee <code>_outputs=</code> uses sequence types.</li> <li><code>scripts/check_ir_builder_usage.py</code> wraps the same heuristics for local iteration and runs as a pre-commit hook. Invoke it manually with <code>poetry run python scripts/check_ir_builder_usage.py</code> when editing converter/plugins code.</li> </ul> <p>Everything below expands on the why and how behind those rules.</p>"},{"location":"developer_guide/advanced_topics/onnx_ir_builder/#prerequisites-and-imports","title":"Prerequisites and Imports","text":"<ul> <li>The ONNX IR package ships with ONNX Script and is available as <code>onnx_ir</code>; install <code>onnx-script</code> or <code>onnx-ir</code> and ensure runtime dependencies (notably <code>numpy</code>) are available.</li> <li>When working from a source checkout, set <code>PYTHONPATH=src</code> before importing.</li> <li>The builder lives in an internal module. Import it explicitly:</li> </ul> <pre><code>import onnx_ir as ir\nfrom onnx_ir._tape import Builder\n</code></pre> <p>Stability note: <code>_tape.Builder</code> is currently internal API (the leading underscore is intentional) and can change. Keep the wrapper that instantiates it confined to XY so updates are easy.</p> <p>Legacy note: The converter no longer maintains a <code>builder.value_info</code> list. Plugins should rely exclusively on <code>_ensure_value_metadata(...)</code> and the fields on each <code>ir.Value</code> when they need shape/type information. Avoid appending to or expecting a global <code>value_info</code> registry.</p>"},{"location":"developer_guide/advanced_topics/onnx_ir_builder/#core-concept","title":"Core Concept","text":"<p><code>Builder</code> subclasses <code>onnx_ir.tape.Tape</code>. It records nodes, initializers, and the opsets they require while exposing every ONNX operator as a dynamic method (for example, <code>builder.Add</code>, <code>builder.Conv</code>).</p> <p>Use it when you want to script graph construction but still hand the collected nodes to <code>ir.Graph</code> or <code>ir.Function</code> later. If you need finer-grained control (custom outputs, metadata, overload selection, or pre-existing <code>ir.Value</code> objects), drop down to <code>Tape.op</code> / <code>Tape.op_multi_out</code> or construct <code>ir.Node</code> directly.</p>"},{"location":"developer_guide/advanced_topics/onnx_ir_builder/#end-to-end-workflow","title":"End-to-End Workflow","text":"<pre><code>import numpy as np\nimport onnx_ir as ir\nfrom onnx_ir._tape import Builder\n\n# 1. Provide typed graph values up front.\nX = ir.val(\"X\", dtype=ir.DataType.FLOAT, shape=[1])\nY = ir.val(\"Y\", dtype=ir.DataType.FLOAT, shape=[1])\n\n# 2. Create a builder (optionally tie it to an existing graph/function).\nbuilder = Builder()\n\n# 3. Register any constant tensors through the builder so outputs stay in sync.\nweight_init = builder.initializer(\n    ir.tensor(np.array([0.25], dtype=np.float32)),\n    name=\"weight\",\n)\n\n# 4. Emit operators. Positional args become inputs; keyword args become ONNX attributes.\nscaled = builder.Mul(X, weight_init, _outputs=[\"scaled\"])  # returns ir.Value\nsummed = builder.Add(scaled, Y, _domain=\"\", _version=18)\n\n# 5. Package the recording into a graph/model when ready.\ndef to_opset_imports(used_opsets: set[tuple[str, int | None]]):\n    result: dict[str, int] = {}\n    for domain, version in used_opsets:\n        if version is None:\n            continue  # fall back to the containing graph's default\n        previous = result.get(domain)\n        if previous is not None and previous != version:\n            raise ValueError(\n                f\"Mixed opset versions requested for domain '{domain}': {previous} vs {version}\"\n            )\n        result[domain] = version\n    return result or {\"\": 18}  # choose an explicit default for the model\n\ngraph = ir.Graph(\n    inputs=[X, Y],\n    outputs=[summed],\n    nodes=builder.nodes,\n    initializers=builder.initializers,\n    opset_imports=to_opset_imports(builder.used_opsets),\n    name=\"scale_and_sum\",\n)\nmodel = ir.Model(graph=graph, ir_version=10)\n</code></pre>"},{"location":"developer_guide/advanced_topics/onnx_ir_builder/#bringing-existing-models-into-the-builder","title":"Bringing Existing Models Into the Builder","text":"<p>The official docs highlight converting <code>onnx.ModelProto</code> to the IR via <code>ir.from_proto</code> or <code>onnx_ir.load</code>. That makes it easy to combine scripted nodes with imported graphs:</p> <pre><code>import onnx\nimport onnx_ir as ir\nfrom onnx_ir._tape import Builder\n\nmodel_proto = onnx.parser.parse_model(MODEL_TEXT)\nmodel = ir.from_proto(model_proto)\n\nbuilder = Builder(model.graph)\nextra = builder.Identity(model.graph.outputs[0])\nmodel.graph.outputs.append(extra)\n</code></pre> <p>You can reverse the process with <code>ir.to_proto(model)</code> when you need to serialize back to protobuf.</p>"},{"location":"developer_guide/advanced_topics/onnx_ir_builder/#what-the-builder-does-for-you","title":"What the Builder Does for You","text":"<ul> <li>Tracks every created <code>ir.Node</code> in insertion order via <code>builder.nodes</code> so you can extend a graph or build a new one.</li> <li>Keeps initializers created through <code>builder.initializer</code> aligned with the eventual graph. When the builder is constructed with <code>graph_like=ir.Graph(...)</code>, the initializer is immediately registered on that graph.</li> <li>Records <code>builder.used_opsets</code> as <code>(domain, version)</code> pairs so you can populate <code>Graph.opset_imports</code> consistently.</li> </ul>"},{"location":"developer_guide/advanced_topics/onnx_ir_builder/#reserved-keyword-arguments","title":"Reserved Keyword Arguments","text":"<p><code>Builder</code> intercepts a few keyword arguments before treating the remainder as ONNX attributes: - <code>_domain</code>: operator domain (default <code>\"\"</code>). - <code>_version</code>: opset version for the operator. Use one consistent value per domain. - <code>_outputs</code>: either an <code>int</code> (number of outputs) or a sequence of output names.   - When you pass a sequence, make it a list/tuple of strings; plain strings count as sequences of characters and will be split unintentionally.</p> <p>Everything else in <code>**kwargs</code> is fed to <code>_convenience.convert_attributes</code>, which automatically turns Python scalars, sequences, tensors, and graphs into the right <code>ir.Attr</code> instances.</p>"},{"location":"developer_guide/advanced_topics/onnx_ir_builder/#tape-api-highlights","title":"Tape API Highlights","text":"<p>The public documentation for <code>onnx_ir.tape</code> at https://onnx.ai/ir-py/api/ir_tape.html spells out the signatures for <code>Tape.op</code>, <code>Tape.op_multi_out</code>, and <code>Tape.initializer</code>: - <code>Tape.op</code> returns the first output <code>ir.Value</code> and accepts keyword-only arguments such as <code>overload</code>, <code>graph</code>, <code>name</code>, <code>doc_string</code>, <code>metadata_props</code>, and <code>output</code>. - <code>Tape.op_multi_out</code> requires either <code>num_outputs</code> or <code>outputs</code> (but not both) and returns an immutable tuple of <code>ir.Value</code> objects. - <code>Tape.initializer</code> insists on a name and on the provided tensor having <code>const_value</code> set; ONNX functions intentionally reject initializers.</p> <p>Keep these signatures in mind when deciding between builder convenience and direct tape usage.</p>"},{"location":"developer_guide/advanced_topics/onnx_ir_builder/#handling-multi-output-operators","title":"Handling Multi-Output Operators","text":"<pre><code>values = builder.If(condition, _outputs=[\"then_out\", \"else_out\"], _version=18)\nthen_out, else_out = values\n</code></pre> <ul> <li>The return type is a tuple of <code>ir.Value</code>. Pull out the node again with <code>then_out.producer()</code> if you need to mutate metadata.</li> <li>For heterogeneous arity where ONNX requires empty slots, pass <code>None</code> in the positional inputs (for example, <code>builder.MaxPool(X, None, strides=[1, 1], _outputs=2)</code>).</li> </ul>"},{"location":"developer_guide/advanced_topics/onnx_ir_builder/#managing-attributes-explicitly","title":"Managing Attributes Explicitly","text":"<ul> <li>Python types are auto-inferred. For ambiguous cases (empty lists or <code>None</code>) create the attribute yourself: <code>builder.Cast(X, to=ir.Attr(\"to\", ir.AttributeType.INT, 1))</code>.</li> <li>Tensor attributes should be created with <code>ir.tensor(...)</code> to guarantee dtype/shape correctness.</li> <li>Graph-typed attributes must be wrapped with <code>ir.AttrGraph</code> or <code>ir.AttrGraphs</code>.</li> </ul>"},{"location":"developer_guide/advanced_topics/onnx_ir_builder/#graph-ownership-cloning","title":"Graph Ownership &amp; Cloning","text":"<ul> <li><code>IRBuilder</code> now keeps its <code>inputs</code>, <code>outputs</code>, and nodes in sync with the underlying <code>onnx_ir.Graph</code> via proxy setters. Reassigning <code>builder.inputs = [...]</code> (or <code>.outputs</code>/<code>.nodes</code>) clears and repopulates the graph-side containers, while <code>builder.initializers</code> remains a list-like shim that delegates to <code>graph.initializers</code>. Prefer mutating these sequences in place, but reassignment is safe when you need to reset them.</li> <li>When exporting a staged graph\u2014either to an <code>ir.Model</code> or into ONNX graph-typed attributes\u2014clone it first using <code>jax2onnx.converter.ir_clone.clone_graph</code>. The helper copies values, initializers, metadata, and nested graphs so the detached graph can be owned by another model/function without triggering \u201cValue \u2026 is already owned by a different graph\u201d errors. Function scopes and control-flow plugins (<code>cond</code>, <code>fori_loop</code>, <code>scan</code>, <code>while_loop</code>) already adopt this pattern; follow suit for any new subgraph emission.</li> </ul>"},{"location":"developer_guide/advanced_topics/onnx_ir_builder/#integrating-with-existing-graphs-or-functions","title":"Integrating with Existing Graphs or Functions","text":"<pre><code>graph = ir.Graph(inputs=[X], outputs=[Z], nodes=[])\nbuilder = Builder(graph)\nintermediate = builder.Relu(X)\n# The node is already appended to `graph`, and names are assigned by the graph's name authority.\n</code></pre> <ul> <li>When bound to a graph, builder calls reuse the graph's naming authority and automatically respect graph invariants.</li> <li>Initializers are registered only for graphs. ONNX functions do not permit initializers, so the builder simply stores them locally when <code>graph_like</code> is an <code>ir.Function</code>.</li> </ul>"},{"location":"developer_guide/advanced_topics/onnx_ir_builder/#limitations-compared-to-tapeop","title":"Limitations Compared to <code>Tape.op</code>","text":"<p>Because <code>_make_node</code> forwards the remaining keyword arguments into the attribute map, the builder cannot set certain <code>Tape</code> parameters at construction time: - <code>overload</code>, <code>graph</code>, <code>name</code>, <code>doc_string</code>, <code>metadata_props</code>, and <code>output</code> are interpreted as attributes. Set them on the resulting node (<code>value.producer()</code>) after creation or call <code>Tape.op</code> directly when you need those parameters. - To attach a node to a different graph than <code>builder.graph_like</code>, instantiate another builder or fall back to <code>Tape.op(graph=...)</code>. - To reuse pre-created <code>ir.Value</code> outputs, call <code>Tape.op(output=existing_value)</code> or <code>Tape.op_multi_out(outputs=[...])</code> rather than relying on <code>_outputs</code>.</p>"},{"location":"developer_guide/advanced_topics/onnx_ir_builder/#common-pitfalls-and-how-to-avoid-them","title":"Common Pitfalls and How to Avoid Them","text":"<ul> <li>Node metadata via kwargs: <code>builder.Add(..., name=\"foo\")</code> creates an attribute named <code>name</code>; it does not rename the node. Use <code>summed.producer().name = \"foo\"</code> after creation instead.</li> <li>Doc strings &amp; metadata props: assign them on the node object (<code>node = summed.producer(); node.doc_string = \"...\"</code>).</li> <li>Debug provenance metadata: setting <code>JAX2ONNX_ENABLE_STACKTRACE_METADATA=1</code> (or <code>stacktrace_metadata=True</code>) records a concise call-site (<code>pkg.jax2onnx.callsite</code>, formatted as <code>function:line</code>) plus the plugin invocation site (<code>pkg.jax2onnx.plugin</code>, formatted as <code>Plugin.lower:line</code> pointing at the builder call) on each node. This is the default reduced payload surfaced in tools like Netron. Set <code>JAX2ONNX_STACKTRACE_DETAIL=full</code> when you also need the full Python (<code>pkg.jax2onnx.stacktrace</code>) and JAX (<code>pkg.jax2onnx.jax_traceback</code>) traces.</li> </ul> <p>Example (line numbers annotated to mirror the metadata):   <code>python   def wide_fn(x):       a = jnp.sin(x)   # wide_fn.py:8       b = jnp.cos(x)   # wide_fn.py:9       c = jnp.tanh(x)  # wide_fn.py:10       d = jnp.exp(x)   # wide_fn.py:11       return a + b * c + d  # wide_fn.py:12</code> - Output naming: pass a list (<code>_outputs=[\"y\"]</code>), not a bare string. - Initializer naming: provide a name whenever the tensor lacks one; <code>Tape.initializer</code> raises otherwise. - Multiple opset versions: if two builder calls request different versions for the same domain, detect and reconcile before finishing the graph. - Optional inputs: include explicit <code>None</code> placeholders to maintain positional semantics. - Attribute values of <code>None</code>: build an <code>ir.Attr</code> with an explicit <code>AttributeType</code>; automatic conversion rejects bare <code>None</code>. - Graph ownership: do not reuse a builder-generated node inside another graph without detaching it first (<code>graph.remove(node)</code>), because each node tracks its owning graph.</p>"},{"location":"developer_guide/advanced_topics/onnx_ir_builder/#initializer-deduplication","title":"Initializer Deduplication","text":"<ul> <li>Prefer <code>ctx.builder.add_initializer_from_scalar/array(...)</code> or <code>ctx.builder.const_i64(...)</code> to create constants. Avoid writing directly to <code>graph.initializers[...]</code>.</li> <li>The upstream <code>GraphInitializers.add(value)</code> overwrites by name. Our builder layer enforces a stricter policy to preserve IR value connections:</li> <li>Identical duplicate (same name + same data/shape/dtype) \u2192 reuse existing initializer; do not replace the object.</li> <li>Conflicting duplicate (same name + different payload) \u2192 raise a <code>ValueError</code>.</li> <li>In function-mode, constants are emitted as <code>Constant</code> nodes; graph initializers are not allowed in ONNX Functions.</li> </ul> <p>Example</p> <pre><code>import numpy as np\n\nw1 = builder.add_initializer_from_array(\"weight\", np.array([1.0], dtype=np.float32))\n# Re-adding with identical payload reuses the same Value (no-op):\nw2 = builder.add_initializer_from_array(\"weight\", np.array([1.0], dtype=np.float32))\nassert w1 is w2\n\n# Re-adding with different payload raises:\nbuilder.add_initializer_from_array(\"weight\", np.array([2.0], dtype=np.float32))  # ValueError\n</code></pre> <p>Rationale - Preserving object identity prevents subtle mismatches where nodes still reference the old <code>ir.Value</code> even though the <code>graph.initializers</code> dict now points to a new one. This keeps optimizer passes, cloning, and structural tests stable and predictable.</p>"},{"location":"developer_guide/advanced_topics/onnx_ir_builder/#checklist-before-serializing","title":"Checklist Before Serializing","text":"<ul> <li>All graph inputs/outputs are <code>ir.Value</code> instances with types and shapes populated (consider using <code>ir.val</code> for convenience).</li> <li>Initializers created through the builder are either registered on the target graph or injected via <code>graph.initializers.add(...)</code>.</li> <li>Duplicate policy: attempting to re-add an initializer with the same name and different data raises. Re-adding an identical initializer reuses the existing value without replacing it, preserving value connections.</li> <li><code>graph.opset_imports</code> reflects the versions implied by <code>builder.used_opsets</code>.</li> <li>Any node-level metadata (names, doc strings, annotations, overloads) is set on the node objects after creation.</li> <li>Perform optional validation such as <code>ir.to_proto(model)</code>, ONNX checker runs, or <code>onnx_ir.load</code> round-trips if XY integrates them.</li> </ul> <p>Keeping these conventions in one place ensures the \"builder\" layer stays predictable for Codex agents and humans alike, reducing churn when the upstream library evolves.</p>"},{"location":"developer_guide/advanced_topics/onnx_ir_builder/#validation-routine","title":"Validation Routine","text":"<ol> <li><code>poetry run python scripts/check_ir_builder_usage.py --diff</code> (lints only staged files; drop <code>--diff</code> to scan the whole tree).</li> <li><code>poetry run ruff check .</code> followed by <code>poetry run ruff format --check .</code> (or let the pre-commit hooks fix issues automatically).</li> <li><code>poetry run pytest -q</code> plus any focused suites you touched (for example <code>tests/primitives/test_jnp.py::Test_linspace</code>).</li> <li>For builder-heavy refactors, run the structural policy tests directly: <code>poetry run pytest -q tests/extra_tests/framework/test_ir_builder_contracts.py</code>.</li> </ol>"},{"location":"developer_guide/advanced_topics/onnx_ir_typing_how_to/","title":"Using the <code>onnx_ir</code> Type Information (PEP 561)","text":"<p>The <code>onnx_ir</code> package ships inline types and a <code>py.typed</code> marker so static type checkers know the annotations are part of the public API. Downstream projects can opt into these types without copying stubs or tweaking search paths\u2014the package already complies with PEP 561.</p>"},{"location":"developer_guide/advanced_topics/onnx_ir_typing_how_to/#what-the-pep-561-marker-gives-you","title":"What the PEP 561 marker gives you","text":"<ul> <li><code>onnx_ir</code> publishes <code>src/onnx_ir/py.typed</code>, signalling to type checkers that the runtime package contains type information.</li> <li>All exports rebind their <code>__module__</code> to <code>onnx_ir</code> (see <code>src/onnx_ir/__init__.py</code>), so hover information and error messages reference the public surface instead of private modules.</li> <li>Because the marker file is empty, the project advertises itself as \u201cfully typed\u201d. If you discover gaps, report them or mark them with <code>typing.cast</code> to keep the promise tidy.</li> </ul>"},{"location":"developer_guide/advanced_topics/onnx_ir_typing_how_to/#install-the-package-as-usual","title":"Install the package as usual","text":"<pre><code>pip install onnx-ir\n</code></pre> <p>You do not need extra stub packages. If you vendor the source, be sure to keep the <code>py.typed</code> file alongside the <code>onnx_ir</code> directory.</p> <p>For pyproject-based projects, declare the dependency in <code>pyproject.toml</code>:</p> <pre><code>[project]\ndependencies = [\n  \"onnx-ir&gt;=0.1.11\",\n]\n</code></pre> <p>Pin to at least the version you tested with so the exported types stay stable.</p>"},{"location":"developer_guide/advanced_topics/onnx_ir_typing_how_to/#configure-your-type-checker","title":"Configure your type checker","text":""},{"location":"developer_guide/advanced_topics/onnx_ir_typing_how_to/#mypy","title":"mypy","text":"<pre><code># mypy.ini\n[mypy]\npython_version = 3.11\nstrict = True\n\n[mypy-onnx_ir.*]\nignore_missing_imports = False  # default, kept for clarity\n</code></pre> <p>Run it with:</p> <pre><code>mypy src\n</code></pre> <p>Because <code>py.typed</code> lives next to the package, mypy discovers the types automatically.</p>"},{"location":"developer_guide/advanced_topics/onnx_ir_typing_how_to/#pyright-pylance","title":"Pyright / Pylance","text":"<p>Add the package to the <code>venv</code> you use for analysis. Pyright picks up the marker automatically; no <code>\"typeCheckingMode\": \"strict\"</code> toggle is required but is recommended:</p> <pre><code>// pyproject.toml or pyrightconfig.json (whichever you use)\n{\n  \"pythonVersion\": \"3.11\",\n  \"typeCheckingMode\": \"strict\",\n  \"reportMissingTypeStubs\": \"warning\"\n}\n</code></pre>"},{"location":"developer_guide/advanced_topics/onnx_ir_typing_how_to/#other-tools","title":"Other tools","text":"<ul> <li>Pytype: as long as <code>onnx_ir</code> is importable, Pytype reads the inline annotations.</li> <li>Ruff (<code>ruff check --select PYI</code>): uses the same metadata when linting.</li> </ul>"},{"location":"developer_guide/advanced_topics/onnx_ir_typing_how_to/#working-with-the-api-in-a-typed-codebase","title":"Working with the API in a typed codebase","text":""},{"location":"developer_guide/advanced_topics/onnx_ir_typing_how_to/#constructing-a-model","title":"Constructing a model","text":"<pre><code>from __future__ import annotations\n\nfrom onnx_ir import Model, Node, tensor, val\n\n\ndef build_constant_add() -&gt; Model:\n    weight = tensor([1.0, 2.0, 3.0], name=\"weight\")\n    bias = tensor([0.1, 0.2, 0.3], name=\"bias\")\n    add_node: Node = Node(\n        op_type=\"Add\",\n        inputs=[weight.value, bias.value],\n        outputs=[val(\"sum\", elem_type=weight.type.elem_type, shape=weight.type.shape)],\n    )\n    return Model.from_nodes([add_node])\n</code></pre> <p>Key points:</p> <ul> <li><code>tensor()</code>, <code>node()</code>, and <code>val()</code> return fully typed objects so attribute access is safe.</li> <li>Structural protocols such as <code>TensorProtocol</code> can type-annotate third-party tensor objects accepted by the API.</li> </ul>"},{"location":"developer_guide/advanced_topics/onnx_ir_typing_how_to/#leveraging-protocols-for-interop","title":"Leveraging protocols for interop","text":"<pre><code>from typing import Iterable\n\nfrom onnx_ir import TensorProtocol\n\n\ndef unwrap_tensor(tensor: TensorProtocol) -&gt; Iterable[float]:\n    return list(tensor.tolist())\n</code></pre> <p>If you pass a NumPy array that implements <code>TensorProtocol</code>, the checker confirms compatibility without extra casts.</p>"},{"location":"developer_guide/advanced_topics/onnx_ir_typing_how_to/#best-practices-for-downstream-libraries","title":"Best practices for downstream libraries","text":"<ul> <li>Adopt future annotations: add <code>from __future__ import annotations</code> at the top of new modules to reduce runtime typing overhead.</li> <li>Preserve type information: if you re-export <code>onnx_ir</code> objects, rebind them in your own <code>__all__</code> so IDEs surface the right module path.</li> <li>Avoid untyped opt-outs: instead of <code>type: ignore[assignment]</code>, prefer <code>typing.cast</code> with explanatory comments; this keeps you aligned with the library\u2019s declared typing coverage.</li> <li>Test your types: run <code>mypy --strict</code> or <code>pyright --level strict</code> in CI to detect API changes early when you upgrade <code>onnx_ir</code>.</li> </ul>"},{"location":"developer_guide/advanced_topics/onnx_ir_typing_how_to/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Checker cannot find the package: confirm the environment where the checker runs has <code>onnx_ir</code> installed\u2014<code>python -m site</code> helps locate <code>site-packages</code>.</li> <li>Marker missing after vendoring: ensure <code>onnx_ir/py.typed</code> ships with your wheel or editable install. Without it, consumers see \u201cMissing type stubs\u201d warnings even though annotations exist.</li> <li>Encountering <code>Any</code> leaks: file an issue with a minimal example; the maintainers mark problematic sections using <code>typing.cast</code> or <code>Protocol</code> adjustments rather than dropping the marker.</li> </ul>"},{"location":"developer_guide/advanced_topics/onnx_ir_typing_how_to/#further-reading","title":"Further reading","text":"<ul> <li>PEP 561 \u2013 Distributing and Packaging Type Information</li> <li>Typing best practices in the Python docs</li> <li>mypy configuration reference</li> </ul>"},{"location":"developer_guide/advanced_topics/subgraph_input_handling/","title":"Control-Flow Subgraph Inputs (ONNX)","text":"<p>Our IR-only pipeline still emits ONNX control-flow nodes, so plugin authors and optimizer work must respect the schema-defined subgraph contracts. Use this guide when touching <code>converter/ir_context.py</code>, the control-flow plugins, or the structural tests that verify body graphs (for example <code>tests/extra_tests/scan/</code>).</p>"},{"location":"developer_guide/advanced_topics/subgraph_input_handling/#tldr","title":"TL;DR","text":"<ul> <li><code>If</code> exposes exactly one explicit input (<code>cond</code>). Branch graphs declare zero formal inputs and capture any extra tensors by name from the parent scope.</li> <li><code>Loop</code> forwards everything explicitly: <code>iteration_num</code>, <code>condition</code>, and N loop-carried values are provided to the body each iteration. Thread outer tensors through the carried tuple when they are not constants.</li> <li><code>Scan</code> mirrors <code>Loop</code>, adding per-step slices for each scan input sequence. Body inputs are ordered as state variables followed by sequence slices; nothing is implicitly captured.</li> </ul>"},{"location":"developer_guide/advanced_topics/subgraph_input_handling/#if-implicit-capture-only","title":"<code>If</code>: implicit capture only","text":"<p>The ONNX <code>If</code> node accepts a single boolean input. Both <code>then_branch</code> and <code>else_branch</code> Graph attributes must list zero inputs, so any tensor needed inside the branch is referenced directly by name and resolved from the enclosing graph. Exporters that try to add branch parameters fail schema validation (<code>input size of if-op should be 1</code>). Our lowering honours that contract\u2014branch builders leave the input list empty and rely on <code>IRContext</code> to resolve captured values.</p>"},{"location":"developer_guide/advanced_topics/subgraph_input_handling/#loop-explicit-interface","title":"<code>Loop</code>: explicit interface","text":"<p><code>Loop</code> nodes receive an optional trip-count <code>M</code>, an optional initial condition, and N loop-carried initial values. Consequently, the body graph declares <code>iteration_num</code>, <code>condition</code>, and those N carried values as explicit inputs. Each iteration returns a continuation flag, the updated carried values, and optional scan outputs that map back to the parent node. When the body needs an outer tensor that is not constant, add it to the carried tuple (passing it through unchanged if necessary). This keeps dependencies explicit and aligns with ONNX Runtime\u2019s validation. Our <code>fori_loop</code> and <code>scan</code> plugins rely on this ordering when constructing body graphs.</p>"},{"location":"developer_guide/advanced_topics/subgraph_input_handling/#scan-explicit-interface-with-sequence-slices","title":"<code>Scan</code>: explicit interface with sequence slices","text":"<p><code>Scan</code> behaves like <code>Loop</code> with additional scan inputs. The body graph lists N state variables followed by M per-iteration slices\u2014one from each scan input tensor. It yields updated state plus K scan outputs that the runtime stacks. Every non-constant value must arrive through these inputs; there is no implicit capture. Thread outer tensors as state variables if you need them on each step. Tests under <code>tests/extra_tests/scan/</code> assert these invariants so regressions surface quickly.</p>"},{"location":"developer_guide/advanced_topics/subgraph_input_handling/#practical-tips","title":"Practical tips","text":"<ul> <li><code>IRContext.get_value_for_var</code> is responsible for materialising captured tensors. Keep its literal handling consistent with these rules.</li> <li>When authoring tests, prefer <code>expect_graph</code> assertions on body graphs to ensure input/output arity matches the spec.</li> <li>If ONNX adjusts the schemas, update the converter code and this guide together so plugin authors retain a single source of truth.</li> </ul>"},{"location":"developer_guide/advanced_topics/subgraph_input_handling/#constants-inside-subgraphs-no-initializers","title":"Constants inside subgraphs (no initializers)","text":"<ul> <li>ONNX Functions and control\u2011flow subgraphs must not contain graph initializers. All constants inside <code>Loop</code>/<code>Scan</code> bodies or Function graphs are emitted as <code>Constant</code> nodes.</li> <li>Our converter enforces this by running subgraph construction in \u201cfunction mode\u201d, which makes builder initializer helpers produce <code>Constant</code> nodes instead of registering initializers on the body graph.</li> <li>Plugin authors should always use <code>ctx.builder.add_initializer_from_*</code> or <code>ctx.bind_const_for_var(...)</code> for constants so the correct form is emitted automatically in subgraphs. Avoid writing to any <code>_initializers</code> lists directly.</li> <li>Post\u2011processing loosens shapes inside subgraphs: value shapes in Loop/Scan bodies are set to rank\u2011only (all dims unknown) to reduce schema friction and improve portability. Structural tests assert this behaviour.</li> </ul>"},{"location":"developer_guide/advanced_topics/typing_improvements/","title":"Typing Improvements Roadmap","text":"<p>Long-running effort to keep the converter + plugin stack type-safe under mypy. This mirrors the guardrails in <code>AGENTS.md</code> (PRNG discipline, ONNX-only IR builders) and keeps metadata/regressions visible earlier in CI.</p>"},{"location":"developer_guide/advanced_topics/typing_improvements/#goals","title":"Goals","text":"<ul> <li>Strengthen static typing in converter + plugin subsystems so shape/dtype drift,   PRNG misuse, and metadata mismatches fail fast.</li> <li>Use shared protocols/helpers (<code>LoweringContextProtocol</code>, <code>SymbolicDimOrigin</code>,   <code>PrimitiveLowering</code>, <code>FunctionLowering</code>, etc.) instead of ad-hoc <code>dict</code>/<code>Any</code>.</li> <li>Keep strict mypy coverage rolling forward via <code>scripts/check_typing.sh</code>.</li> </ul>"},{"location":"developer_guide/advanced_topics/typing_improvements/#strategy","title":"Strategy","text":"<ol> <li>Review mypy config regularly and identify the next converter/plugin hot paths    to bring under strict overrides.</li> <li>Introduce shared typed helpers for IR builders, lowering contexts, and PRNG    metadata so new modules inherit concrete signatures.</li> <li>Tighten mypy settings (per-package strict overrides + CI helper) so coverage    ratchets forward without regressing performance.</li> <li>Annotate prioritized modules, land the strict override, and resolve new typing    errors immediately.</li> </ol>"},{"location":"developer_guide/advanced_topics/typing_improvements/#completed-milestones","title":"Completed Milestones","text":"<ul> <li>mapped the converter/plugin modules most affected by stricter mypy in   <code>pyproject.toml</code>.</li> <li>Added shared typing helpers (<code>SymbolicDimOrigin</code>, <code>LoweringContextProtocol</code>,   <code>AxisOverrideInfo</code>, <code>RngTrace</code>, <code>PrimitiveLowering</code>, <code>FunctionLowering</code>) and   migrated converter + registry infrastructure to use them.</li> <li>Verified the converter no longer touches ONNX protobufs directly; IR-only flow   stays intact after removing the old serde shim.</li> <li>Hardened typing around the plugin registry + Issue52 fixtures/sandboxes; the   scatter/broadcast/loop-concat helpers exercise the new protocols.</li> <li>Added <code>scripts/check_typing.sh</code> so CI runs <code>poetry run mypy --config-file   pyproject.toml</code> with the expanded target set; wired in <code>scripts/report_rng_traces.py</code>   so RNG helpers stay visible.</li> <li>Simplified mypy config so <code>files</code> tracks converter/plugins/tests roots while strict   overrides gate the curated set; <code>scripts/check_typing.sh</code> remains fast.</li> <li>Extended strict coverage across high-traffic numpy/nn/random/attention plugins   (reshape/tile/einsum/stack/split/take/transpose/squeeze/select/prod/outer/sort,   activations, RNG primitives, attention shims, etc.) by annotating lowers, binding   specs, and batch rules with shared protocols.</li> <li>Brought image/linear/conv/layer_norm/equinox helpers under strict overrides so   conversion guardrails also apply to Equinox modules.</li> </ul>"},{"location":"user_guide/api/","title":"API Reference","text":""},{"location":"user_guide/api/#jax2onnx.user_interface","title":"<code>jax2onnx.user_interface</code>","text":"<p>User-facing API for converting JAX functions and models to ONNX.</p> <p>This module provides the primary interface for exporting JAX/Flax models to the ONNX format. It supports dynamic shapes, runtime parameters, and numerical validation against ONNX Runtime.</p> <p>Key Functions:</p> <ul> <li>to_onnx: Convert a JAX function or Flax module to an ONNX model.</li> <li>onnx_function: Decorator to mark a function or class as an ONNX function node.</li> <li>allclose: Validate numerical equivalence between JAX and ONNX Runtime outputs.</li> </ul> Example <p>from jax2onnx import to_onnx import jax.numpy as jnp</p> <p>def my_model(x): ...     return jnp.sin(x)</p> <p>to_onnx(my_model, inputs=[('B', 10)], return_mode=\"file\", output_path=\"model.onnx\")</p>"},{"location":"user_guide/api/#jax2onnx.user_interface.allclose","title":"<code>allclose(fn, onnx_model_path, inputs, input_params=None, rtol=0.001, atol=1e-05, *, enable_double_precision=False)</code>","text":"<p>Checks if JAX and ONNX Runtime outputs remain numerically close.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable</code> <p>JAX callable to compare against the exported ONNX model.</p> required <code>onnx_model_path</code> <code>str</code> <p>Path to a serialized model that ORT can execute.</p> required <code>inputs</code> <code>List[Any]</code> <p>Concrete input arrays (or shape tuples, which will be sampled).</p> required <code>input_params</code> <code>Optional[Dict[str, Any]]</code> <p>Optional keyword arguments applied to both call sites.</p> <code>None</code> <code>rtol</code> <code>float</code> <p>Relative tolerance for floating-point comparisons.</p> <code>0.001</code> <code>atol</code> <code>float</code> <p>Absolute tolerance for floating-point comparisons.</p> <code>1e-05</code> <code>enable_double_precision</code> <code>bool</code> <p>Temporarily enable <code>jax_enable_x64</code> while running the comparison. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p><code>(is_match, message)</code> where <code>is_match</code> indicates success and <code>message</code></p> <code>str</code> <p>provides context when a mismatch occurs.</p> Example <p>import jax.numpy as jnp from jax2onnx import to_onnx, allclose</p> Source code in <code>jax2onnx/user_interface.py</code> <pre><code>def allclose(\n    fn: Callable,\n    onnx_model_path: str,\n    inputs: List[Any],\n    input_params: Optional[Dict[str, Any]] = None,\n    rtol: float = 1e-3,\n    atol: float = 1e-5,\n    *,\n    enable_double_precision: bool = False,\n) -&gt; Tuple[bool, str]:\n    \"\"\"\n    Checks if JAX and ONNX Runtime outputs remain numerically close.\n\n    Args:\n        fn: JAX callable to compare against the exported ONNX model.\n        onnx_model_path: Path to a serialized model that ORT can execute.\n        inputs: Concrete input arrays (or shape tuples, which will be sampled).\n        input_params: Optional keyword arguments applied to both call sites.\n        rtol: Relative tolerance for floating-point comparisons.\n        atol: Absolute tolerance for floating-point comparisons.\n        enable_double_precision: Temporarily enable `jax_enable_x64` while running the\n            comparison. Defaults to False.\n\n    Returns:\n        `(is_match, message)` where `is_match` indicates success and `message`\n        provides context when a mismatch occurs.\n\n    Example:\n        &gt;&gt;&gt; import jax.numpy as jnp\n        &gt;&gt;&gt; from jax2onnx import to_onnx, allclose\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # 1. Define and Export\n        &gt;&gt;&gt; def my_func(x):\n        ...     return jnp.sin(x)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; model_path = to_onnx(\n        ...     my_func,\n        ...     inputs=[('B', 10)],\n        ...     return_mode=\"file\",\n        ...     output_path=\"my_model.onnx\"\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # 2. Validate\n        &gt;&gt;&gt; # Provide concrete shapes for validation (replacing dynamic dim 'B')\n        &gt;&gt;&gt; validation_inputs = [(5, 10)]\n        &gt;&gt;&gt; is_match, msg = allclose(my_func, model_path, inputs=validation_inputs, atol=1e-5)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; assert is_match, f\"Validation failed: {msg}\"\n    \"\"\"\n\n    logging.info(\n        \"Comparing JAX and ONNX outputs (path=%s, rtol=%s, atol=%s)\",\n        onnx_model_path,\n        rtol,\n        atol,\n    )\n\n    def _is_shape(x: Any) -&gt; bool:\n        return isinstance(x, (tuple, list)) and all(\n            isinstance(dim, (int, str)) for dim in x\n        )\n\n    xs: List[Any]\n    if all(_is_shape(x) for x in inputs):\n        xs = [\n            np.random.rand(*[d if isinstance(d, int) else 2 for d in shape]).astype(\n                np.float32\n            )\n            for shape in inputs\n        ]\n    else:\n        xs = list(inputs)\n\n    params = dict(input_params or {})\n    with _temporary_x64(enable_double_precision):\n        with jax.default_matmul_precision(\"float32\"):\n            return _run_allclose(fn, onnx_model_path, xs, params, rtol=rtol, atol=atol)\n</code></pre>"},{"location":"user_guide/api/#jax2onnx.user_interface.allclose--1-define-and-export","title":"1. Define and Export","text":"<p>def my_func(x): ...     return jnp.sin(x)</p> <p>model_path = to_onnx( ...     my_func, ...     inputs=[('B', 10)], ...     return_mode=\"file\", ...     output_path=\"my_model.onnx\" ... )</p>"},{"location":"user_guide/api/#jax2onnx.user_interface.allclose--2-validate","title":"2. Validate","text":""},{"location":"user_guide/api/#jax2onnx.user_interface.allclose--provide-concrete-shapes-for-validation-replacing-dynamic-dim-b","title":"Provide concrete shapes for validation (replacing dynamic dim 'B')","text":"<p>validation_inputs = [(5, 10)] is_match, msg = allclose(my_func, model_path, inputs=validation_inputs, atol=1e-5)</p> <p>assert is_match, f\"Validation failed: {msg}\"</p>"},{"location":"user_guide/api/#jax2onnx.user_interface.onnx_function","title":"<code>onnx_function(target=None, *, unique=False, namespace=None, name=None, type=None)</code>","text":"<p>Decorator to mark a function or class as an ONNX function.</p> <p>This decorator is used to indicate that a function or class should be converted to an ONNX function node when included in a model. It allows the function to be traced and exported as a reusable component with its own namespace in the ONNX graph.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Optional[Union[Callable, type]]</code> <p>The target function or class to decorate. When omitted, the decorator must be called with parentheses.</p> <code>None</code> <code>unique</code> <code>bool</code> <p>If True, reuse a single ONNX Function definition for all call sites that share the same callable type and captured parameters.</p> <code>False</code> <code>namespace</code> <code>Optional[str]</code> <p>Custom domain prefix for the emitted FunctionProto. Defaults to <code>\"custom\"</code> when omitted.</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>Optional human-readable base name for the ONNX function. When set, this overrides the callable's Python name for the function <code>op_type</code> and FunctionProto name; the domain still derives from <code>namespace</code>.</p> <code>None</code> <code>type</code> <code>Optional[str]</code> <p>Alias for <code>name</code>; preferred keyword for setting the function <code>op_type</code>/display name in ONNX.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Callable, type]</code> <p>The decorated function or class with ONNX function capabilities.</p> Example <p>from jax2onnx import onnx_function import jax.numpy as jnp</p> <p>@onnx_function def my_custom_op(x, y): ...     return jnp.sin(x) * y</p> Source code in <code>jax2onnx/user_interface.py</code> <pre><code>def onnx_function(\n    target: Optional[Union[Callable, type]] = None,\n    *,\n    unique: bool = False,\n    namespace: Optional[str] = None,\n    name: Optional[str] = None,\n    type: Optional[str] = None,  # noqa: A002 - user-facing keyword\n) -&gt; Union[Callable, type]:\n    \"\"\"\n    Decorator to mark a function or class as an ONNX function.\n\n    This decorator is used to indicate that a function or class should be converted to\n    an ONNX function node when included in a model. It allows the function to be traced\n    and exported as a reusable component with its own namespace in the ONNX graph.\n\n    Args:\n        target: The target function or class to decorate. When omitted, the decorator\n            must be called with parentheses.\n        unique: If True, reuse a single ONNX Function definition for all call sites\n            that share the same callable type and captured parameters.\n        namespace: Custom domain prefix for the emitted FunctionProto. Defaults to\n            ``\"custom\"`` when omitted.\n        name: Optional human-readable base name for the ONNX function. When set,\n            this overrides the callable's Python name for the function `op_type`\n            and FunctionProto name; the domain still derives from ``namespace``.\n        type: Alias for ``name``; preferred keyword for setting the function\n            `op_type`/display name in ONNX.\n\n    Returns:\n        The decorated function or class with ONNX function capabilities.\n\n    Example:\n        &gt;&gt;&gt; from jax2onnx import onnx_function\n        &gt;&gt;&gt; import jax.numpy as jnp\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; @onnx_function\n        &gt;&gt;&gt; def my_custom_op(x, y):\n        ...     return jnp.sin(x) * y\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Also works with Flax modules:\n        &gt;&gt;&gt; from flax import nnx\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; @onnx_function\n        &gt;&gt;&gt; class MLPBlock(nnx.Module):\n        &gt;&gt;&gt;     def __init__(self, features, rngs):\n        &gt;&gt;&gt;         self.dense = nnx.Linear(features, rngs=rngs)\n        &gt;&gt;&gt;         self.activation = nnx.relu\n        &gt;&gt;&gt;\n        &gt;&gt;&gt;     def __call__(self, x):\n        &gt;&gt;&gt;         return self.activation(self.dense(x))\n    \"\"\"\n\n    # Prefer the explicit `type` override; fall back to `name` for BC.\n    display = type if isinstance(type, str) and type else name\n    return onnx_function_impl(\n        target, unique=unique, namespace=namespace, name=display, type=display\n    )\n</code></pre>"},{"location":"user_guide/api/#jax2onnx.user_interface.onnx_function--also-works-with-flax-modules","title":"Also works with Flax modules:","text":"<p>from flax import nnx</p> <p>@onnx_function class MLPBlock(nnx.Module):     def init(self, features, rngs):         self.dense = nnx.Linear(features, rngs=rngs)         self.activation = nnx.relu</p> <pre><code>def __call__(self, x):\n    return self.activation(self.dense(x))\n</code></pre>"},{"location":"user_guide/api/#jax2onnx.user_interface.to_onnx","title":"<code>to_onnx(fn, inputs, input_params=None, model_name='jax_model', opset=21, *, enable_double_precision=False, record_primitive_calls_file=None, return_mode='proto', output_path=None)</code>","text":"<pre><code>to_onnx(\n    fn: Callable,\n    inputs: Sequence[UserInputSpec],\n    input_params: Optional[Mapping[str, object]] = ...,\n    model_name: str = ...,\n    opset: int = ...,\n    *,\n    enable_double_precision: bool = ...,\n    record_primitive_calls_file: Optional[str] = ...,\n    return_mode: Literal[\"proto\"] = ...,\n    output_path: None = ...\n) -&gt; onnx.ModelProto\n</code></pre><pre><code>to_onnx(\n    fn: Callable,\n    inputs: Sequence[UserInputSpec],\n    input_params: Optional[Mapping[str, object]] = ...,\n    model_name: str = ...,\n    opset: int = ...,\n    *,\n    enable_double_precision: bool = ...,\n    record_primitive_calls_file: Optional[str] = ...,\n    return_mode: Literal[\"ir\"],\n    output_path: Optional[PathLikeStr] = ...\n) -&gt; ir.Model\n</code></pre><pre><code>to_onnx(\n    fn: Callable,\n    inputs: Sequence[UserInputSpec],\n    input_params: Optional[Mapping[str, object]] = ...,\n    model_name: str = ...,\n    opset: int = ...,\n    *,\n    enable_double_precision: bool = ...,\n    record_primitive_calls_file: Optional[str] = ...,\n    return_mode: Literal[\"file\"],\n    output_path: PathLikeStr\n) -&gt; str\n</code></pre> <p>Converts a JAX function or model into an ONNX model.</p> <p>This function serves as the main entry point for converting JAX/Flax models to ONNX format. It supports dynamic shapes and additional runtime parameters.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable</code> <p>The JAX function or Flax module to convert.</p> required <code>inputs</code> <code>Sequence[UserInputSpec]</code> <p>Sequence of input specifications. Each entry may be: * a <code>jax.ShapeDtypeStruct</code> (or <code>jax.core.ShapedArray</code>); * any array-like object exposing <code>.shape</code> and <code>.dtype</code>   (e.g. <code>jax.Array</code>, <code>np.ndarray</code>); * a tuple/list of ints/strs describing the desired shape.   Example: <code>[('B', 128), (1, 10)]</code> implies two inputs, the first with a dynamic batch dimension 'B' and fixed size 128.</p> required <code>input_params</code> <code>Optional[Mapping[str, object]]</code> <p>Optional mapping of string keys to runtime parameters that should be exposed as inputs in the ONNX model rather than baked into the export (e.g. <code>\"deterministic\"</code> flags).</p> <code>None</code> <code>model_name</code> <code>str</code> <p>Name to give the ONNX model. Defaults to \"jax_model\".</p> <code>'jax_model'</code> <code>opset</code> <code>int</code> <p>ONNX opset version to target. Defaults to 21.</p> <code>21</code> <code>enable_double_precision</code> <code>bool</code> <p>If True, export tensors as tensor(double). Defaults to False (use tensor(float)).</p> <code>False</code> <code>record_primitive_calls_file</code> <code>Optional[str]</code> <p>Optional path to a file. If provided, details of each JAX primitive encountered during conversion will be recorded to this file. This log can be used by developers to manually create new test cases. Defaults to None (disabled).</p> <code>None</code> <code>return_mode</code> <code>ReturnMode</code> <p>Output mode. <code>\"proto\"</code> (default) returns an ONNX ModelProto, <code>\"ir\"</code> returns the intermediate onnx_ir.Model, and <code>\"file\"</code> serialises directly to disk.</p> <code>'proto'</code> <code>output_path</code> <code>Optional[PathLikeStr]</code> <p>Destination path (str or PathLike) required when <code>return_mode</code> is <code>\"file\"</code>. Ignored otherwise.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[ModelProto, Model, str]</code> <ul> <li>If <code>return_mode=\"proto\"</code> (default): Returns an <code>onnx.ModelProto</code> object.</li> </ul> <code>Union[ModelProto, Model, str]</code> <ul> <li>If <code>return_mode=\"ir\"</code>: Returns an <code>onnx_ir.Model</code> object (intermediate representation).</li> </ul> <code>Union[ModelProto, Model, str]</code> <ul> <li>If <code>return_mode=\"file\"</code>: Returns the string path to the saved file.</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>return_mode</code> is \"file\" but <code>output_path</code> is not provided.</p> <code>ValueError</code> <p>If <code>return_mode</code> is invalid.</p> <code>TypeError</code> <p>If <code>input_params</code> keys are not strings.</p> Example <p>import jax import jax.numpy as jnp from jax2onnx import to_onnx</p> Source code in <code>jax2onnx/user_interface.py</code> <pre><code>def to_onnx(\n    fn: Callable,\n    inputs: Sequence[UserInputSpec],\n    input_params: Optional[Mapping[str, object]] = None,\n    model_name: str = \"jax_model\",\n    opset: int = 21,\n    *,  # All arguments after this must be keyword-only\n    enable_double_precision: bool = False,\n    record_primitive_calls_file: Optional[str] = None,\n    return_mode: ReturnMode = \"proto\",\n    output_path: Optional[PathLikeStr] = None,\n) -&gt; Union[onnx.ModelProto, ir.Model, str]:\n    \"\"\"\n    Converts a JAX function or model into an ONNX model.\n\n    This function serves as the main entry point for converting JAX/Flax models to ONNX format.\n    It supports dynamic shapes and additional runtime parameters.\n\n    Args:\n        fn: The JAX function or Flax module to convert.\n        inputs: Sequence of input specifications. Each entry may be:\n            * a `jax.ShapeDtypeStruct` (or `jax.core.ShapedArray`);\n            * any array-like object exposing `.shape` and `.dtype`\n              (e.g. `jax.Array`, `np.ndarray`);\n            * a tuple/list of ints/strs describing the desired shape.\n              Example: `[('B', 128), (1, 10)]` implies two inputs, the first with a dynamic batch dimension 'B' and fixed size 128.\n        input_params: Optional mapping of string keys to runtime parameters that\n            should be exposed as inputs in the ONNX model rather than baked into\n            the export (e.g. `\"deterministic\"` flags).\n        model_name: Name to give the ONNX model. Defaults to \"jax_model\".\n        opset: ONNX opset version to target. Defaults to 21.\n        enable_double_precision: If True, export tensors as tensor(double). Defaults to False (use tensor(float)).\n        record_primitive_calls_file: Optional path to a file. If provided,\n            details of each JAX primitive encountered during conversion will be\n            recorded to this file. This log can be used by developers to manually\n            create new test cases. Defaults to None (disabled).\n        return_mode: Output mode. `\"proto\"` (default) returns an ONNX ModelProto,\n            `\"ir\"` returns the intermediate onnx_ir.Model, and `\"file\"`\n            serialises directly to disk.\n        output_path: Destination path (str or PathLike) required when `return_mode` is\n            `\"file\"`. Ignored otherwise.\n\n    Returns:\n        * If `return_mode=\"proto\"` (default): Returns an `onnx.ModelProto` object.\n        * If `return_mode=\"ir\"`: Returns an `onnx_ir.Model` object (intermediate representation).\n        * If `return_mode=\"file\"`: Returns the string path to the saved file.\n\n    Raises:\n        ValueError: If `return_mode` is \"file\" but `output_path` is not provided.\n        ValueError: If `return_mode` is invalid.\n        TypeError: If `input_params` keys are not strings.\n\n    Example:\n        &gt;&gt;&gt; import jax\n        &gt;&gt;&gt; import jax.numpy as jnp\n        &gt;&gt;&gt; from jax2onnx import to_onnx\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Define a simple JAX function\n        &gt;&gt;&gt; def linear(x, w, b):\n        ...     return jnp.dot(x, w) + b\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Define input shapes: 'B' indicates a dynamic batch dimension\n        &gt;&gt;&gt; input_specs = [\n        ...     ('B', 32),  # x: [Batch, 32]\n        ...     (32, 10),   # w: [32, 10]\n        ...     (10,)       # b: [10]\n        ... ]\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Convert and save to file directly (Recommended)\n        &gt;&gt;&gt; to_onnx(\n        ...     linear,\n        ...     inputs=input_specs,\n        ...     model_name=\"linear_model\",\n        ...     return_mode=\"file\",\n        ...     output_path=\"linear_model.onnx\"\n        ... )\n    \"\"\"\n\n    logging.info(\n        f\"Converting JAX function to ONNX model with parameters: \"\n        f\"model_name={model_name}, opset={opset}, input_shapes={inputs}, \"\n        f\"input_params={input_params}, \"\n        f\"enable_double_precision={enable_double_precision}, \"\n        f\"record_primitive_calls_file={record_primitive_calls_file}, \"\n        f\"return_mode={return_mode}, output_path={output_path}\"\n    )\n\n    # Determine the nature of the 'inputs' argument to prepare for to_onnx_impl\n    normalized_mode = _normalize_return_mode(return_mode)\n\n    file_path: Optional[str] = None\n    if normalized_mode == \"file\":\n        if output_path is None:\n            raise ValueError(\n                \"`output_path` must be provided when return_mode is 'file'.\"\n            )\n        path_value = os.fspath(output_path)\n        if isinstance(path_value, bytes):\n            path_value = path_value.decode()\n        file_path = cast(str, path_value)\n\n    normalized_inputs: List[InputSpec] = []\n    if inputs:\n        normalized_inputs = _normalize_input_specs(inputs)\n\n    param_map: Dict[str, object] = {}\n    if input_params:\n        for key, value in input_params.items():\n            if not isinstance(key, str):\n                raise TypeError(\n                    \"input_params must use string keys; \"\n                    f\"received key of type {type(key)}.\"\n                )\n            param_map[key] = value\n\n    with _temporary_x64(enable_double_precision):\n        result = to_onnx_impl(\n            fn=fn,\n            inputs=normalized_inputs,\n            input_params=param_map,\n            model_name=model_name,\n            opset=opset,\n            enable_double_precision=enable_double_precision,\n            record_primitive_calls_file=record_primitive_calls_file,\n            protective_clone=(normalized_mode == \"ir\"),\n        )\n\n        postprocess_ir_model(\n            result,\n            promote_to_double=enable_double_precision,\n        )\n\n    def _save_model_proto(\n        model_proto: onnx.ModelProto,\n        dest: str,\n        *,\n        external_threshold: int = 1_048_576,  # 1 MB default before spilling to .data\n    ) -&gt; str:\n        dest_dir = os.path.dirname(dest)\n        if dest_dir:\n            os.makedirs(dest_dir, exist_ok=True)\n        data_location = os.path.basename(dest) + \".data\"\n        data_path = os.path.join(dest_dir or \".\", data_location)\n        onnx.save_model(\n            model_proto,\n            dest,\n            save_as_external_data=True,\n            all_tensors_to_one_file=True,\n            location=data_location,\n            size_threshold=external_threshold,\n            convert_attribute=False,\n        )\n        # Only keep the .data sidecar if the export actually referenced external data.\n        if not any(init.external_data for init in model_proto.graph.initializer):\n            # No external payloads; remove an empty sidecar if one was produced.\n            try:\n                if os.path.exists(data_path) and os.path.getsize(data_path) == 0:\n                    os.remove(data_path)\n            except OSError:\n                pass\n        return dest\n\n    _materialize_input_params_on_ir(result, param_map)\n    if normalized_mode == \"ir\":\n        return result\n\n    model_proto = ir.to_proto(result)\n    if normalized_mode == \"file\":\n        assert file_path is not None\n        return _save_model_proto(model_proto, file_path)\n    return model_proto\n</code></pre>"},{"location":"user_guide/api/#jax2onnx.user_interface.to_onnx--define-a-simple-jax-function","title":"Define a simple JAX function","text":"<p>def linear(x, w, b): ...     return jnp.dot(x, w) + b</p>"},{"location":"user_guide/api/#jax2onnx.user_interface.to_onnx--define-input-shapes-b-indicates-a-dynamic-batch-dimension","title":"Define input shapes: 'B' indicates a dynamic batch dimension","text":"<p>input_specs = [ ...     ('B', 32),  # x: [Batch, 32] ...     (32, 10),   # w: [32, 10] ...     (10,)       # b: [10] ... ]</p>"},{"location":"user_guide/api/#jax2onnx.user_interface.to_onnx--convert-and-save-to-file-directly-recommended","title":"Convert and save to file directly (Recommended)","text":"<p>to_onnx( ...     linear, ...     inputs=input_specs, ...     model_name=\"linear_model\", ...     return_mode=\"file\", ...     output_path=\"linear_model.onnx\" ... )</p>"},{"location":"user_guide/examples/","title":"Examples","text":"Component Description Testcases Since MlpExample A simple Equinox MLP (converter pipeline). <code>mlp_training_mode</code> \u2705<code>mlp_training_mode_f64</code> \u2705<code>mlp_inference_mode</code> \u2705<code>mlp_inference_mode_f64</code> \u2705<code>mlp_batched_training_mode</code> \u2705<code>mlp_batched_training_mode_f64</code> \u2705 0.8.0 SimpleLinearExample A simple linear layer example using Equinox (converter). <code>simple_linear</code> \u2705<code>simple_linear_f64</code> \u2705<code>nn_linear</code> \u2705<code>nn_linear_f64</code> \u2705 0.7.1 Attention Multi-Head Self-Attention using Equinox modules. <code>attention_dynamic</code> \u2705<code>attention</code> \u2705 0.10.0 AttentionCore Multi-Head Self-Attention without rotary processing. <code>attention_core_dynamic</code> \u2705<code>attention_core</code> \u2705 0.10.0 Block Transformer Block. <code>transformer_block_dynamic</code> \u2705<code>transformer_block</code> \u2705 0.10.0 DINOv3VisionTransformer DINOv3 Vision Transformer <code>eqx_dinov3_vit_Ti14_dynamic</code> \u2705<code>eqx_dinov3_vit_Ti14</code> \u2705<code>eqx_dinov3_vit_S14_dynamic</code> \u2705<code>eqx_dinov3_vit_S14</code> \u2705<code>eqx_dinov3_vit_B14_dynamic</code> \u2705<code>eqx_dinov3_vit_B14</code> \u2705<code>eqx_dinov3_vit_S16_dynamic</code> \u2705<code>eqx_dinov3_vit_S16</code> \u2705 0.10.0 PatchEmbed Image to Patch Embedding. <code>patch_embed</code> \u2705 0.10.0 AttentionBlock Self-attention block with rotary embeddings and sinks. <code>gpt_oss_attention_block</code> \u2705 0.10.2 MLPBlock Mixture-of-experts SwiGLU feed-forward block. <code>gpt_oss_mlp_block</code> \u2705 0.10.2 RMSNorm Root mean square normalisation used by GPT-OSS. <code>gpt_oss_rmsnorm_dynamic</code> \u2705<code>gpt_oss_rmsnorm</code> \u2705 0.10.2 Transformer Full GPT-OSS Transformer stack. <code>gpt_oss_transformer</code> \u2705 0.10.2 TransformerBlock GPT-OSS Transformer layer (attention + MoE). <code>gpt_oss_transformer_block_dynamic</code> \u2705<code>gpt_oss_transformer_block</code> \u2705 0.10.2 GPT A simple GPT model that reuses nnx.MultiHeadAttention. <code>gpt_dynamic</code> \u2705<code>gpt</code> \u2705 0.7.0 GPT_Attention A multi-head attention layer. <code>gpt_attention</code> \u2705 0.7.1 GPT_CausalSelfAttention A causal self-attention module. <code>gpt_causal_self_attention_dynamic</code> \u2705<code>gpt_causal_self_attention</code> \u2705 0.7.0 GPT_Embeddings Combines token and position embeddings with dropout. <code>gpt_embeddings_dynamic</code> \u2705<code>gpt_embeddings</code> \u2705 0.7.0 GPT_Head The head of the GPT model. <code>gpt_head_dynamic</code> \u2705<code>gpt_head</code> \u2705 0.7.0 GPT_MLP An MLP block with GELU activation from nanoGPT. <code>gpt_mlp_dynamic</code> \u2705<code>gpt_mlp</code> \u2705 0.7.0 GPT_PositionEmbedding A positional embedding layer using nnx.Embed. <code>gpt_position_embedding</code> \u2705 0.7.0 GPT_TokenEmbedding A token embedding layer using nnx.Embed. <code>gpt_token_embedding_dynamic</code> \u2705<code>gpt_token_embedding</code> \u2705 0.7.0 GPT_TransformerBlock A transformer block combining attention and MLP. <code>gpt_block_dynamic</code> \u2705<code>gpt_block</code> \u2705 0.7.0 GPT_TransformerStack A stack of transformer blocks. <code>gpt_transformer_stack_dynamic</code> \u2705<code>gpt_transformer_stack</code> \u2705 0.7.0 GPT_broadcast_add Simple dynamic broadcast + add <code>gpt_broadcast_add_dynamic_dynamic</code> \u2705<code>gpt_broadcast_add_dynamic_dynamic_f64</code> \u2705<code>gpt_broadcast_add_dynamic</code> \u2705<code>gpt_broadcast_add_dynamic_f64</code> \u2705 0.7.0 cfl_timestep Tests the CFL condition timestep calculation. <code>cfl_timestep_f64</code> \u2705 0.6.5 weno_reconstruction Tests the complex arithmetic pattern found in WENO schemes. <code>weno_reconstruction_f64</code> \u2705 0.6.5 fori_loop_test fori_loop_test: demonstrates jax.lax.fori_loop with a simple loop. <code>fori_loop_test</code> \u2705<code>fori_loop_test_f64</code> \u2705 0.6.3 issue18_abs Test jnp.abs from issue 18 <code>abs_fn</code> \u2705<code>abs_fn_f64</code> \u2705 0.6.3 issue18_arange Test jnp.arange from issue 18 <code>arange_fn</code> \u2705 0.6.3 issue18_fori_loop Test jax.lax.fori_loop from issue 18 <code>fori_loop_fn</code> \u2705<code>fori_loop_fn_f64</code> \u2705 0.6.3 issue18_linspace Test jnp.linspace from issue 18 <code>linspace_fn</code> \u2705 0.6.3 issue18_scan Test jax.lax.scan from issue 18 (no xs) <code>scan_fn</code> \u2705 0.6.3 issue18_sign Test jnp.sign from issue 18 <code>sign_fn</code> \u2705<code>sign_fn_f64</code> \u2705 0.6.3 issue18_where Test jnp.where from issue 18 <code>where_fn</code> \u2705<code>where_fn_f64</code> \u2705 0.6.3 issue18_while_loop Test jax.lax.while_loop from issue 18 <code>while_loop_fn</code> \u2705 0.9.0 select_test Demonstrates jnp.select with scalar and tensor predicates. <code>select_test_all_options</code> \u2705<code>select_test_scalar_select_option_0</code> \u2705<code>select_test_scalar_select_option_1</code> \u2705<code>select_test_scalar_select_option_2</code> \u2705<code>select_test_default_case</code> \u2705 0.9.0 sort_test sort_test: demonstrates jnp.sort on slices of an input array. <code>sort_test_basic</code> \u2705 0.9.0 cond_scatter_add_mul Scatter add/mul inside conditional branches (converter). <code>cond_scatter_add_mul_f64_a</code> \u2705<code>cond_scatter_add_mul_f64_b</code> \u2705 0.8.0 cond_scatter_repro Reproduces a bug where lax.cond subgraphs do not inherit parent initializers. <code>cond_scatter_repro_f64</code> \u2705 0.6.4 remat2 Tests a simple case of <code>jax.checkpoint</code> (also known as <code>jax.remat2</code>). <code>checkpoint_scalar_f32</code> \u2705<code>checkpoint_scalar_f32_f64</code> \u2705 0.6.5 scatter_window Window-scatter (H\u00d7W patch) with implicit batch (depth-3 path). Exercises GatherScatterMode.FILL_OR_DROP and double precision. Regression of a prior conversion failure. <code>scatter_window_update_f64_example</code> \u2705 0.7.4 two_times_silu Regression for calling jax.nn.silu twice (issue #139). <code>two_times_silu_scalar</code> \u2705<code>two_times_silu_scalar_f64</code> \u2705 0.10.2 LinenCNN A simple convolutional neural network (CNN). <code>simple_cnn_static</code> \u2705<code>simple_cnn_dynamic</code> \u2705 0.11.0 LinenMLP A simple Linen MLP with BatchNorm, Dropout, and GELU activation. <code>simple_linen_mlp_static</code> \u2705<code>simple_linen_mlp_static_f64</code> \u2705<code>simple_linen_mlp_dynamic</code> \u2705<code>simple_linen_mlp_dynamic_f64</code> \u2705<code>simple_linen_mlp_with_call_params_dynamic</code> \u2705<code>simple_linen_mlp_with_call_params_dynamic_f64</code> \u2705<code>simple_linen_mlp_with_call_params</code> \u2705<code>simple_linen_mlp_with_call_params_f64</code> \u2705 0.11.0 LinenMLPSequential A Linen MLP built from flax.linen.Sequential. <code>simple_linen_mlp_sequential_static</code> \u2705<code>simple_linen_mlp_sequential_static_f64</code> \u2705<code>simple_linen_mlp_sequential_dynamic</code> \u2705<code>simple_linen_mlp_sequential_dynamic_f64</code> \u2705 0.11.0 MaxText_deepseek2_16b MaxText model: deepseek2-16b <code>maxtext_deepseek2-16b</code> \u2705 0.11.1 MaxText_deepseek2_236b MaxText model: deepseek2-236b <code>maxtext_deepseek2-236b</code> \u2705 0.11.1 MaxText_deepseek3_671b MaxText model: deepseek3-671b <code>maxtext_deepseek3-671b</code> \u2705 0.11.1 MaxText_deepseek3_671b_2dfsdp MaxText model: deepseek3-671b-2dfsdp <code>maxtext_deepseek3-671b-2dfsdp</code> \u2705 0.11.1 MaxText_deepseek3_test MaxText model: deepseek3-test <code>maxtext_deepseek3-test</code> \u2705 0.11.1 MaxText_deepseek3_tiny MaxText model: deepseek3-tiny <code>maxtext_deepseek3-tiny</code> \u2705 0.11.1 MaxText_gemma2_27b MaxText model: gemma2-27b <code>maxtext_gemma2-27b</code> \u2705 0.11.1 MaxText_gemma2_2b MaxText model: gemma2-2b <code>maxtext_gemma2-2b</code> \u2705 0.11.1 MaxText_gemma2_9b MaxText model: gemma2-9b <code>maxtext_gemma2-9b</code> \u2705 0.11.1 MaxText_gemma3_12b MaxText model: gemma3-12b <code>maxtext_gemma3-12b</code> \u2705 0.11.1 MaxText_gemma3_27b MaxText model: gemma3-27b <code>maxtext_gemma3-27b</code> \u2705 0.11.1 MaxText_gemma3_4b MaxText model: gemma3-4b <code>maxtext_gemma3-4b</code> \u2705 0.11.1 MaxText_gemma_2b MaxText model: gemma-2b <code>maxtext_gemma-2b</code> \u2705 0.11.1 MaxText_gemma_7b MaxText model: gemma-7b <code>maxtext_gemma-7b</code> \u2705 0.11.1 MaxText_gpt3_175b MaxText model: gpt3-175b <code>maxtext_gpt3-175b</code> \u2705 0.11.1 MaxText_gpt3_22b MaxText model: gpt3-22b <code>maxtext_gpt3-22b</code> \u2705 0.11.1 MaxText_gpt3_52k MaxText model: gpt3-52k <code>maxtext_gpt3-52k</code> \u2705 0.11.1 MaxText_gpt3_6b MaxText model: gpt3-6b <code>maxtext_gpt3-6b</code> \u2705 0.11.1 MaxText_kimi_k2_1t MaxText model: kimi-k2-1t <code>maxtext_kimi-k2-1t</code> \u2705 0.11.1 MaxText_llama2_13b MaxText model: llama2-13b <code>maxtext_llama2-13b</code> \u2705 0.11.1 MaxText_llama2_70b MaxText model: llama2-70b <code>maxtext_llama2-70b</code> \u2705 0.11.1 MaxText_llama2_7b MaxText model: llama2-7b <code>maxtext_llama2-7b</code> \u2705 0.11.1 MaxText_llama3_1_405b MaxText model: llama3.1-405b <code>maxtext_llama3.1-405b</code> \u2705 0.11.1 MaxText_llama3_1_70b MaxText model: llama3.1-70b <code>maxtext_llama3.1-70b</code> \u2705 0.11.1 MaxText_llama3_1_8b MaxText model: llama3.1-8b <code>maxtext_llama3.1-8b</code> \u2705 0.11.1 MaxText_llama3_3_70b MaxText model: llama3.3-70b <code>maxtext_llama3.3-70b</code> \u2705 0.11.1 MaxText_llama3_405b MaxText model: llama3-405b <code>maxtext_llama3-405b</code> \u2705 0.11.1 MaxText_llama3_70b MaxText model: llama3-70b <code>maxtext_llama3-70b</code> \u2705 0.11.1 MaxText_llama3_8b MaxText model: llama3-8b <code>maxtext_llama3-8b</code> \u2705 0.11.1 MaxText_mistral_7b MaxText model: mistral-7b <code>maxtext_mistral-7b</code> \u2705 0.11.1 MaxText_qwen3_0_6b MaxText model: qwen3-0.6b <code>maxtext_qwen3-0.6b</code> \u2705 0.11.1 MaxText_qwen3_14b MaxText model: qwen3-14b <code>maxtext_qwen3-14b</code> \u2705 0.11.1 MaxText_qwen3_235b_a22b MaxText model: qwen3-235b-a22b <code>maxtext_qwen3-235b-a22b</code> \u2705 0.11.1 MaxText_qwen3_30b_a3b MaxText model: qwen3-30b-a3b <code>maxtext_qwen3-30b-a3b</code> \u2705 0.11.1 MaxText_qwen3_32b MaxText model: qwen3-32b <code>maxtext_qwen3-32b</code> \u2705 0.11.1 MaxText_qwen3_480b_a35b MaxText model: qwen3-480b-a35b <code>maxtext_qwen3-480b-a35b</code> \u2705 0.11.1 MaxText_qwen3_4b MaxText model: qwen3-4b <code>maxtext_qwen3-4b</code> \u2705 0.11.1 MaxText_qwen3_4b_thinking_2507 MaxText model: qwen3-4b-thinking-2507 <code>maxtext_qwen3-4b-thinking-2507</code> \u2705 0.11.1 MaxText_qwen3_8b MaxText model: qwen3-8b <code>maxtext_qwen3-8b</code> \u2705 0.11.1 MaxText_qwen3_next_80b_a3b MaxText model: qwen3-next-80b-a3b <code>maxtext_qwen3-next-80b-a3b</code> \u2705 0.11.1 MaxText_qwen3_omni_30b_a3b MaxText model: qwen3-omni-30b-a3b <code>maxtext_qwen3-omni-30b-a3b</code> \u2705 0.11.1 AutoEncoder A simple autoencoder example (converter pipeline). <code>simple_autoencoder</code> \u2705<code>simple_autoencoder_f64</code> \u2705 0.2.0 CNN A simple convolutional neural network (CNN). <code>simple_cnn_static</code> \u2705<code>simple_cnn_dynamic</code> \u2705 0.2.0 ForiLoop fori_loop example using nnx-compatible primitives (converter). <code>fori_loop_counter</code> \u2705<code>fori_loop_counter_f64</code> \u2705 0.5.1 GRUCell Flax/nnx GRUCell lowered through converter primitives. <code>gru_cell_basic</code> \u2705 0.7.2 MLP A simple Multi-Layer Perceptron (MLP) with BatchNorm, Dropout, and GELU activation. <code>simple_mlp_static</code> \u2705<code>simple_mlp_static_f64</code> \u2705<code>simple_mlp_dynamic</code> \u2705<code>simple_mlp_dynamic_f64</code> \u2705<code>simple_mlp_with_call_params_dynamic</code> \u2705<code>simple_mlp_with_call_params_dynamic_f64</code> \u2705<code>simple_mlp_with_call_params</code> \u2705<code>simple_mlp_with_call_params_f64</code> \u2705 0.1.0 MultiHeadAttention nnx.MultiHeadAttention exercised in several configurations, including custom attention_fn and symbolic batch variants. <code>multihead_attention_nn_dynamic</code> \u2705<code>multihead_attention_nn</code> \u2705<code>multihead_attention_nnx_dynamic</code> \u2705<code>multihead_attention_nnx</code> \u2705<code>multihead_attention_2_nnx_dynamic</code> \u2705<code>multihead_attention_2_nnx</code> \u2705 0.2.0 SequentialReLU Two stateless nnx.relu activations chained via nnx.Sequential. <code>sequential_double_relu</code> \u2705<code>sequential_double_relu_f64</code> \u2705 0.7.1 SequentialWithResidual nnx.Sequential nested within a residual block to regress earlier bugs. <code>sequential_nested_with_residual</code> \u2705 0.7.1 TransformerDecoderWithSequential Tiny nnx Transformer decoder using nnx.Sequential in the FFN block. <code>tiny_decoder_with_sequential</code> \u2705<code>tiny_decoder_with_sequential_and_full_dynamic_shapes_dynamic</code> \u2705 0.7.1 TransformerDecoderWithoutSequential Tiny nnx Transformer decoder with explicit FFN layers (no Sequential). <code>tiny_decoder_without_sequential</code> \u2705 0.7.1 FlaxDINOv3VisionTransformer DINOv3 Vision Transformer <code>nnx_dinov3_vit_Ti14_dynamic</code> \u2705<code>nnx_dinov3_vit_Ti14</code> \u2705<code>nnx_dinov3_vit_S14_dynamic</code> \u2705<code>nnx_dinov3_vit_S14</code> \u2705<code>nnx_dinov3_vit_B14_dynamic</code> \u2705<code>nnx_dinov3_vit_B14</code> \u2705<code>nnx_dinov3_vit_S16_dynamic</code> \u2705<code>nnx_dinov3_vit_S16</code> \u2705 0.10.3 NnxDinoAttention Multi-Head Self-Attention using Flax/NNX modules. <code>nnx_attention_dynamic</code> \u2705<code>nnx_attention</code> \u2705 0.10.3 NnxDinoAttentionCore Multi-Head Self-Attention without rotary processing. <code>nnx_attention_core_dynamic</code> \u2705<code>nnx_attention_core</code> \u2705 0.10.3 NnxDinoBlock Transformer Block. <code>nnx_transformer_block_dynamic</code> \u2705<code>nnx_transformer_block</code> \u2705 0.10.3 NnxDinoPatchEmbed Image to Patch Embedding. <code>nnx_patch_embed</code> \u2705 0.10.3 FlaxAttentionBlock Attention block from the GPT-OSS Flax reference (no KV cache). <code>gpt_oss_attention_flax</code> \u2705 0.10.2 FlaxMLPBlock Mixture-of-experts MLP block from the GPT-OSS Flax port. <code>gpt_oss_mlp_flax</code> \u2705 0.10.2 FlaxRMSNorm Flax RMSNorm used in the GPT-OSS JAX port. <code>gpt_oss_rmsnorm_flax_dynamic</code> \u2705<code>gpt_oss_rmsnorm_flax</code> \u2705 0.10.2 FlaxRotaryEmbedding Rotary position embedding helper from the GPT-OSS Flax port. <code>gpt_oss_rotary_flax</code> \u2705 0.10.2 FlaxSDPA JIT sdpa helper from the GPT-OSS Flax port. <code>gpt_oss_sdpa_flax</code> \u2705 0.10.2 FlaxTransformer Full GPT-OSS Flax transformer (embedding, blocks, head). <code>gpt_oss_transformer_flax</code> \u2705 0.10.2 FlaxTransformerBlock Single GPT-OSS Flax transformer block (attention + MoE MLP). <code>gpt_oss_transformer_block_flax</code> \u2705 0.10.2 onnx_functions_000 One function boundary on an outer NNX module (new-world). <code>000_one_function_on_outer_layer_dynamic</code> \u2705<code>000_one_function_on_outer_layer</code> \u2705 0.4.0 onnx_functions_001 one function on an inner layer. <code>001_one_function_inner_dynamic</code> \u2705<code>001_one_function_inner</code> \u2705 0.4.0 onnx_functions_002 two nested functions. <code>002_two_nested_functions_dynamic</code> \u2705<code>002_two_nested_functions</code> \u2705 0.4.0 onnx_functions_003 two nested functions. <code>003_two_simple_nested_functions_dynamic</code> \u2705<code>003_two_simple_nested_functions</code> \u2705 0.4.0 onnx_functions_004 nested function plus component <code>004_nested_function_plus_component_dynamic</code> \u2705<code>004_nested_function_plus_component</code> \u2705 0.4.0 onnx_functions_005 nested function plus more components <code>005_nested_function_plus_component_dynamic</code> \u2705<code>005_nested_function_plus_component</code> \u2705 0.4.0 onnx_functions_006 one function on an outer layer. <code>006_one_function_outer_dynamic</code> \u2705<code>006_one_function_outer</code> \u2705 0.4.0 onnx_functions_007 transformer block with nested mlp block with call parameter <code>007_transformer_block_dynamic</code> \u2705<code>007_transformer_block</code> \u2705 0.4.0 onnx_functions_008 transformer block with nested mlp block no call parameter <code>008_transformer_block_dynamic</code> \u2705<code>008_transformer_block</code> \u2705 0.4.0 onnx_functions_009 transformer block using decorator on class and function <code>009_transformer_block_dynamic</code> \u2705<code>009_transformer_block</code> \u2705 0.4.0 onnx_functions_010 transformer stack <code>010_transformer_stack_dynamic</code> \u2705<code>010_transformer_stack</code> \u2705 0.4.0 onnx_functions_012 Vision Transformer (ViT) <code>012_vit_conv_embedding_dynamic</code> \u2705<code>012_vit_conv_embedding</code> \u2705 0.4.0 onnx_functions_013 Vision Transformer (ViT) <code>013_vit_conv_embedding_with_call_params_dynamic</code> \u2705<code>013_vit_conv_embedding_with_call_params</code> \u2705<code>013_vit_conv_embedding_with_internal_call_params_dynamic</code> \u2705<code>013_vit_conv_embedding_with_internal_call_params</code> \u2705 0.4.0 onnx_functions_014 one function on an outer layer. <code>014_one_function_with_input_param_with_default_value</code> \u2705<code>014_one_function_without_input_param_with_default_value_dynamic</code> \u2705<code>014_one_function_without_input_param_with_default_value</code> \u2705 0.4.0 onnx_functions_015 one function on an outer layer. <code>015_one_function_with_input_param_without_default_value_dynamic</code> \u2705<code>015_one_function_with_input_param_without_default_value</code> \u2705 0.4.0 onnx_functions_016 nested function plus more components <code>016_internal_function_with_input_param_with_default_value_dynamic</code> \u2705<code>016_internal_function_with_input_param_with_default_value</code> \u2705 0.4.0 onnx_functions_017 Demonstrates @onnx_function(unique=True) reuse across call sites. <code>017_unique_function_reuse</code> \u2705 0.10.0 ClassificationHead Classification head for Vision Transformer <code>vit_classification_head_dynamic</code> \u2705<code>vit_classification_head</code> \u2705 0.4.0 ClassificationHeadFlatten Classification head for Vision Transformer <code>vit_classification_head_flat_dynamic</code> \u2705<code>vit_classification_head_flat</code> \u2705 0.4.0 ConcatClsToken Concatenate CLS token to the input embedding <code>vit_concat_cls_token_dynamic</code> \u2705<code>vit_concat_cls_token</code> \u2705 0.4.0 ConcatClsTokenFlatten Concatenate CLS token to the input embedding <code>vit_concat_cls_token_flat_dynamic</code> \u2705<code>vit_concat_cls_token_flat</code> \u2705 0.4.0 ConvEmbedding Convolutional Token Embedding for MNIST with hierarchical downsampling. <code>vit_mnist_conv_embedding_dynamic</code> \u2705<code>vit_mnist_conv_embedding</code> \u2705 0.1.0 ConvEmbeddingFlatten Convolutional Token Embedding for MNIST with hierarchical downsampling. <code>vit_mnist_conv_embedding_flat_dynamic</code> \u2705<code>vit_mnist_conv_embedding_flat</code> \u2705 0.1.0 FeedForward MLP in Transformer <code>vit_feed_forward_dynamic</code> \u2705<code>vit_feed_forward</code> \u2705 0.1.0 FeedForwardFlatten MLP in Transformer <code>vit_feed_forward_flat_dynamic</code> \u2705<code>vit_feed_forward_flat</code> \u2705 0.1.0 GetToken Get the CLS token from the input embedding <code>vit_get_token_dynamic</code> \u2705<code>vit_get_token</code> \u2705 0.4.0 GetTokenFlatten Get the CLS token from the input embedding <code>vit_get_token_flat_dynamic</code> \u2705<code>vit_get_token_flat</code> \u2705 0.4.0 PatchEmbedding Cutting the image into patches and linearly embedding them. <code>vit_patch_embedding_dynamic</code> \u2705<code>vit_patch_embedding</code> \u2705 0.1.0 PatchEmbeddingFlatten Cutting the image into patches and linearly embedding them. <code>vit_patch_embedding_flat_dynamic</code> \u2705<code>vit_patch_embedding_flat</code> \u2705 0.1.0 PositionalEmbedding Add positional embedding to the input embedding <code>vit_positional_embedding_dynamic</code> \u2705<code>vit_positional_embedding</code> \u2705 0.4.0 PositionalEmbeddingFlatten Add positional embedding to the input embedding <code>vit_positional_embedding_flat_dynamic</code> \u2705<code>vit_positional_embedding_flat</code> \u2705 0.4.0 TransformerBlock Transformer from 'Attention Is All You Need.' <code>vit_transformer_block_dynamic</code> \u2705<code>vit_transformer_block</code> \u2705 0.1.0 TransformerBlockFlatten Transformer from 'Attention Is All You Need.' <code>vit_transformer_block_flat_dynamic</code> \u2705<code>vit_transformer_block_flat</code> \u2705 0.1.0 TransformerStack Stack of Transformer blocks <code>vit_transformer_stack_dynamic</code> \u2705<code>vit_transformer_stack</code> \u2705 0.1.0 TransformerStackFlatten Stack of Transformer blocks <code>vit_transformer_stack_flat_dynamic</code> \u2705<code>vit_transformer_stack_flat</code> \u2705 0.1.0 VisionTransformer A Vision Transformer (ViT) model for MNIST with configurable embedding type. <code>vit_model_conv_embedding_dynamic</code> \u2705<code>vit_model_conv_embedding</code> \u2705<code>vit_model_patch_embedding</code> \u2705 0.2.0 VisionTransformerFlatten A Vision Transformer (ViT) model for MNIST with configurable embedding type. <code>vit_model_conv_embedding_flat_dynamic</code> \u2705<code>vit_model_conv_embedding_flat</code> \u2705<code>vit_model_patch_embedding_flat_dynamic</code> \u2705<code>vit_model_patch_embedding_flat</code> \u2705 0.2.0"},{"location":"user_guide/getting_started/","title":"Getting Started","text":""},{"location":"user_guide/getting_started/#quickstart","title":"Quickstart","text":"<p>Install and export your first model in minutes:</p> <pre><code>pip install jax2onnx\n</code></pre> <p>Convert your JAX callable to ONNX in just a few lines:</p> <pre><code>from flax import nnx\nfrom jax2onnx import to_onnx\n\n# Define a simple MLP (from Flax docs)\nclass MLP(nnx.Module):\n    def __init__(self, din, dmid, dout, *, rngs): \n        self.linear1 = nnx.Linear(din, dmid, rngs=rngs)\n        self.dropout = nnx.Dropout(rate=0.1, rngs=rngs)\n        self.bn = nnx.BatchNorm(dmid, rngs=rngs)\n        self.linear2 = nnx.Linear(dmid, dout, rngs=rngs) \n    def __call__(self, x): \n        x = nnx.gelu(self.dropout(self.bn(self.linear1(x))))\n        return self.linear2(x)\n\n# Instantiate model\nmy_callable = MLP(din=30, dmid=20, dout=10, rngs=nnx.Rngs(0))\n\n# Export straight to disk without keeping the proto in memory\nto_onnx(\n    my_callable,\n    [(\"B\", 30)],\n    return_mode=\"file\",\n    output_path=\"my_callable.onnx\",\n)\n</code></pre> <p>\ud83d\udd0e See it visualized:  <code>my_callable.onnx</code></p>"},{"location":"user_guide/getting_started/#onnx-functions-minimal-example","title":"ONNX Functions \u2014 Minimal Example","text":"<p>ONNX functions help encapsulate reusable subgraphs. Simply use the <code>@onnx_function</code> decorator to make your callable an ONNX function.</p> <pre><code>from flax import nnx\nfrom jax2onnx import onnx_function, to_onnx\n\n# just an @onnx_function decorator to make your callable an ONNX function\n@onnx_function\nclass MLPBlock(nnx.Module):\n  def __init__(self, dim, *, rngs):\n    self.linear1 = nnx.Linear(dim, dim, rngs=rngs)\n    self.linear2 = nnx.Linear(dim, dim, rngs=rngs)\n    self.batchnorm = nnx.BatchNorm(dim, rngs=rngs)\n  def __call__(self, x):\n    return nnx.gelu(self.linear2(self.batchnorm(nnx.gelu(self.linear1(x)))))\n\n# Use it inside another module\nclass MyModel(nnx.Module):\n  def __init__(self, dim, *, rngs):\n    self.block1 = MLPBlock(dim, rngs=rngs)\n    self.block2 = MLPBlock(dim, rngs=rngs)\n  def __call__(self, x):\n    return self.block2(self.block1(x))\n\ncallable = MyModel(256, rngs=nnx.Rngs(0))\nto_onnx(\n    callable,\n    [(100, 256)],\n    return_mode=\"file\",\n    output_path=\"model_with_function.onnx\",\n)\n</code></pre> <p>\ud83d\udd0e See it visualized: <code>model_with_function.onnx</code></p>"},{"location":"user_guide/getting_started/#troubleshooting","title":"Troubleshooting","text":"<p>If conversion doesn't work out of the box, it could be due to:</p> <ul> <li> <p>Non-dynamic function references:   JAXPR-based conversion requires function references to be resolved dynamically at call-time. Solution: Wrap your function call inside a lambda to enforce dynamic resolution:   <code>python   my_dynamic_callable_function = lambda x: original_function(x)</code></p> </li> <li> <p>Unsupported primitives:   The callable may use a primitive not yet or not fully supported by <code>jax2onnx</code>. Solution: Write a plugin to handle the unsupported function (this is straightforward!).</p> </li> </ul> <p>Looking for provenance details while debugging? Check out the new Stacktrace Metadata guide.</p>"},{"location":"user_guide/stacktrace_metadata/","title":"Stacktrace Metadata Levels","text":"<p>When <code>JAX2ONNX_ENABLE_STACKTRACE_METADATA</code> is enabled, the converter emits provenance metadata on each ONNX node. The system has two tiers so you can choose between a lightweight overview and a full debug trace.</p>"},{"location":"user_guide/stacktrace_metadata/#level-1-lightweight","title":"Level 1 \u2014 Lightweight","text":"<ul> <li><code>pkg.jax2onnx.callsite</code> stores the originating user function and line (<code>function:line</code>).</li> <li><code>pkg.jax2onnx.plugin</code> stores the plugin (or lowering helper) and the line where it emitted the node (<code>Plugin.lower:line</code>).</li> </ul> <p>This is the default reduced payload surfaced in tools like Netron:</p> <p></p>"},{"location":"user_guide/stacktrace_metadata/#how-to-enable","title":"How to enable","text":"<pre><code>JAX2ONNX_ENABLE_STACKTRACE_METADATA=1 python -m your_export_script\n</code></pre> <p>To convert a function inline:</p> <pre><code>JAX2ONNX_ENABLE_STACKTRACE_METADATA=1 python - &lt;&lt;'PY'\nimport jax\nimport jax.numpy as jnp\nfrom jax2onnx import to_onnx\n\ndef fn(x):\n    return jnp.sin(x)\n\nto_onnx(\n    fn,\n    [jax.ShapeDtypeStruct((2,), jnp.float32)],\n    return_mode=\"file\",\n    output_path=\"fn.onnx\",\n)\nPY\n</code></pre> <p>Open the resulting ONNX in Netron to see the callsite/plugin metadata per node.</p>"},{"location":"user_guide/stacktrace_metadata/#level-2-verbose","title":"Level 2 \u2014 Verbose","text":"<p>Set <code>JAX2ONNX_STACKTRACE_DETAIL=full</code> to capture the complete Python stack traces in addition to the Level\u00a01 metadata:</p> <ul> <li><code>pkg.jax2onnx.stacktrace</code> holds the full Python call stack at the point the node was created.</li> <li><code>pkg.jax2onnx.jax_traceback</code> mirrors the JAX equation traceback.</li> </ul> <p></p>"},{"location":"user_guide/stacktrace_metadata/#how-to-enable_1","title":"How to enable","text":"<pre><code>JAX2ONNX_ENABLE_STACKTRACE_METADATA=1 \\\nJAX2ONNX_STACKTRACE_DETAIL=full \\\npython -m your_export_script\n</code></pre> <p>Or inline:</p> <pre><code>JAX2ONNX_ENABLE_STACKTRACE_METADATA=1 \\\nJAX2ONNX_STACKTRACE_DETAIL=full \\\npython - &lt;&lt;'PY'\nimport jax\nimport jax.numpy as jnp\nfrom jax2onnx import to_onnx\n\ndef fn(x):\n    return jnp.sin(x)\n\nto_onnx(\n    fn,\n    [jax.ShapeDtypeStruct((2,), jnp.float32)],\n    return_mode=\"file\",\n    output_path=\"fn_full.onnx\",\n)\nPY\n</code></pre> <p>The ONNX nodes will now carry both the lightweight metadata and the full Python/JAX trace strings for deep debugging.</p> <p>Use Level\u00a01 for routine debugging, and switch to Level\u00a02 when you need to drill into the full call flow across user code and plugin lowerings.</p>"},{"location":"user_guide/stacktrace_metadata/#background-implementation-notes","title":"Background &amp; Implementation Notes","text":"<ul> <li>Originated from issue #109, requesting stacktrace metadata to help debug exported ONNX graphs.</li> <li>Metadata keys live under the <code>pkg.jax2onnx.*</code> namespace and are attached per ONNX node during lowering.</li> <li>Level\u00a01 keeps exports lightweight (<code>callsite</code> + <code>plugin</code>), while Level\u00a02 adds full Python/JAX stack dumps.</li> <li>Tests/fixtures must be regenerated whenever metadata changes (<code>JAX2ONNX_ENABLE_STACKTRACE_METADATA=1</code> during updates) to keep <code>expect_graph</code> references in sync.</li> </ul>"},{"location":"user_guide/supported_components/","title":"Supported JAX/ONNX Components","text":"JAX Component ONNX Components Testcases Since core.custom_jvp_generic \u2796 <code>custom_jvp_square</code> \u2705<code>custom_jvp_square_f64</code> \u2705 0.7.1 core.custom_vjp_generic \u2796 <code>custom_vjp_square</code> \u2705<code>custom_vjp_square_f64</code> \u2705 0.7.1 core.dim_as_value CastGatherReshapeShape <code>dim_as_value_dynamic</code> \u2705<code>dim_as_value_dynamic_f64</code> \u2705<code>dim_as_value</code> \u2705<code>dim_as_value_f64</code> \u2705 0.5.0 core.jit_inline \u2796 <code>jit_identity</code> \u2705<code>jit_identity_f64</code> \u2705 0.9.0 eqx.conv Conv <code>eqx_conv2d_nchw</code> \u2705<code>eqx_conv2d_batched_nchw</code> \u2705 0.10.0 eqx.dropout DropoutNot <code>eqx_dropout_inference_mode</code> \u2705<code>eqx_dropout_inference_mode_f64</code> \u2705<code>eqx_dropout_training_mode</code> \u2705<code>eqx_dropout_training_mode_f64</code> \u2705<code>eqx_dropout_dynamic_inference</code> \u2705<code>eqx_dropout_dynamic_inference_f64</code> \u2705<code>eqx_dropout_batched_inference_dynamic</code> \u2705<code>eqx_dropout_batched_inference_dynamic_f64</code> \u2705<code>eqx_dropout_batched_inference</code> \u2705<code>eqx_dropout_batched_inference_f64</code> \u2705 0.8.0 eqx.identity Identity <code>eqx_identity_static</code> \u2705<code>eqx_identity_static_f64</code> \u2705<code>eqx_identity_symbolic_batch_dynamic</code> \u2705<code>eqx_identity_symbolic_batch_dynamic_f64</code> \u2705<code>eqx_identity_symbolic_batch</code> \u2705<code>eqx_identity_symbolic_batch_f64</code> \u2705 0.8.0 eqx.layer_norm LayerNormalization <code>layer_norm</code> \u2705<code>layer_norm_f64</code> \u2705<code>layer_norm_multiaxis</code> \u2705<code>layer_norm_multiaxis_f64</code> \u2705<code>batched_layer_norm_dynamic</code> \u2705<code>batched_layer_norm_dynamic_f64</code> \u2705<code>batched_layer_norm</code> \u2705<code>batched_layer_norm_f64</code> \u2705<code>layer_norm_no_bias_no_scale</code> \u2705<code>layer_norm_no_bias_no_scale_f64</code> \u2705 0.8.0 eqx.linear GemmReshape <code>eqx_linear_symbolic_batch_dynamic</code> \u2705<code>eqx_linear_symbolic_batch_dynamic_f64</code> \u2705<code>eqx_linear_symbolic_batch</code> \u2705<code>eqx_linear_symbolic_batch_f64</code> \u2705<code>eqx_linear_no_bias_symbolic_batch_dynamic</code> \u2705<code>eqx_linear_no_bias_symbolic_batch_dynamic_f64</code> \u2705<code>eqx_linear_no_bias_symbolic_batch</code> \u2705<code>eqx_linear_no_bias_symbolic_batch_f64</code> \u2705<code>eqx_linear_no_bias_vector</code> \u2705<code>eqx_linear_no_bias_vector_f64</code> \u2705<code>eqx_linear_high_rank</code> \u2705<code>eqx_linear_high_rank_f64</code> \u2705<code>eqx_linear_vector</code> \u2705<code>eqx_linear_vector_f64</code> \u2705 0.8.0 eqx.multihead_attention GemmMatMulSoftmax <code>eqx_multihead_attention</code> \u2705<code>eqx_multihead_attention_core_dynamic</code> \u2705<code>eqx_multihead_attention_core_static</code> \u2705 0.10.0 eqx.rms_norm RMSNormalization <code>rms_norm_vector</code> \u2705<code>rms_norm_vector_f64</code> \u2705<code>rms_norm_no_affine</code> \u2705<code>rms_norm_no_affine_f64</code> \u2705 0.10.2 eqx.rotary_positional_embedding AddConcatMultiply <code>eqx_rotary_positional_embedding</code> \u2705<code>eqx_rotary_positional_embedding_heads</code> \u2705 0.10.0 jax_image.resize Resize <code>resize_linear</code> \u2705<code>resize_nearest</code> \u2705 0.10.0 jnp.add Add <code>add</code> \u2705<code>add_f64</code> \u2705<code>jnp_add_vector</code> \u2705<code>jnp_add_vector_f64</code> \u2705<code>jnp_add_broadcast</code> \u2705<code>jnp_add_broadcast_f64</code> \u2705<code>add_vmap_batching</code> \u2705<code>add_vmap_batching_f64</code> \u2705 0.8.0 jnp.arange Range <code>arange_data_dependent_indices</code> \u2705<code>arange_stop_only_concrete_input_val</code> \u2705<code>arange_stop_only_concrete_input_val_f64</code> \u2705<code>arange_start_stop_concrete_input_val</code> \u2705<code>arange_start_stop_concrete_input_val_f64</code> \u2705<code>arange_start_stop_step_concrete_input_val</code> \u2705<code>arange_start_stop_step_concrete_input_val_f64</code> \u2705<code>arange_float_concrete_input_val</code> \u2705<code>arange_float_concrete_input_val_f64</code> \u2705<code>arange_static_stop_only_int</code> \u2705<code>arange_static_stop_only_int_f64</code> \u2705<code>arange_static_stop_only_float</code> \u2705<code>arange_static_stop_only_float_f64</code> \u2705<code>arange_static_start_stop_int</code> \u2705<code>arange_static_start_stop_int_f64</code> \u2705<code>arange_static_start_stop_step_int</code> \u2705<code>arange_static_start_stop_step_int_f64</code> \u2705<code>arange_static_empty_result_pos_step</code> \u2705<code>arange_static_empty_result_pos_step_f64</code> \u2705<code>arange_static_empty_result_neg_step</code> \u2705<code>arange_static_empty_result_neg_step_f64</code> \u2705<code>arange_static_negative_step</code> \u2705<code>arange_static_negative_step_f64</code> \u2705<code>arange_static_float_step_explicit_dtype</code> \u2705<code>arange_static_float_step_explicit_dtype_f64</code> \u2705<code>arange_static_float_step_inferred_dtype</code> \u2705<code>arange_static_float_step_inferred_dtype_f64</code> \u2705<code>arange_static_stop_zero</code> \u2705<code>arange_static_stop_zero_f64</code> \u2705<code>arange_static_start_equals_stop</code> \u2705<code>arange_static_start_equals_stop_f64</code> \u2705<code>arange_static_large_numbers_int</code> \u2705<code>arange_static_large_numbers_int_f64</code> \u2705 0.5.2 jnp.clip MaxMin <code>clip_i32_scalar_bounds</code> \u2705<code>clip_i32_scalar_bounds_f64</code> \u2705<code>clip_f32_scalar_bounds_no_upcast_f64_mode</code> \u2705<code>clip_only_upper</code> \u2705<code>clip_only_upper_f64</code> \u2705<code>clip_only_lower</code> \u2705<code>clip_only_lower_f64</code> \u2705<code>clip_broadcast_bounds</code> \u2705 0.8.0 jnp.concatenate Concat <code>concatenate_basic</code> \u2705<code>concatenate_basic_f64</code> \u2705<code>concatenate_mixed_dtypes</code> \u2705<code>concatenate_mixed_dtypes_f64</code> \u2705<code>concatenate_with_explicit_dtype</code> \u2705<code>concatenate_with_explicit_dtype_casts_inputs</code> \u2705<code>concatenate_abstract_middle_dim_dynamic</code> \u2705<code>concatenate_abstract_middle_dim_dynamic_f64</code> \u2705<code>concatenate_abstract_middle_dim</code> \u2705<code>concatenate_abstract_middle_dim_f64</code> \u2705<code>concatenate_tile_and_symbolic_dynamic</code> \u2705<code>concatenate_tile_and_symbolic_dynamic_f64</code> \u2705<code>concatenate_tile_and_symbolic</code> \u2705<code>concatenate_tile_and_symbolic_f64</code> \u2705 0.8.0 jnp.conj Identity <code>jnp_conj_real</code> \u2705<code>jnp_conj_real_f64</code> \u2705<code>jnp_conj_complex64</code> \u2705<code>jnp_conj_complex64_f64</code> \u2705<code>conj_vmap_batching</code> \u2705<code>conj_vmap_batching_f64</code> \u2705 0.10.1 jnp.cumsum CumSum <code>jnp_cumsum_axis1</code> \u2705<code>jnp_cumsum_axis1_f64</code> \u2705<code>jnp_cumsum_reverse_dtype</code> \u2705<code>cumsum_axis2_i32</code> \u2705<code>cumsum_axis2_i32_f64</code> \u2705<code>cumsum_axis2_reverse_i32</code> \u2705<code>cumsum_axis2_reverse_i32_f64</code> \u2705<code>cumsum_vmap_batching</code> \u2705<code>cumsum_vmap_batching_f64</code> \u2705 0.8.0 jnp.einsum Einsum <code>einsum_vector_dot</code> \u2705<code>einsum_vector_dot_f64</code> \u2705<code>einsum_matrix_vector</code> \u2705<code>einsum_matrix_vector_f64</code> \u2705<code>einsum_matrix_matrix_dynamic</code> \u2705<code>einsum_matrix_matrix_dynamic_f64</code> \u2705<code>einsum_matrix_matrix</code> \u2705<code>einsum_matrix_matrix_f64</code> \u2705<code>einsum_transpose</code> \u2705<code>einsum_transpose_f64</code> \u2705<code>einsum_batch_transpose_dynamic</code> \u2705<code>einsum_batch_transpose_dynamic_f64</code> \u2705<code>einsum_batch_transpose</code> \u2705<code>einsum_batch_transpose_f64</code> \u2705<code>einsum_diag</code> \u2705<code>einsum_diag_f64</code> \u2705<code>einsum_sum_reduce</code> \u2705<code>einsum_sum_reduce_f64</code> \u2705<code>einsum_multi_operand</code> \u2705<code>einsum_multi_operand_f64</code> \u2705<code>einsum_attention_logits_orig_dynamic</code> \u2705<code>einsum_attention_logits_orig_dynamic_f64</code> \u2705<code>einsum_attention_logits_orig</code> \u2705<code>einsum_attention_logits_orig_f64</code> \u2705<code>einsum_attention_output_orig_dynamic</code> \u2705<code>einsum_attention_output_orig_dynamic_f64</code> \u2705<code>einsum_attention_output_orig</code> \u2705<code>einsum_attention_output_orig_f64</code> \u2705<code>einsum_attention_logits_batched_dynamic</code> \u2705<code>einsum_attention_logits_batched_dynamic_f64</code> \u2705<code>einsum_attention_logits_batched</code> \u2705<code>einsum_attention_logits_batched_f64</code> \u2705<code>einsum_attention_output_batched_dynamic</code> \u2705<code>einsum_attention_output_batched_dynamic_f64</code> \u2705<code>einsum_attention_output_batched</code> \u2705<code>einsum_attention_output_batched_f64</code> \u2705<code>einsum_ellipsis_rank_mismatch</code> \u2705<code>einsum_ellipsis_rank_mismatch_f64</code> \u2705<code>einsum_attention_logits_batched_rank_mismatch</code> \u2705<code>einsum_attention_logits_batched_rank_mismatch_f64</code> \u2705 0.1.0 jnp.fft DFT <code>jnp_fft_complex64</code> \u2705<code>jnp_fft_complex128</code> \u2705 0.10.1 jnp.ifft DFT <code>jnp_ifft_complex64</code> \u2705 0.10.1 jnp.linspace AddRange <code>linspace_static_basic</code> \u2705<code>linspace_static_basic_f64</code> \u2705<code>linspace_static_endpoint_false</code> \u2705<code>linspace_static_endpoint_false_f64</code> \u2705<code>linspace_static_int_inputs_default_dtype</code> \u2705<code>linspace_static_int_inputs_default_dtype_f64</code> \u2705<code>linspace_basic_f32</code> \u2705<code>linspace_basic_f32_f64</code> \u2705<code>linspace_endpoint_false_i32</code> \u2705<code>linspace_endpoint_false_i32_f64</code> \u2705<code>linspace_num_zero</code> \u2705<code>linspace_num_zero_f64</code> \u2705<code>linspace_num_one</code> \u2705<code>linspace_num_one_f64</code> \u2705<code>linspace_static_num_0</code> \u2705<code>linspace_static_num_0_f64</code> \u2705<code>linspace_static_num_1</code> \u2705<code>linspace_static_num_1_f64</code> \u2705<code>linspace_vmap_batching</code> \u2705<code>linspace_vmap_batching_f64</code> \u2705 0.5.2 jnp.matmul MatMul <code>matmul_1d</code> \u2705<code>matmul_1d_f64</code> \u2705<code>matmul_1d_2d</code> \u2705<code>matmul_1d_2d_f64</code> \u2705<code>matmul_2d</code> \u2705<code>matmul_2d_f64</code> \u2705<code>matmul_2d_1d</code> \u2705<code>matmul_2d_1d_f64</code> \u2705<code>matmul_3d</code> \u2705<code>matmul_3d_f64</code> \u2705<code>matmul_dynamic_dynamic</code> \u2705<code>matmul_dynamic_dynamic_f64</code> \u2705<code>matmul_dynamic</code> \u2705<code>matmul_dynamic_f64</code> \u2705<code>matmul_dynamic_a_dynamic</code> \u2705<code>matmul_dynamic_a_dynamic_f64</code> \u2705<code>matmul_dynamic_a</code> \u2705<code>matmul_dynamic_a_f64</code> \u2705<code>matmul_complex64</code> \u2705<code>matmul_complex64_f64</code> \u2705<code>matmul_vmap_batching</code> \u2705<code>matmul_vmap_batching_f64</code> \u2705 0.1.0 jnp.outer Mul <code>outer_vector</code> \u2705<code>outer_vector_f64</code> \u2705<code>outer</code> \u2705<code>outer_f64</code> \u2705<code>outer_vmap_batching</code> \u2705<code>outer_vmap_batching_f64</code> \u2705 0.10.0 jnp.pow Pow <code>jnp_pow_vector</code> \u2705<code>jnp_pow_vector_f64</code> \u2705<code>pow_jnp_pow</code> \u2705<code>pow_jnp_pow_f64</code> \u2705<code>pow_vmap_batching</code> \u2705<code>pow_vmap_batching_f64</code> \u2705 0.8.0 jnp.power Pow <code>jnp_power_vector</code> \u2705<code>jnp_power_vector_f64</code> \u2705<code>pow_jnp_power</code> \u2705<code>pow_jnp_power_f64</code> \u2705<code>power_vmap_batching</code> \u2705<code>power_vmap_batching_f64</code> \u2705 0.8.0 jnp.prod ReduceProd <code>basic_prod</code> \u2705<code>basic_prod_f64</code> \u2705<code>prod_with_axis</code> \u2705<code>prod_with_axis_f64</code> \u2705<code>prod_with_keepdims</code> \u2705<code>prod_with_keepdims_f64</code> \u2705<code>jnp_prod_basic</code> \u2705<code>jnp_prod_basic_f64</code> \u2705<code>jnp_prod_axis</code> \u2705<code>jnp_prod_axis_f64</code> \u2705<code>jnp_prod_keepdims</code> \u2705<code>jnp_prod_keepdims_f64</code> \u2705<code>prod_vmap_batching</code> \u2705<code>prod_vmap_batching_f64</code> \u2705 0.8.0 jnp.reshape Reshape <code>reshape_1</code> \u2705<code>reshape_1_f64</code> \u2705<code>reshape_2</code> \u2705<code>reshape_2_f64</code> \u2705<code>reshape_3</code> \u2705<code>reshape_3_f64</code> \u2705<code>reshape_4_dynamic</code> \u2705<code>reshape_4_dynamic_f64</code> \u2705<code>reshape_4</code> \u2705<code>reshape_4_f64</code> \u2705<code>reshape_to_scalar</code> \u2705<code>reshape_to_scalar_f64</code> \u2705<code>reshape_from_scalar</code> \u2705<code>reshape_from_scalar_f64</code> \u2705<code>reshape_cnn_dynamic</code> \u2705<code>reshape_cnn_dynamic_f64</code> \u2705<code>reshape_cnn</code> \u2705<code>reshape_cnn_f64</code> \u2705<code>reshape_valid_flatten_trailing</code> \u2705<code>reshape_valid_flatten_trailing_f64</code> \u2705<code>reshape_with_target_shape_from_symbolic_dim_computation</code> \u2705<code>reshape_with_target_shape_from_symbolic_dim_computation_f64</code> \u2705<code>reshape_basic</code> \u2705<code>reshape_basic_f64</code> \u2705<code>reshape_infer</code> \u2705<code>reshape_infer_f64</code> \u2705<code>reshape_symbolic_flatten_dynamic</code> \u2705<code>reshape_symbolic_flatten_dynamic_f64</code> \u2705<code>reshape_symbolic_flatten</code> \u2705<code>reshape_symbolic_flatten_f64</code> \u2705<code>reshape_vmap_batching_issue_144</code> \u2705<code>reshape_vmap_batching_issue_144_f64</code> \u2705 0.1.0 jnp.rfft DFT <code>jnp_rfft_float32</code> \u2705 0.10.1 jnp.select Where <code>select_simple</code> \u2705<code>select_simple_f64</code> \u2705<code>select_broadcast</code> \u2705<code>select_broadcast_f64</code> \u2705<code>select_gpt2_attention_mask_dynamic</code> \u2705<code>select_gpt2_attention_mask_dynamic_f64</code> \u2705<code>select_gpt2_attention_mask</code> \u2705<code>select_gpt2_attention_mask_f64</code> \u2705<code>select_basic</code> \u2705<code>select_basic_f64</code> \u2705<code>select_vmap_batching</code> \u2705<code>select_vmap_batching_f64</code> \u2705 0.7.1 jnp.shape Shape <code>shape_basic</code> \u2705<code>shape_basic_f64</code> \u2705<code>shape_dynamic_dynamic</code> \u2705<code>shape_dynamic_dynamic_f64</code> \u2705<code>shape_dynamic</code> \u2705<code>shape_dynamic_f64</code> \u2705<code>shape_vmap_batching</code> \u2705<code>shape_vmap_batching_f64</code> \u2705 0.4.0 jnp.sort Sort <code>sort_1d</code> \u2705<code>sort_1d_f64</code> \u2705<code>sort_2d_axis0</code> \u2705<code>sort_2d_axis0_f64</code> \u2705<code>sort_basic</code> \u2705<code>sort_basic_f64</code> \u2705<code>sort_vmap_batching</code> \u2705<code>sort_vmap_batching_f64</code> \u2705 0.5.2 jnp.split Split <code>split_by_sections</code> \u2705<code>split_by_sections_f64</code> \u2705<code>split_by_indices</code> \u2705<code>split_by_indices_f64</code> \u2705<code>split_by_indices_symbolic_dynamic</code> \u2705<code>split_by_indices_symbolic_dynamic_f64</code> \u2705<code>split_by_indices_symbolic</code> \u2705<code>split_by_indices_symbolic_f64</code> \u2705<code>split_sections</code> \u2705<code>split_sections_f64</code> \u2705<code>split_indices_numpy</code> \u2705<code>split_indices_numpy_f64</code> \u2705 0.7.2 jnp.squeeze Squeeze <code>squeeze_single_dim</code> \u2705<code>squeeze_single_dim_f64</code> \u2705<code>squeeze_multiple_dims</code> \u2705<code>squeeze_multiple_dims_f64</code> \u2705<code>squeeze_vit_output</code> \u2705<code>squeeze_vit_output_f64</code> \u2705<code>squeeze_dynamic_batch_dynamic</code> \u2705<code>squeeze_dynamic_batch_dynamic_f64</code> \u2705<code>squeeze_dynamic_batch</code> \u2705<code>squeeze_dynamic_batch_f64</code> \u2705<code>squeeze_all_dims</code> \u2705<code>squeeze_all_dims_f64</code> \u2705<code>squeeze_negative_axis</code> \u2705<code>squeeze_negative_axis_f64</code> \u2705<code>squeeze_negative_axis_tuple</code> \u2705<code>squeeze_negative_axis_tuple_f64</code> \u2705<code>squeeze_dynamic_and_negative_axis_dynamic</code> \u2705<code>squeeze_dynamic_and_negative_axis_dynamic_f64</code> \u2705<code>squeeze_dynamic_and_negative_axis</code> \u2705<code>squeeze_dynamic_and_negative_axis_f64</code> \u2705<code>squeeze_vmap_batching</code> \u2705<code>squeeze_vmap_batching_f64</code> \u2705 0.1.0 jnp.stack ConcatUnsqueeze <code>stack_axis_0</code> \u2705<code>stack_axis_0_f64</code> \u2705<code>stack_axis_1</code> \u2705<code>stack_axis_1_f64</code> \u2705<code>stack_negative_axis</code> \u2705<code>stack_negative_axis_f64</code> \u2705<code>stack_scalars</code> \u2705<code>stack_scalars_f64</code> \u2705<code>jnp_stack_axis0</code> \u2705<code>jnp_stack_axis0_f64</code> \u2705<code>jnp_stack_axis1</code> \u2705<code>jnp_stack_axis1_f64</code> \u2705<code>jnp_stack_negative_axis</code> \u2705<code>jnp_stack_negative_axis_f64</code> \u2705<code>jnp_stack_scalars</code> \u2705<code>jnp_stack_scalars_f64</code> \u2705 0.8.0 jnp.take Gather <code>take_data_dependent_indices</code> \u2705<code>take_basic_axis1</code> \u2705<code>take_basic_axis1_f64</code> \u2705<code>take_vmap_batching</code> \u2705<code>take_vmap_batching_f64</code> \u2705 0.7.0 jnp.tile Tile <code>tile_repeats</code> \u2705<code>tile_repeats_f64</code> \u2705<code>tile_a</code> \u2705<code>tile_a_f64</code> \u2705<code>tile_b</code> \u2705<code>tile_b_f64</code> \u2705<code>tile_c</code> \u2705<code>tile_c_f64</code> \u2705<code>tile_d</code> \u2705<code>tile_d_f64</code> \u2705<code>tile_dynamic_input_static</code> \u2705<code>tile_dynamic_input_static_f64</code> \u2705<code>tile_dynamic_input_dynamic</code> \u2705<code>tile_dynamic_input_dynamic_f64</code> \u2705<code>tile_dynamic_input</code> \u2705<code>tile_dynamic_input_f64</code> \u2705<code>tile_pad</code> \u2705<code>tile_pad_f64</code> \u2705<code>tile_param_symbolic_dynamic</code> \u2705<code>tile_param_symbolic_dynamic_f64</code> \u2705<code>tile_param_symbolic</code> \u2705<code>tile_param_symbolic_f64</code> \u2705<code>tile_with_symbolic_repeats_static</code> \u2705<code>tile_with_symbolic_repeats_static_f64</code> \u2705<code>tile_with_symbolic_repeats_dynamic</code> \u2705<code>tile_with_symbolic_repeats_dynamic_f64</code> \u2705<code>tile_with_symbolic_repeats</code> \u2705<code>tile_with_symbolic_repeats_f64</code> \u2705<code>jnp_tile_basic</code> \u2705<code>jnp_tile_basic_f64</code> \u2705<code>jnp_tile_scalar_repeats</code> \u2705<code>jnp_tile_scalar_repeats_f64</code> \u2705<code>jnp_tile_pad_rank</code> \u2705<code>jnp_tile_pad_rank_f64</code> \u2705<code>jnp_tile_symbolic_dynamic</code> \u2705<code>jnp_tile_symbolic_dynamic_f64</code> \u2705<code>jnp_tile_symbolic</code> \u2705<code>jnp_tile_symbolic_f64</code> \u2705<code>tile_vmap_batching</code> \u2705<code>tile_vmap_batching_f64</code> \u2705 0.8.0 jnp.transpose Transpose <code>transpose_basic</code> \u2705<code>transpose_basic_f64</code> \u2705<code>transpose_reverse_default</code> \u2705<code>transpose_reverse_default_f64</code> \u2705<code>transpose_high_dim</code> \u2705<code>transpose_high_dim_f64</code> \u2705<code>transpose_3d</code> \u2705<code>transpose_3d_f64</code> \u2705<code>transpose_4d</code> \u2705<code>transpose_4d_f64</code> \u2705<code>transpose_no_axes</code> \u2705<code>transpose_no_axes_f64</code> \u2705<code>transpose_reverse</code> \u2705<code>transpose_reverse_f64</code> \u2705<code>transpose_square_matrix</code> \u2705<code>transpose_square_matrix_f64</code> \u2705<code>transpose_vmap_batching</code> \u2705<code>transpose_vmap_batching_f64</code> \u2705 0.1.0 jnp.unstack SplitSqueeze <code>unstack_axis_0</code> \u2705<code>unstack_axis_0_f64</code> \u2705<code>unstack_axis_0_f64</code> \u2705<code>unstack_axis_1</code> \u2705<code>unstack_axis_1_f64</code> \u2705<code>unstack_axis_1_f64</code> \u2705<code>unstack_negative_axis</code> \u2705<code>unstack_negative_axis_f64</code> \u2705<code>unstack_vmap_batching</code> \u2705<code>unstack_vmap_batching_f64</code> \u2705 0.7.1 jnp.where Where <code>where_simple</code> \u2705<code>where_simple_f64</code> \u2705<code>where_broadcast</code> \u2705<code>where_broadcast_f64</code> \u2705<code>where_gpt_mask_scores_literal_else_dynamic</code> \u2705<code>where_gpt_mask_scores_literal_else_dynamic_f64</code> \u2705<code>where_gpt_mask_scores_literal_else</code> \u2705<code>where_gpt_mask_scores_literal_else_f64</code> \u2705<code>where_multidim_condition_scalar_branches_broadcast</code> \u2705<code>where_multidim_condition_scalar_branches_broadcast_f64</code> \u2705<code>where_A</code> \u2705<code>where_A_f64</code> \u2705<code>where_B</code> \u2705<code>where_B_f64</code> \u2705<code>where_gpt_mask_scores_scalar_else_dynamic</code> \u2705<code>where_gpt_mask_scores_scalar_else_dynamic_f64</code> \u2705<code>where_gpt_mask_scores_scalar_else</code> \u2705<code>where_gpt_mask_scores_scalar_else_f64</code> \u2705<code>where_int_condition_cast</code> \u2705<code>where_int_condition_cast_f64</code> \u2705<code>where_literal_else_pyfloat</code> \u2705<code>where_literal_else_pyfloat_f64</code> \u2705<code>where_jax_int_literals_broadcast_f64_mode</code> \u2705<code>where_dtype_mismatch_f64_vs_i32_promote</code> \u2705<code>jnp_where_basic</code> \u2705<code>jnp_where_basic_f64</code> \u2705<code>jnp_where_broadcast</code> \u2705<code>jnp_where_broadcast_f64</code> \u2705<code>jnp_where_scalar_else</code> \u2705<code>jnp_where_scalar_else_f64</code> \u2705<code>where_vmap_batching</code> \u2705<code>where_vmap_batching_f64</code> \u2705 0.8.0 lax.abs Abs <code>abs</code> \u2705<code>abs_f64</code> \u2705 0.5.0 lax.add Add <code>add</code> \u2705<code>add_f64</code> \u2705<code>add_const</code> \u2705<code>add_const_f64</code> \u2705<code>add_complex64</code> \u2705<code>add_complex64_f64</code> \u2705 0.2.0 lax.add_any Add <code>add_any_via_jvp_on_mul</code> \u2705<code>add_any_via_jvp_on_mul_f64</code> \u2705 0.8.0 lax.and AndBitwiseAnd <code>and_bool</code> \u2705<code>and_bool_f64</code> \u2705<code>and_int</code> \u2705<code>and_int_f64</code> \u2705 0.6.5 lax.argmax ArgMax <code>argmax_float_axis0</code> \u2705<code>argmax_float_axis0_f64</code> \u2705<code>argmax_float_axis1</code> \u2705<code>argmax_float_axis1_f64</code> \u2705<code>argmax_boolean_input_axis0_specific_values</code> \u2705<code>argmax_boolean_input_axis0_specific_values_f64</code> \u2705<code>argmax_boolean_input_axis1_specific_values</code> \u2705<code>argmax_boolean_input_axis1_specific_values_f64</code> \u2705<code>argmax_boolean_random_input_axis0</code> \u2705<code>argmax_boolean_random_input_axis0_f64</code> \u2705 0.2.0 lax.argmin ArgMin <code>argmin_test1</code> \u2705<code>argmin_test1_f64</code> \u2705<code>argmin_test2</code> \u2705<code>argmin_test2_f64</code> \u2705 0.2.0 lax.bitcast_convert_type Bitcast <code>bitcast_scalar_f32_to_i32</code> \u2705<code>bitcast_scalar_f32_to_i32_f64</code> \u2705<code>bitcast_tensor_i32_to_f32</code> \u2705<code>bitcast_tensor_i32_to_f32_f64</code> \u2705 0.7.2 lax.bitwise_not BitwiseNotNot <code>bitwise_not_bool</code> \u2705<code>bitwise_not_bool_f64</code> \u2705<code>bitwise_not_i32</code> \u2705<code>bitwise_not_i32_f64</code> \u2705 0.7.5 lax.broadcast_in_dim ExpandIdentityReshape <code>broadcast_in_dim</code> \u2705<code>broadcast_in_dim_f64</code> \u2705<code>broadcast_in_dim_2d_to_3d</code> \u2705<code>broadcast_in_dim_2d_to_3d_f64</code> \u2705<code>broadcast_in_dim_scalar</code> \u2705<code>broadcast_in_dim_scalar_f64</code> \u2705<code>broadcast_in_dim_batch_dynamic</code> \u2705<code>broadcast_in_dim_batch_dynamic_f64</code> \u2705<code>broadcast_in_dim_batch</code> \u2705<code>broadcast_in_dim_batch_f64</code> \u2705<code>broadcast_in_dim_dynamic_B_dynamic</code> \u2705<code>broadcast_in_dim_dynamic_B_dynamic_f64</code> \u2705<code>broadcast_in_dim_dynamic_B</code> \u2705<code>broadcast_in_dim_dynamic_B_f64</code> \u2705 0.2.0 lax.clamp MaxMin <code>clamp_i32_scalar_bounds</code> \u2705<code>clamp_i32_scalar_bounds_f64</code> \u2705<code>clamp_scalar_float_bounds_match_x</code> \u2705<code>clamp_scalar_float_bounds_match_x_f64</code> \u2705<code>clamp_vector_bounds_match</code> \u2705<code>clamp_pyint_bounds_promote_to_x_dtype</code> \u2705<code>clamp_pyint_bounds_promote_to_x_dtype_f64</code> \u2705 0.7.5 lax.concatenate CastConcat <code>concatenate</code> \u2705<code>concatenate_f64</code> \u2705<code>concatenate_axis1_dynamic</code> \u2705<code>concatenate_axis1_dynamic_f64</code> \u2705<code>concatenate_axis1</code> \u2705<code>concatenate_axis1_f64</code> \u2705<code>concatenate_axis0</code> \u2705<code>concatenate_axis0_f64</code> \u2705<code>concatenate_3d</code> \u2705<code>concatenate_3d_f64</code> \u2705<code>concatenate_internal_int32_then_cast_to_f32_zeroarg</code> \u2705 0.2.0 lax.cond If <code>cond_scalar</code> \u2705<code>cond_scalar_f64</code> \u2705<code>cond_multiple_operands_in_tuple</code> \u2705<code>cond_multiple_operands_in_tuple_f64</code> \u2705<code>cond_my_new_complex_scenario</code> \u2705<code>cond_my_new_complex_scenario_f64</code> \u2705<code>cond_nested_conditional</code> \u2705<code>cond_nested_conditional_f64</code> \u2705<code>cond_variables</code> \u2705<code>cond_variables_f64</code> \u2705<code>cond_internal_constant_f64</code> \u2705<code>cond_passthrough_identity</code> \u2705<code>cond_passthrough_identity_f64</code> \u2705<code>cond_with_scatter</code> \u2705<code>cond_with_scatter_f64</code> \u2705 0.5.1 lax.conj Identity <code>conj_real</code> \u2705<code>conj_real_f64</code> \u2705<code>conj_complex64</code> \u2705<code>conj_complex64_f64</code> \u2705 0.10.1 lax.conv Conv <code>conv</code> \u2705<code>conv2</code> \u2705<code>conv_nchw</code> \u2705<code>conv_nhwc</code> \u2705<code>conv_general_dilated_nhwc_output</code> \u2705<code>conv_complex64</code> \u2705<code>conv_complex64_nhwc</code> \u2705<code>conv_complex128_grouped</code> \u2705 0.2.0 lax.convert_element_type Cast <code>convert_element_type</code> \u2705<code>convert_element_type_f64</code> \u2705 0.2.0 lax.copy Identity <code>copy_float32_array</code> \u2705<code>copy_int64_scalar</code> \u2705 lax.cos Cos <code>cos</code> \u2705<code>cos_f64</code> \u2705 0.4.4 lax.cosh Cosh <code>cosh</code> \u2705<code>cosh_f64</code> \u2705 0.4.4 lax.cumsum CumSum <code>cumsum_i32_axis2</code> \u2705<code>cumsum_i32_axis2_f64</code> \u2705<code>cumsum_f32_axism1_reverse</code> \u2705<code>cumsum_f32_axism1_reverse_f64</code> \u2705 0.7.4 lax.device_put Identity <code>device_put_array</code> \u2705<code>device_put_array_f64</code> \u2705<code>device_put_scalar</code> \u2705<code>device_put_scalar_f64</code> \u2705 0.4.0 lax.div Div <code>div</code> \u2705<code>div_f64</code> \u2705<code>div_const</code> \u2705<code>div_const_f64</code> \u2705<code>div_complex64</code> \u2705<code>div_complex64_f64</code> \u2705 0.2.0 lax.dot_general EinsumMatMul/Gemm <code>dot_contract_nm</code> \u2705<code>dot_contract_nm_f64</code> \u2705<code>dot_contract_min</code> \u2705<code>dot_contract_min_f64</code> \u2705<code>dot_general</code> \u2705<code>dot_general_f64</code> \u2705<code>dot_general_lhs1_rhs1</code> \u2705<code>dot_general_lhs1_rhs1_f64</code> \u2705<code>dot_double_contract</code> \u2705<code>dot_double_contract_f64</code> \u2705<code>dot_batched_double_contract</code> \u2705<code>dot_batched_double_contract_f64</code> \u2705<code>dot_highrank_batch</code> \u2705<code>dot_highrank_batch_f64</code> \u2705<code>dot_contract_inner_lhs_with_middle_rhs</code> \u2705<code>dot_contract_inner_lhs_with_middle_rhs_f64</code> \u2705<code>dot_outer_product</code> \u2705<code>dot_outer_product_f64</code> \u2705<code>dot_full_contract_scalar</code> \u2705<code>dot_full_contract_scalar_f64</code> \u2705<code>dot_general_complex_matmul</code> \u2705<code>dot_general_complex_matmul_f64</code> \u2705 0.2.0 lax.dynamic_slice Slice <code>dynamic_slice_test1</code> \u2705<code>dynamic_slice_test1_f64</code> \u2705<code>dynamic_slice_2d</code> \u2705<code>dynamic_slice_2d_f64</code> \u2705<code>dynamic_slice_3d</code> \u2705<code>dynamic_slice_3d_f64</code> \u2705<code>dynamic_slice_vit_like_dynamic</code> \u2705<code>dynamic_slice_vit_like_dynamic_f64</code> \u2705<code>dynamic_slice_vit_like</code> \u2705<code>dynamic_slice_vit_like_f64</code> \u2705 0.1.0 lax.dynamic_update_slice ScatterND <code>dus_1d_scalar_update</code> \u2705<code>dus_1d_scalar_update_f64</code> \u2705<code>dus_1d_block_update</code> \u2705<code>dus_1d_block_update_f64</code> \u2705<code>dus_2d_block_update</code> \u2705<code>dus_2d_block_update_f64</code> \u2705<code>dus_3d_block_update</code> \u2705<code>dus_3d_block_update_f64</code> \u2705<code>dus_4d_block_update</code> \u2705<code>dus_4d_block_update_f64</code> \u2705 0.8.1 lax.eq Equal <code>eq</code> \u2705<code>eq_f64</code> \u2705 0.2.0 lax.erf Erf <code>erf</code> \u2705 0.4.4 lax.exp Exp <code>exp</code> \u2705<code>exp_f64</code> \u2705 0.2.0 lax.fft DFT <code>fft_complex64_1d</code> \u2705<code>fft_complex64_len8</code> \u2705<code>fft_complex64_batch</code> \u2705<code>fft_complex128_1d</code> \u2705<code>fft_complex128_len8</code> \u2705<code>ifft_complex64_1d</code> \u2705<code>ifft_complex64_len8</code> \u2705<code>ifft_complex128_batch</code> \u2705<code>rfft_real32_1d</code> \u2705<code>rfft_real64_len8</code> \u2705<code>irfft_complex64_1d</code> \u2705<code>irfft_complex128_len8</code> \u2705 0.10.1 lax.fori_loop Loop <code>fori_loop_counter</code> \u2705<code>fori_loop_counter_f64</code> \u2705<code>fori_loop_zero</code> \u2705<code>fori_loop_zero_f64</code> \u2705<code>fori_loop_vector</code> \u2705<code>fori_loop_vector_f64</code> \u2705<code>fori_loop_example</code> \u2705<code>fori_loop_example_f64</code> \u2705<code>fori_loop_test</code> \u2705<code>fori_loop_test_f64</code> \u2705 0.5.1 lax.gather GatherND <code>gather_trig_where_pipeline_f64_indices_i64</code> \u2705<code>gather_trig_where_pipeline_f64_indices_i32</code> \u2705<code>gather_f64_data_i64_indices_output_is_f64</code> \u2705<code>gather_f64_data_i32_indices_cast_and_output_is_f64</code> \u2705<code>gather_static</code> \u2705<code>gather_static_f64</code> \u2705<code>gather_dynamic_batch_simple_index_dynamic</code> \u2705<code>gather_dynamic_batch_simple_index_dynamic_f64</code> \u2705<code>gather_dynamic_batch_simple_index</code> \u2705<code>gather_dynamic_batch_simple_index_f64</code> \u2705 0.2.0 lax.greater_equal GreaterOrEqual <code>greater_equal</code> \u2705<code>greater_equal_f64</code> \u2705 0.7.5 lax.gt Greater <code>gt</code> \u2705<code>gt_f64</code> \u2705 0.2.0 lax.imag Mul <code>imag_complex64_input</code> \u2705<code>imag_complex64_input_f64</code> \u2705 0.10.2 lax.integer_pow Pow <code>integer_pow</code> \u2705<code>integer_pow_f64</code> \u2705 0.2.0 lax.iota Range <code>iota_int32</code> \u2705<code>iota_int32_f64</code> \u2705<code>iota_float32</code> \u2705<code>iota_float32_f64</code> \u2705<code>broadcasted_iota</code> \u2705<code>broadcasted_iota_f64</code> \u2705 0.5.0 lax.less_equal LessOrEqual <code>less_equal</code> \u2705<code>less_equal_f64</code> \u2705 0.7.5 lax.log Log <code>log</code> \u2705<code>log_f64</code> \u2705 0.2.0 lax.log1p AddLog <code>log1p</code> \u2705<code>log1p_f64</code> \u2705 0.11.0 lax.logistic Sigmoid <code>lax_logistic_basic</code> \u2705<code>lax_logistic_basic_f64</code> \u2705 0.7.2 lax.lt Less <code>lt</code> \u2705<code>lt_f64</code> \u2705 0.2.0 lax.max Max <code>max</code> \u2705<code>max_f64</code> \u2705 0.2.0 lax.min Min <code>min_test1</code> \u2705<code>min_test1_f64</code> \u2705 0.1.0 lax.mul Mul <code>mul_test1</code> \u2705<code>mul_test1_f64</code> \u2705<code>mul_test2</code> \u2705<code>mul_test2_f64</code> \u2705<code>mul_pyfloat_promotes_to_array_dtype_f64</code> \u2705<code>mul_scalar_broadcast_promote_to_f64</code> \u2705<code>mul_complex128</code> \u2705<code>mul_complex64</code> \u2705 0.1.0 lax.ne EqualNot <code>ne</code> \u2705<code>ne_f64</code> \u2705 0.2.0 lax.neg Neg <code>neg</code> \u2705<code>neg_f64</code> \u2705 0.2.0 lax.or BitwiseOrOr <code>or_bool_vec</code> \u2705<code>or_bool_vec_f64</code> \u2705<code>or_int_vec</code> \u2705<code>or_int_vec_f64</code> \u2705 0.7.2 lax.pad Pad <code>pad_const_1d</code> \u2705<code>pad_const_1d_f64</code> \u2705<code>pad_const_2d</code> \u2705<code>pad_const_2d_f64</code> \u2705<code>pad_const_2d_cval</code> \u2705<code>pad_const_2d_cval_f64</code> \u2705<code>pad_inside_scan_smoke_f64</code> \u2705<code>pad_inside_nested_scan_smoke_f64</code> \u2705 0.8.0 lax.pjit \u2796 <code>pjit_inline_mul</code> \u2705<code>pjit_inline_mul_f64</code> \u2705<code>pjit_inline_tuple</code> \u2705<code>pjit_inline_tuple_f64</code> \u2705 0.1.0 lax.pow Pow <code>pow_basic</code> \u2705<code>pow_basic_f64</code> \u2705<code>pow_lax</code> \u2705<code>pow_lax_f64</code> \u2705 0.8.2 lax.real Identity <code>real_complex64_input</code> \u2705<code>real_complex64_input_f64</code> \u2705 0.10.2 lax.reduce_and ReduceMin <code>reduce_and_all_true</code> \u2705<code>reduce_and_all_true_f64</code> \u2705<code>reduce_and_one_false</code> \u2705<code>reduce_and_one_false_f64</code> \u2705<code>reduce_and_keepdims</code> \u2705<code>reduce_and_keepdims_f64</code> \u2705 0.6.1 lax.reduce_max ReduceMax <code>reduce_max</code> \u2705<code>reduce_max_f64</code> \u2705<code>reduce_max_allaxes</code> \u2705<code>reduce_max_allaxes_f64</code> \u2705<code>reduce_max_axes_input</code> \u2705<code>reduce_max_axes_input_f64</code> \u2705<code>reduce_max_keepdims</code> \u2705<code>reduce_max_keepdims_f64</code> \u2705 0.2.0 lax.reduce_min ReduceMin <code>reduce_min</code> \u2705<code>reduce_min_f64</code> \u2705<code>reduce_min_allaxes</code> \u2705<code>reduce_min_allaxes_f64</code> \u2705<code>reduce_min_keepdims</code> \u2705<code>reduce_min_keepdims_f64</code> \u2705 0.2.0 lax.reduce_or ReduceMax <code>reduce_or_all_false</code> \u2705<code>reduce_or_all_false_f64</code> \u2705<code>reduce_or_one_true</code> \u2705<code>reduce_or_one_true_f64</code> \u2705<code>reduce_or_keepdims</code> \u2705<code>reduce_or_keepdims_f64</code> \u2705 0.6.1 lax.reduce_prod ReduceProd <code>reduce_prod</code> \u2705<code>reduce_prod_f64</code> \u2705<code>reduce_prod_allaxes</code> \u2705<code>reduce_prod_allaxes_f64</code> \u2705<code>reduce_prod_dtype</code> \u2705<code>reduce_prod_dtype_f64</code> \u2705<code>reduce_prod_dtype_f64</code> \u2705<code>reduce_prod_dtype_f64_f64</code> \u2705<code>reduce_prod_keepdims</code> \u2705<code>reduce_prod_keepdims_f64</code> \u2705 0.6.1 lax.reduce_sum ReduceSum <code>reduce_sum</code> \u2705<code>reduce_sum_f64</code> \u2705<code>reduce_sum_allaxes</code> \u2705<code>reduce_sum_allaxes_f64</code> \u2705<code>reduce_sum_dtype</code> \u2705<code>reduce_sum_dtype_f64</code> \u2705<code>reduce_sum_dtype_f64</code> \u2705<code>reduce_sum_dtype_f64_f64</code> \u2705<code>reduce_sum_keepdims</code> \u2705<code>reduce_sum_keepdims_f64</code> \u2705 0.2.0 lax.reduce_window_sum Conv <code>reduce_window_sum_valid</code> \u2705<code>reduce_window_sum_same_padding</code> \u2705<code>reduce_window_sum_stride_dilate</code> \u2705<code>reduce_window_sum_int32</code> \u2705<code>reduce_window_sum_base_dilation</code> \u2705 0.10.1 lax.reduce_xor ModReduceSum <code>reduce_xor_all_false</code> \u2705<code>reduce_xor_all_false_f64</code> \u2705<code>reduce_xor_one_true</code> \u2705<code>reduce_xor_one_true_f64</code> \u2705<code>reduce_xor_two_true</code> \u2705<code>reduce_xor_two_true_f64</code> \u2705<code>reduce_xor_keepdims</code> \u2705<code>reduce_xor_keepdims_f64</code> \u2705 0.6.1 lax.rem DivMod <code>rem_int</code> \u2705<code>rem_int_f64</code> \u2705<code>rem_float</code> \u2705<code>rem_float_f64</code> \u2705<code>rem_int_neg</code> \u2705<code>rem_int_neg_f64</code> \u2705<code>rem_float_neg</code> \u2705<code>rem_float_neg_f64</code> \u2705 0.6.5 lax.remat2 \u2796 <code>remat2_scalar_sin_chain</code> \u2705<code>remat2_scalar_sin_chain_f64</code> \u2705<code>remat2_tuple_passthrough</code> \u2705<code>remat2_tuple_passthrough_f64</code> \u2705 0.6.5 lax.reshape Reshape <code>reshape_after_transpose_folds_const_shape</code> \u2705<code>reshape_after_transpose_folds_const_shape_f64</code> \u2705<code>reshape_flatten_trailing_folds_const_shape</code> \u2705<code>reshape_flatten_trailing_folds_const_shape_f64</code> \u2705<code>reshape</code> \u2705<code>reshape_f64</code> \u2705<code>reshape_valid_squeeze_middle_dim_from_problematic_source</code> \u2705<code>reshape_valid_squeeze_middle_dim_from_problematic_source_f64</code> \u2705<code>reshape_valid_flatten_trailing</code> \u2705<code>reshape_valid_flatten_trailing_f64</code> \u2705<code>reshape_with_target_shape_from_symbolic_dim_computation</code> \u2705<code>reshape_with_target_shape_from_symbolic_dim_computation_f64</code> \u2705<code>reshape_with_inferred_dimension_from_input_dynamic_dynamic</code> \u2705<code>reshape_with_inferred_dimension_from_input_dynamic_dynamic_f64</code> \u2705<code>reshape_with_inferred_dimension_from_input_dynamic</code> \u2705<code>reshape_with_inferred_dimension_from_input_dynamic_f64</code> \u2705<code>reshape_with_inferred_dimension_from_input</code> \u2705<code>reshape_with_inferred_dimension_from_input_f64</code> \u2705<code>reshape_merge_symbolic_with_static_and_check_name_dynamic</code> \u2705<code>reshape_merge_symbolic_with_static_and_check_name</code> \u2705 0.2.0 lax.rev Flip <code>rev_vector</code> \u2705<code>rev_vector_f64</code> \u2705<code>rev_matrix_axes01</code> \u2705<code>rev_matrix_axes01_f64</code> \u2705 0.7.5 lax.rsqrt DivSqrt <code>rsqrt</code> \u2705<code>rsqrt_f64</code> \u2705 0.10.2 lax.scan Scan <code>scan_identity_slice_helper</code> \u2705<code>scan_identity_slice_helper_f64</code> \u2705<code>scan_cumsum</code> \u2705<code>scan_cumsum_f64</code> \u2705<code>scan_carry_only</code> \u2705<code>scan_carry_only_f64</code> \u2705<code>scan_multiple_sequences</code> \u2705<code>scan_multiple_sequences_f64</code> \u2705<code>scan_multiple_carry</code> \u2705<code>scan_multiple_carry_f64</code> \u2705<code>scan_matrix_carry_multidim_xs</code> \u2705<code>scan_matrix_carry_multidim_xs_f64</code> \u2705<code>scan_no_xs</code> \u2705<code>scan_no_xs_f64</code> \u2705<code>scan_fn</code> \u2705<code>scan_fn_f64</code> \u2705<code>scan_jit_no_xs</code> \u2705<code>scan_jit_no_xs_f64</code> \u2705<code>scan_captured_scalar</code> \u2705<code>scan_captured_scalar_f64</code> \u2705<code>scan_rank0_sequence_vectorized</code> \u2705<code>scan_rank0_sequence_vectorized_f64</code> \u2705<code>scan_two_diff_lengths</code> \u2705<code>scan_two_diff_lengths_f64</code> \u2705<code>scan_two_diff_lengths_broadcast</code> \u2705<code>scan_two_diff_lengths_broadcast_f64</code> \u2705<code>scan_two_diff_lengths_with_broadcast</code> \u2705<code>scan_nested_len_mismatch</code> \u2705<code>scan_nested_len_mismatch_f64</code> \u2705<code>scan_captured_scalar_with_xs</code> \u2705<code>scan_captured_vector_with_xs_f64</code> \u2705 0.5.1 lax.scatter ScatterND <code>scatter_set_axis0</code> \u2705<code>scatter_set_axis0_f64</code> \u2705<code>scatter_set_middle</code> \u2705<code>scatter_set_middle_f64</code> \u2705<code>scatter_set_single</code> \u2705<code>scatter_set_single_f64</code> \u2705<code>scatter_set_vector</code> \u2705<code>scatter_set_vector_f64</code> \u2705<code>scatter_correct_axis_determination</code> \u2705<code>scatter_correct_axis_determination_f64</code> \u2705<code>scatter_updates_slice_needed_axis0</code> \u2705<code>scatter_updates_slice_needed_axis0_f64</code> \u2705<code>scatter_from_user_warning_shapes_valid_jax</code> \u2705<code>scatter_from_user_warning_shapes_valid_jax_f64</code> \u2705<code>scatter_user_error_scenario_precise</code> \u2705<code>scatter_user_error_scenario_precise_f64</code> \u2705<code>scatter_window_update_f64</code> \u2705<code>scatter_window_update_depth3_shapes_ok</code> \u2705<code>scatter_static_slice_set_f64</code> \u2705<code>scatter_depth2_fp64_type_mismatch</code> \u2705<code>scatter_clip_2d_window_at_edge</code> \u2705<code>scatter_simple_2d_window_out_of_bounds</code> \u2705<code>scatter_depth2_mixed_dtypes_fp_mismatch_f64</code> \u2705<code>scatter_depth2_mixed_dtypes_fp_mismatch</code> \u2705 0.4.4 lax.scatter_add ScatterND(reduction='add') <code>scatter_add_vector</code> \u2705<code>scatter_add_vector_f64</code> \u2705<code>scatter_add_scalar</code> \u2705<code>scatter_add_scalar_f64</code> \u2705<code>scatter_add_simple_1d</code> \u2705<code>scatter_add_simple_1d_f64</code> \u2705<code>scatter_add_batch_updates_1d_operand</code> \u2705<code>scatter_add_batch_updates_1d_operand_f64</code> \u2705<code>scatter_add_window_2d_operand_1d_indices</code> \u2705<code>scatter_add_window_2d_operand_1d_indices_f64</code> \u2705<code>scatter_add_mismatched_window_dims_from_user_report</code> \u2705<code>scatter_add_mismatched_window_dims_from_user_report2</code> \u2705<code>scatter_add_mismatched_window_dims_from_user_report3</code> \u2705<code>scatter_add_fluids_pattern_updates_5_4_1_1</code> \u2705<code>scatter_add_in_cond_float64</code> \u2705<code>scatter_add_fp64_dtype_mismatch</code> \u2705<code>scatter_add_depth2_depth2_helper_regression</code> \u2705<code>scatter_depth2_fp64_type_mismatch</code> \u2705 0.5.3 lax.scatter_max ScatterND(reduction='max') <code>scatter_max_simple_1d</code> \u2705<code>scatter_max_simple_1d_f64</code> \u2705<code>scatter_max_batch_updates_1d_operand</code> \u2705<code>scatter_max_batch_updates_1d_operand_f64</code> \u2705<code>scatter_max_window_2d_operand_1d_indices</code> \u2705<code>scatter_max_window_2d_operand_1d_indices_f64</code> \u2705<code>scatter_max_fp64_dtype_path_check</code> \u2705<code>scatter_max_depth2_helper_regression_fp64</code> \u2705 0.7.5 lax.scatter_min ScatterND(reduction='min') <code>scatter_min_simple_1d</code> \u2705<code>scatter_min_simple_1d_f64</code> \u2705<code>scatter_min_batch_updates_1d_operand</code> \u2705<code>scatter_min_batch_updates_1d_operand_f64</code> \u2705<code>scatter_min_window_2d_operand_1d_indices</code> \u2705<code>scatter_min_window_2d_operand_1d_indices_f64</code> \u2705<code>scatter_min_fp64_dtype_path_check</code> \u2705<code>scatter_min_depth2_helper_regression_fp64</code> \u2705 0.7.5 lax.scatter_mul ScatterND(reduction='mul') <code>scatter_mul_simple_1d</code> \u2705<code>scatter_mul_simple_1d_f64</code> \u2705<code>scatter_mul_batch_updates_1d_operand</code> \u2705<code>scatter_mul_batch_updates_1d_operand_f64</code> \u2705<code>scatter_mul_window_2d_operand_1d_indices</code> \u2705<code>scatter_mul_window_2d_operand_1d_indices_f64</code> \u2705<code>scatter_mul_mismatched_window_dims_from_user_report</code> \u2705<code>scatter_mul_mismatched_window_dims_from_user_report2</code> \u2705<code>scatter_mul_mismatched_window_dims_from_user_report3</code> \u2705<code>scatter_mul_fluids_pattern_updates_5_4_1_1</code> \u2705<code>scatter_mul_in_cond_float64</code> \u2705 0.6.4 lax.select Where <code>select_simple</code> \u2705<code>select_simple_f64</code> \u2705<code>select_basic</code> \u2705<code>select_basic_f64</code> \u2705<code>select_mask_scores_tensor_else_dynamic</code> \u2705<code>select_mask_scores_tensor_else_dynamic_f64</code> \u2705<code>select_mask_scores_tensor_else_dynamic_f64</code> \u2705<code>select_mask_scores_tensor_else</code> \u2705<code>select_mask_scores_tensor_else_f64</code> \u2705<code>select_mask_scores_tensor_else_f64</code> \u2705 0.7.1 lax.select_n EqualWhere <code>select_n_bool_predicate_two_cases_float</code> \u2705<code>select_n_bool_predicate_two_cases_float_f64</code> \u2705<code>select_n_bool_predicate_two_cases_int</code> \u2705<code>select_n_bool_predicate_two_cases_int_f64</code> \u2705<code>select_n_bool_predicate_scalar_broadcast</code> \u2705<code>select_n_bool_predicate_scalar_broadcast_f64</code> \u2705<code>select_n_int_indices_three_cases</code> \u2705<code>select_n_int_indices_three_cases_f64</code> \u2705<code>select_n_int_indices_four_cases</code> \u2705<code>select_n_int_indices_four_cases_f64</code> \u2705 0.2.0 lax.shard_map \u2796 <code>shard_map_inline_add</code> \u2705<code>shard_map_inline_add_f64</code> \u2705 0.10.2 lax.shift_right_logical BitShift <code>shift_right_logical_vec</code> \u2705<code>shift_right_logical_vec_f64</code> \u2705<code>shift_right_logical_scalar</code> \u2705<code>shift_right_logical_scalar_f64</code> \u2705 0.7.2 lax.sign Sign <code>sign</code> \u2705<code>sign_f64</code> \u2705 0.5.0 lax.sin Sin <code>sin</code> \u2705<code>sin_f64</code> \u2705 0.4.4 lax.sinh Sinh <code>sinh</code> \u2705<code>sinh_f64</code> \u2705 0.4.4 lax.slice Slice <code>slice_test1</code> \u2705<code>slice_test1_f64</code> \u2705<code>slice_3d_none_strides</code> \u2705<code>slice_3d_none_strides_f64</code> \u2705<code>slice_scan_axis_drop</code> \u2705<code>slice_scan_axis_drop_f64</code> \u2705 0.1.0 lax.sort TopK <code>sort_1d</code> \u2705<code>sort_1d_f64</code> \u2705<code>sort_2d</code> \u2705<code>sort_2d_f64</code> \u2705 0.2.0 lax.split Split <code>lax_split_equal_parts</code> \u2705<code>lax_split_equal_parts_f64</code> \u2705<code>lax_split_unequal_parts</code> \u2705<code>lax_split_unequal_parts_f64</code> \u2705 0.7.2 lax.sqrt Sqrt <code>sqrt</code> \u2705<code>sqrt_f64</code> \u2705 0.2.0 lax.square Mul <code>square</code> \u2705<code>square_f64</code> \u2705 0.2.0 lax.squeeze Squeeze <code>squeeze_single_axis</code> \u2705<code>squeeze_single_axis_f64</code> \u2705<code>squeeze_all_unit_dims_default</code> \u2705<code>squeeze_all_unit_dims_default_f64</code> \u2705<code>lax_squeeze_specific_axis_0</code> \u2705<code>lax_squeeze_specific_axis_0_f64</code> \u2705<code>lax_squeeze_multiple_axes</code> \u2705<code>lax_squeeze_multiple_axes_f64</code> \u2705<code>lax_squeeze_no_op_empty_dims</code> \u2705<code>lax_squeeze_no_op_empty_dims_f64</code> \u2705<code>lax_squeeze_problem_case_input_squeeze_only_axis_0</code> \u2705<code>lax_squeeze_problem_case_input_squeeze_only_axis_0_f64</code> \u2705<code>lax_squeeze_problem_case_input_squeeze_axes_0_2</code> \u2705<code>lax_squeeze_problem_case_input_squeeze_axes_0_2_f64</code> \u2705<code>lax_squeeze_problem_case_input_squeeze_all_dims_explicitly</code> \u2705<code>lax_squeeze_problem_case_input_squeeze_all_dims_explicitly_f64</code> \u2705 0.2.0 lax.stop_gradient Identity <code>stop_gradient</code> \u2705<code>stop_gradient_f64</code> \u2705<code>stop_gradient_basic</code> \u2705<code>stop_gradient_basic_f64</code> \u2705 0.2.0 lax.sub Sub <code>sub_test1</code> \u2705<code>sub_test1_f64</code> \u2705<code>sub_test2</code> \u2705<code>sub_test2_f64</code> \u2705<code>sub_const</code> \u2705<code>sub_const_f64</code> \u2705<code>sub_complex64</code> \u2705<code>sub_complex64_f64</code> \u2705 0.1.0 lax.tanh Tanh <code>tanh</code> \u2705<code>tanh_f64</code> \u2705 0.2.0 lax.top_k TopK <code>top_k_last_axis</code> \u2705<code>top_k_last_axis_f64</code> \u2705<code>top_k_matrix</code> \u2705<code>top_k_matrix_f64</code> \u2705 0.10.2 lax.transpose Transpose <code>transpose_basic</code> \u2705<code>transpose_basic_f64</code> \u2705<code>transpose_square_matrix</code> \u2705<code>transpose_square_matrix_f64</code> \u2705<code>transpose_3d</code> \u2705<code>transpose_3d_f64</code> \u2705<code>transpose_4d</code> \u2705<code>transpose_4d_f64</code> \u2705<code>transpose_reverse</code> \u2705<code>transpose_reverse_f64</code> \u2705<code>transpose_no_axes</code> \u2705<code>transpose_no_axes_f64</code> \u2705<code>transpose_nhwc_to_nchw</code> \u2705<code>transpose_nhwc_to_nchw_f64</code> \u2705 0.2.0 lax.while_loop Loop <code>while_scalar_counter</code> \u2705<code>while_scalar_counter_f64</code> \u2705<code>while_tuple_state</code> \u2705<code>while_tuple_state_f64</code> \u2705<code>while_loop_counter</code> \u2705<code>while_loop_counter_f64</code> \u2705<code>while_loop_vector</code> \u2705<code>while_loop_vector_f64</code> \u2705<code>while_loop_f64</code> \u2705<code>while_loop_multi_state_f32</code> \u2705<code>while_loop_multi_state_f64</code> \u2705<code>while_loop_with_closure</code> \u2705<code>while_loop_with_closure_f64</code> \u2705<code>while_loop_basic</code> \u2705<code>while_loop_two_state</code> \u2705<code>while_loop_captured_tracer</code> \u2705<code>while_loop_with_scalar_state</code> \u2705<code>while_loop_renamed_passthrough</code> \u2705<code>while_loop_closure_topo</code> \u2705<code>while_loop_mixed_rank</code> \u2705<code>while_loop_tracer_passthrough</code> \u2705<code>while_loop_no_loop_output_reused_as_input</code> \u2705<code>while_loop_4d_and_scalar_state</code> \u2705<code>while_loop_4d_and_scalar_state_f64</code> \u2705<code>while_loop_cnn_scalar_state_bug</code> \u2705<code>while_loop_cnn_scalar_state_bug_f64</code> \u2705<code>while_loop_nnx_repro</code> \u2705<code>while_loop_nnx_repro_f64</code> \u2705 0.5.1 linen.activation \u2796 <code>activation_glu_basic</code> \u2705<code>activation_glu_basic_f64</code> \u2705<code>activation_hard_sigmoid_basic</code> \u2705<code>activation_hard_sigmoid_basic_f64</code> \u2705<code>activation_hard_silu_basic</code> \u2705<code>activation_hard_silu_basic_f64</code> \u2705<code>activation_hard_swish_basic</code> \u2705<code>activation_hard_swish_basic_f64</code> \u2705<code>activation_hard_tanh_basic</code> \u2705<code>activation_hard_tanh_basic_f64</code> \u2705<code>activation_log_sigmoid_basic</code> \u2705<code>activation_log_sigmoid_basic_f64</code> \u2705<code>activation_log_softmax_basic</code> \u2705<code>activation_log_softmax_basic_f64</code> \u2705<code>activation_relu6_basic</code> \u2705<code>activation_relu6_basic_f64</code> \u2705<code>activation_silu_basic</code> \u2705<code>activation_silu_basic_f64</code> \u2705<code>activation_swish_basic</code> \u2705<code>activation_swish_basic_f64</code> \u2705<code>activation_tanh_basic</code> \u2705<code>activation_tanh_basic_f64</code> \u2705<code>activation_normalize_basic</code> \u2705<code>activation_normalize_basic_f64</code> \u2705<code>activation_one_hot_basic</code> \u2705<code>activation_one_hot_basic_f64</code> \u2705 0.11.0 linen.avg_pool AveragePoolTranspose <code>avg_pool_dynamic</code> \u2705<code>avg_pool</code> \u2705<code>avg_pool_same_padding_dynamic</code> \u2705<code>avg_pool_same_padding</code> \u2705<code>avg_pool_default_padding_dynamic</code> \u2705<code>avg_pool_default_padding</code> \u2705<code>avg_pool_stride1_dynamic</code> \u2705<code>avg_pool_stride1</code> \u2705<code>avg_pool_win3x3_stride2_dynamic</code> \u2705<code>avg_pool_win3x3_stride2</code> \u2705<code>avg_pool_stride_none_dynamic</code> \u2705<code>avg_pool_stride_none</code> \u2705<code>avg_pool_count_include_pad_false_dynamic</code> \u2705<code>avg_pool_count_include_pad_false</code> \u2705 0.11.0 linen.batch_norm BatchNormalization <code>batch_norm_no_bias_no_scale_dynamic</code> \u2705<code>batch_norm_no_bias_no_scale</code> \u2705<code>batch_norm_bias_no_scale_dynamic</code> \u2705<code>batch_norm_bias_no_scale</code> \u2705<code>batch_norm_no_bias_scale_dynamic</code> \u2705<code>batch_norm_no_bias_scale</code> \u2705<code>batch_norm_bias_scale_dynamic</code> \u2705<code>batch_norm_bias_scale</code> \u2705<code>batch_norm_3d_dynamic</code> \u2705<code>batch_norm_3d</code> \u2705<code>batch_norm_4d_dynamic</code> \u2705<code>batch_norm_4d</code> \u2705<code>batch_norm_4d_no_bias_no_scale_dynamic</code> \u2705<code>batch_norm_4d_no_bias_no_scale</code> \u2705 0.11.0 linen.bidirectional ConcatLoop <code>bidirectional_basic_dynamic</code> \u2705<code>bidirectional_basic_dynamic_f64</code> \u2705<code>bidirectional_basic</code> \u2705<code>bidirectional_basic_f64</code> \u2705 0.11.0 linen.conv CastLikeConvReshapeTranspose <code>conv_basic_dynamic</code> \u2705<code>conv_basic</code> \u2705<code>conv_no_bias</code> \u2705<code>conv_stride</code> \u2705 0.11.0 linen.conv_local ConvGemmReshapeTranspose <code>conv_local_valid</code> \u2705<code>conv_local_same</code> \u2705 0.11.0 linen.conv_lstm_cell AddConvGemmMulSigmoidTanh <code>conv_lstm_cell_basic_dynamic</code> \u2705<code>conv_lstm_cell_basic</code> \u2705 0.11.0 linen.conv_transpose ConvTranspose <code>conv_transpose_basic_dynamic</code> \u2705<code>conv_transpose_basic</code> \u2705<code>conv_transpose_valid_stride</code> \u2705 0.11.0 linen.dense Gemm <code>dense_basic_dynamic</code> \u2705<code>dense_basic_dynamic_f64</code> \u2705<code>dense_basic</code> \u2705<code>dense_basic_f64</code> \u2705<code>dense_high_rank_dynamic_dynamic</code> \u2705<code>dense_high_rank_dynamic_dynamic_f64</code> \u2705<code>dense_high_rank_static</code> \u2705<code>dense_high_rank_static_f64</code> \u2705<code>dense_high_rank_no_bias</code> \u2705<code>dense_high_rank_no_bias_f64</code> \u2705<code>dense_no_bias_dynamic</code> \u2705<code>dense_no_bias_dynamic_f64</code> \u2705<code>dense_no_bias</code> \u2705<code>dense_no_bias_f64</code> \u2705 0.11.0 linen.dense_general CastLikeConcatGemmReshapeShapeSlice <code>dense_general_basic_dynamic</code> \u2705<code>dense_general_basic_dynamic_f64</code> \u2705<code>dense_general_basic</code> \u2705<code>dense_general_basic_f64</code> \u2705<code>dense_general_multi_out</code> \u2705<code>dense_general_multi_out_f64</code> \u2705<code>dense_general_contract_last_two</code> \u2705<code>dense_general_contract_last_two_f64</code> \u2705<code>dense_general_dynamic_batch_dynamic</code> \u2705<code>dense_general_dynamic_batch_dynamic_f64</code> \u2705<code>dense_general_no_bias</code> \u2705<code>dense_general_no_bias_f64</code> \u2705 0.11.0 linen.dot_product_attention MatMulMulSoftmaxTransposeWhere <code>dot_product_attention_basic</code> \u2705<code>dot_product_attention_basic_f64</code> \u2705 0.11.0 linen.dot_product_attention_weights AddDivMatMulMulReduceSumSoftmaxWhere <code>dot_product_attention_weights_basic</code> \u2705<code>dot_product_attention_weights_basic_f64</code> \u2705 0.11.0 linen.dropout Dropout <code>dropout_init_params_dynamic</code> \u2705<code>dropout_init_params_dynamic_f64</code> \u2705<code>dropout_init_params</code> \u2705<code>dropout_init_params_f64</code> \u2705<code>dropout_call_params_dynamic</code> \u2705<code>dropout_call_params_dynamic_f64</code> \u2705<code>dropout_call_params</code> \u2705<code>dropout_call_params_f64</code> \u2705 0.11.0 linen.einsum AddEinsumReshape <code>einsum_with_bias</code> \u2705<code>einsum_with_bias_f64</code> \u2705<code>einsum_no_bias</code> \u2705<code>einsum_no_bias_f64</code> \u2705 0.11.0 linen.embed Gather <code>token_embedding_dynamic</code> \u2705<code>token_embedding_dynamic_f64</code> \u2705<code>token_embedding</code> \u2705<code>token_embedding_f64</code> \u2705<code>positional_embedding_dynamic</code> \u2705<code>positional_embedding_dynamic_f64</code> \u2705<code>positional_embedding</code> \u2705<code>positional_embedding_f64</code> \u2705 0.11.0 linen.group_norm GroupNormalization <code>group_norm_rank4</code> \u2705<code>group_norm_rank2_dynamic</code> \u2705<code>group_norm_rank2</code> \u2705<code>group_norm_no_bias_no_scale_dynamic</code> \u2705<code>group_norm_no_bias_no_scale</code> \u2705 0.11.0 linen.gru_cell AddGemmMulSigmoidTanh <code>gru_cell_basic_dynamic</code> \u2705<code>gru_cell_basic_dynamic_f64</code> \u2705<code>gru_cell_basic</code> \u2705<code>gru_cell_basic_f64</code> \u2705 0.11.0 linen.instance_norm GroupNormalization <code>instance_norm_rank4_dynamic</code> \u2705<code>instance_norm_rank4</code> \u2705<code>instance_norm_rank2_dynamic</code> \u2705<code>instance_norm_rank2</code> \u2705 0.11.0 linen.layer_norm LayerNormalization <code>layer_norm_dynamic</code> \u2705<code>layer_norm</code> \u2705<code>layer_norm_no_bias_no_scale_dynamic</code> \u2705<code>layer_norm_no_bias_no_scale</code> \u2705<code>layer_norm_multiaxis_dynamic</code> \u2705<code>layer_norm_multiaxis</code> \u2705<code>layer_norm_default_epsilon_dynamic</code> \u2705<code>layer_norm_default_epsilon</code> \u2705 0.11.0 linen.lstm_cell AddGemmMulSigmoidTanh <code>lstm_cell_basic_dynamic</code> \u2705<code>lstm_cell_basic_dynamic_f64</code> \u2705<code>lstm_cell_basic</code> \u2705<code>lstm_cell_basic_f64</code> \u2705 0.11.0 linen.make_attention_mask CastMul <code>make_attention_mask_basic</code> \u2705<code>make_attention_mask_basic_f64</code> \u2705 0.11.0 linen.make_causal_mask CastLess <code>make_causal_mask_basic</code> \u2705<code>make_causal_mask_basic_f64</code> \u2705 0.11.0 linen.max_pool MaxPool <code>max_pool</code> \u2705<code>max_pool_same_padding</code> \u2705<code>max_pool_basic</code> \u2705<code>max_pool_same_dynamic</code> \u2705<code>max_pool_same</code> \u2705 0.11.0 linen.mgu_cell AddGemmMulSigmoidTanh <code>mgu_cell_basic_dynamic</code> \u2705<code>mgu_cell_basic_dynamic_f64</code> \u2705<code>mgu_cell_basic</code> \u2705<code>mgu_cell_basic_f64</code> \u2705 0.11.0 linen.min_pool MaxPoolNeg <code>min_pool</code> \u2705<code>min_pool_same_padding</code> \u2705<code>min_pool_basic</code> \u2705<code>min_pool_same_dynamic</code> \u2705<code>min_pool_same</code> \u2705 0.11.0 linen.multi_head_attention AddGemmMatMulMulReshapeSoftmaxTranspose <code>multi_head_attention_basic_dynamic</code> \u2705<code>multi_head_attention_basic_dynamic_f64</code> \u2705<code>multi_head_attention_basic</code> \u2705<code>multi_head_attention_basic_f64</code> \u2705<code>multi_head_attention_no_bias_dynamic</code> \u2705<code>multi_head_attention_no_bias_dynamic_f64</code> \u2705<code>multi_head_attention_no_bias</code> \u2705<code>multi_head_attention_no_bias_f64</code> \u2705 0.11.0 linen.multi_head_dot_product_attention AddGemmMatMulMulReshapeSoftmaxTranspose <code>multi_head_dot_product_attention_basic_dynamic</code> \u2705<code>multi_head_dot_product_attention_basic_dynamic_f64</code> \u2705<code>multi_head_dot_product_attention_basic</code> \u2705<code>multi_head_dot_product_attention_basic_f64</code> \u2705<code>multi_head_dot_product_attention_no_bias_dynamic</code> \u2705<code>multi_head_dot_product_attention_no_bias_dynamic_f64</code> \u2705<code>multi_head_dot_product_attention_no_bias</code> \u2705<code>multi_head_dot_product_attention_no_bias_f64</code> \u2705 0.11.0 linen.optimized_lstm_cell AddGemmMulSigmoidTanh <code>optimized_lstm_cell_basic_dynamic</code> \u2705<code>optimized_lstm_cell_basic_dynamic_f64</code> \u2705<code>optimized_lstm_cell_basic</code> \u2705<code>optimized_lstm_cell_basic_f64</code> \u2705 0.11.0 linen.pool AveragePoolMaxPoolMulNegTranspose <code>pool_max_basic_dynamic</code> \u2705<code>pool_max_basic</code> \u2705<code>pool_min_basic_dynamic</code> \u2705<code>pool_min_basic</code> \u2705<code>pool_sum_basic_dynamic</code> \u2705<code>pool_sum_basic</code> \u2705 0.11.0 linen.rms_norm RMSNormalization <code>rms_norm_basic</code> \u2705<code>rms_norm_use_scale_false</code> \u2705<code>rms_norm_4d_dynamic_dynamic</code> \u2705<code>rms_norm_4d_dynamic</code> \u2705 0.11.0 linen.rnn Loop <code>rnn_basic_dynamic</code> \u2705<code>rnn_basic_dynamic_f64</code> \u2705<code>rnn_basic</code> \u2705<code>rnn_basic_f64</code> \u2705 0.11.0 linen.self_attention AddGemmMatMulMulReshapeSoftmaxTranspose <code>self_attention_basic_dynamic</code> \u2705<code>self_attention_basic_dynamic_f64</code> \u2705<code>self_attention_basic</code> \u2705<code>self_attention_basic_f64</code> \u2705<code>self_attention_no_bias_dynamic</code> \u2705<code>self_attention_no_bias_dynamic_f64</code> \u2705<code>self_attention_no_bias</code> \u2705<code>self_attention_no_bias_f64</code> \u2705 0.11.0 linen.simple_cell AddGemmMulSigmoidTanh <code>simple_cell_basic_dynamic</code> \u2705<code>simple_cell_basic_dynamic_f64</code> \u2705<code>simple_cell_basic</code> \u2705<code>simple_cell_basic_f64</code> \u2705 0.11.0 linen.spectral_norm DivMatMul <code>spectral_norm_dense_dynamic</code> \u2705<code>spectral_norm_dense</code> \u2705 0.11.0 linen.weight_norm DivMul <code>weight_norm_dense_dynamic</code> \u2705<code>weight_norm_dense</code> \u2705 0.11.0 nn.celu Celu <code>jaxnn_celu</code> \u2705<code>jaxnn_celu_1</code> \u2705<code>jaxnn_celu_alpha_default</code> \u2705<code>jaxnn_celu_alpha_custom_dynamic</code> \u2705<code>jaxnn_celu_alpha_custom</code> \u2705 0.7.1 nn.dot_product_attention AddMatMulMulSoftmaxTransposeWhere <code>dpa_basic</code> \u2705<code>dpa_basic_f64</code> \u2705<code>dpa_positional_bias_mask</code> \u2705<code>dpa_positional_bias_mask_f64</code> \u2705<code>dpa_diff_heads_embed</code> \u2705<code>dpa_diff_heads_embed_f64</code> \u2705<code>dpa_batch4_seq16</code> \u2705<code>dpa_batch4_seq16_f64</code> \u2705<code>dpa_float64</code> \u2705<code>dpa_heads1_embed4</code> \u2705<code>dpa_heads1_embed4_f64</code> \u2705<code>dpa_heads8_embed8</code> \u2705<code>dpa_heads8_embed8_f64</code> \u2705<code>dpa_batch1_seq2</code> \u2705<code>dpa_batch1_seq2_f64</code> \u2705<code>dpa_batch8_seq4</code> \u2705<code>dpa_batch8_seq4_f64</code> \u2705<code>dpa_axis1</code> \u2705<code>dpa_axis1_f64</code> \u2705<code>dpa_with_tensor_mask</code> \u2705<code>dpa_with_tensor_mask_f64</code> \u2705<code>dpa_tiny_mask_all_valid</code> \u2705<code>dpa_tiny_mask_all_valid_f64</code> \u2705<code>dpa_tiny_mask_mixed</code> \u2705<code>dpa_tiny_mask_mixed_f64</code> \u2705<code>dpa_one_false</code> \u2705<code>dpa_one_false_f64</code> \u2705<code>dpa_mostly_false</code> \u2705<code>dpa_mostly_false_f64</code> \u2705<code>dpa_with_causal_mask</code> \u2705<code>dpa_with_causal_mask_f64</code> \u2705<code>dpa_with_padding_mask</code> \u2705<code>dpa_with_padding_mask_f64</code> \u2705<code>dpa_with_local_window_mask</code> \u2705<code>dpa_with_local_window_mask_f64</code> \u2705<code>dpa_mask_none</code> \u2705 0.8.0 nn.elu Elu <code>jaxnn_elu</code> \u2705<code>jaxnn_elu_1</code> \u2705<code>jaxnn_elu_default</code> \u2705<code>jaxnn_elu_custom_alpha_dynamic</code> \u2705<code>jaxnn_elu_custom_alpha</code> \u2705 0.7.1 nn.gelu Gelu <code>jaxnn_gelu</code> \u2705<code>jaxnn_gelu_1</code> \u2705<code>jaxnn_gelu_approx</code> \u2705<code>jaxnn_gelu_exact</code> \u2705<code>jaxnn_gelu_tanh_dynamic</code> \u2705<code>jaxnn_gelu_tanh</code> \u2705 0.7.1 nn.identity Identity <code>jaxnn_identity</code> \u2705<code>jaxnn_identity_f64</code> \u2705<code>jaxnn_identity_1</code> \u2705<code>jaxnn_identity_1_f64</code> \u2705<code>jaxnn_identity_basic</code> \u2705<code>jaxnn_identity_basic_f64</code> \u2705<code>jaxnn_identity_dynamic_dynamic</code> \u2705<code>jaxnn_identity_dynamic_dynamic_f64</code> \u2705<code>jaxnn_identity_dynamic</code> \u2705<code>jaxnn_identity_dynamic_f64</code> \u2705 0.7.1 nn.leaky_relu LeakyRelu <code>jaxnn_leaky_relu</code> \u2705<code>jaxnn_leaky_relu_1</code> \u2705<code>jaxnn_leaky_relu_default_dynamic</code> \u2705<code>jaxnn_leaky_relu_default</code> \u2705<code>jaxnn_leaky_relu_custom</code> \u2705 0.7.1 nn.mish Mish <code>jaxnn_mish</code> \u2705<code>jaxnn_mish_1</code> \u2705<code>jaxnn_mish_basic</code> \u2705 0.7.1 nn.relu Relu <code>jaxnn_relu</code> \u2705<code>jaxnn_relu_f64</code> \u2705<code>jaxnn_relu_1</code> \u2705<code>jaxnn_relu_1_f64</code> \u2705<code>jaxnn_relu_basic</code> \u2705<code>jaxnn_relu_basic_f64</code> \u2705<code>jaxnn_relu_dynamic_dynamic</code> \u2705<code>jaxnn_relu_dynamic_dynamic_f64</code> \u2705<code>jaxnn_relu_dynamic</code> \u2705<code>jaxnn_relu_dynamic_f64</code> \u2705 0.7.1 nn.selu Selu <code>jaxnn_selu</code> \u2705<code>jaxnn_selu_1</code> \u2705<code>jaxnn_selu_basic_dynamic</code> \u2705<code>jaxnn_selu_basic</code> \u2705 0.7.1 nn.sigmoid Sigmoid <code>jaxnn_sigmoid</code> \u2705<code>jaxnn_sigmoid_f64</code> \u2705<code>jaxnn_sigmoid_1</code> \u2705<code>jaxnn_sigmoid_1_f64</code> \u2705 0.7.1 nn.soft_sign Softsign <code>jaxnn_soft_sign</code> \u2705<code>jaxnn_soft_sign_1</code> \u2705<code>jaxnn_softsign_basic</code> \u2705 0.7.1 nn.softmax Softmax <code>softmax</code> \u2705<code>softmax_f64</code> \u2705<code>softmax_2d</code> \u2705<code>softmax_2d_f64</code> \u2705<code>softmax_3d</code> \u2705<code>softmax_3d_f64</code> \u2705<code>softmax_mask_where</code> \u2705<code>softmax_mask_where_f64</code> \u2705 0.7.1 nn.softplus Softplus <code>jaxnn_softplus</code> \u2705<code>jaxnn_softplus_1</code> \u2705<code>jaxnn_softplus_basic</code> \u2705 0.7.1 nn.truncated_normal \u2796 <code>initializer</code> \u2705<code>random_truncated_normal_positional</code> \u2705<code>flax_dense_like_init</code> \u2705 0.7.1 nnx.avg_pool AveragePoolTranspose <code>avg_pool_dynamic</code> \u2705<code>avg_pool</code> \u2705<code>avg_pool_same_padding_dynamic</code> \u2705<code>avg_pool_same_padding</code> \u2705<code>avg_pool_default_padding_dynamic</code> \u2705<code>avg_pool_default_padding</code> \u2705<code>avg_pool_stride1_dynamic</code> \u2705<code>avg_pool_stride1</code> \u2705<code>avg_pool_win3x3_stride2_dynamic</code> \u2705<code>avg_pool_win3x3_stride2</code> \u2705<code>avg_pool_stride_none_dynamic</code> \u2705<code>avg_pool_stride_none</code> \u2705<code>avg_pool_count_include_pad_false_dynamic</code> \u2705<code>avg_pool_count_include_pad_false</code> \u2705 0.1.0 nnx.batch_norm BatchNormalization <code>batch_norm_no_bias_no_scale_dynamic</code> \u2705<code>batch_norm_no_bias_no_scale</code> \u2705<code>batch_norm_bias_no_scale_dynamic</code> \u2705<code>batch_norm_bias_no_scale</code> \u2705<code>batch_norm_no_bias_scale_dynamic</code> \u2705<code>batch_norm_no_bias_scale</code> \u2705<code>batch_norm_bias_scale_dynamic</code> \u2705<code>batch_norm_bias_scale</code> \u2705<code>batch_norm_3d_dynamic</code> \u2705<code>batch_norm_3d</code> \u2705<code>batch_norm_4d_dynamic</code> \u2705<code>batch_norm_4d</code> \u2705<code>batch_norm_4d_no_bias_no_scale_dynamic</code> \u2705<code>batch_norm_4d_no_bias_no_scale</code> \u2705 0.1.0 nnx.conv CastLikeConvReshapeTranspose <code>conv_basic_bias_dynamic</code> \u2705<code>conv_basic_bias</code> \u2705<code>conv_basic_bias_2</code> \u2705<code>conv_basic_bias_3</code> \u2705<code>conv_stride2_bias</code> \u2705<code>conv_no_bias_dynamic</code> \u2705<code>conv_no_bias</code> \u2705<code>conv_valid_padding</code> \u2705<code>conv_stride1</code> \u2705<code>conv_stride2</code> \u2705<code>conv_different_kernel</code> \u2705<code>conv_float64</code> \u2705<code>conv_single_batch</code> \u2705<code>conv_large_batch</code> \u2705<code>conv_1d</code> \u2705<code>conv_1d_more_1d_inputs</code> \u2705<code>conv_1d_more_2d_inputs</code> \u2705<code>conv_1d_large_kernel</code> \u2705<code>conv_1d_dilation</code> \u2705<code>conv_1d_stride_dilation</code> \u2705<code>conv_2d_asymmetric_kernel</code> \u2705<code>conv_2d_asymmetric_stride</code> \u2705<code>conv_2d_asymmetric_dilation</code> \u2705<code>conv_2d_large_dilation</code> \u2705<code>conv_2d_large_stride</code> \u2705<code>conv_2d_mixed_params</code> \u2705<code>conv_2d_same_padding_mixed_dilation</code> \u2705<code>conv_3d_basic</code> \u2705<code>conv_3d_stride</code> \u2705<code>conv_3d_asymmetric</code> \u2705<code>conv_3d_dilation</code> \u2705<code>conv_2d_small_input</code> \u2705<code>conv_2d_many_channels</code> \u2705<code>conv_1d_wide_input</code> \u2705<code>conv_2d_kernel_1x1</code> \u2705<code>conv_1d_kernel_1</code> \u2705<code>conv_2d_group_conv</code> \u2705<code>conv_1d_group_conv_more_dims</code> \u2705<code>conv_2d_depthwise</code> \u2705<code>conv_1d_complex_on_4d</code> \u2705<code>conv_2d_complex_on_5d</code> \u2705<code>conv_2d_asymmetric_on_5d</code> \u2705<code>conv_1d_high_dilation_on_3d</code> \u2705<code>conv_1d_large_kernel_on_4d</code> \u2705<code>conv_2d_group_stride_dilation</code> \u2705<code>conv_1d_group_on_higher_dim</code> \u2705<code>conv_1d_same_padding_on_3d</code> \u2705<code>conv_3d_group_complex</code> \u2705<code>conv_1d_unit_group_on_multi_dim</code> \u2705 0.1.0 nnx.dot_product_attention AddMatMulMulSoftmaxTransposeWhere <code>dpa_basic</code> \u2705<code>dpa_basic_f64</code> \u2705<code>dpa_with_tensor_mask</code> \u2705<code>dpa_with_bias</code> \u2705<code>dpa_with_causal_mask</code> \u2705<code>dpa_with_causal_mask_f64</code> \u2705<code>dpa_with_mask_and_bias</code> \u2705 0.1.0 nnx.dropout Dropout <code>dropout_init_params_dynamic</code> \u2705<code>dropout_init_params_dynamic_f64</code> \u2705<code>dropout_init_params</code> \u2705<code>dropout_init_params_f64</code> \u2705<code>dropout_call_params_dynamic</code> \u2705<code>dropout_call_params_dynamic_f64</code> \u2705<code>dropout_call_params</code> \u2705<code>dropout_call_params_f64</code> \u2705 0.1.0 nnx.einsum AddEinsum <code>einsum_module_with_bias</code> \u2705<code>einsum_module_with_bias_f64</code> \u2705<code>einsum_module_no_bias</code> \u2705<code>einsum_module_no_bias_f64</code> \u2705 0.4.2 nnx.elu Elu <code>elu</code> \u2705<code>elu_default_dynamic</code> \u2705<code>elu_default</code> \u2705<code>elu_alpha</code> \u2705 0.2.0 nnx.embed Gather <code>token_embedding_dynamic</code> \u2705<code>token_embedding_dynamic_f64</code> \u2705<code>token_embedding</code> \u2705<code>token_embedding_f64</code> \u2705<code>positional_embedding_dynamic</code> \u2705<code>positional_embedding_dynamic_f64</code> \u2705<code>positional_embedding</code> \u2705<code>positional_embedding_f64</code> \u2705 0.7.0 nnx.gelu Gelu <code>gelu</code> \u2705<code>gelu_1</code> \u2705<code>gelu_2</code> \u2705<code>gelu_2_f64</code> \u2705<code>gelu_3_dynamic</code> \u2705<code>gelu_3_dynamic_f64</code> \u2705<code>gelu_3</code> \u2705<code>gelu_3_f64</code> \u2705<code>gelu_4</code> \u2705<code>gelu_4_f64</code> \u2705<code>gelu_5_dynamic</code> \u2705<code>gelu_5_dynamic_f64</code> \u2705<code>gelu_5</code> \u2705<code>gelu_5_f64</code> \u2705 0.1.0 nnx.group_norm GroupNormalization <code>group_norm</code> \u2705<code>group_norm_rank2_dynamic</code> \u2705<code>group_norm_rank2</code> \u2705<code>group_norm_rank4</code> \u2705<code>group_norm_no_bias_dynamic</code> \u2705<code>group_norm_no_bias</code> \u2705<code>group_norm_no_bias_no_scale_dynamic</code> \u2705<code>group_norm_no_bias_no_scale</code> \u2705<code>group_norm_bias_no_scale_dynamic</code> \u2705<code>group_norm_bias_no_scale</code> \u2705<code>group_norm_no_scale_dynamic</code> \u2705<code>group_norm_no_scale</code> \u2705<code>group_norm_no_bias_scale_dynamic</code> \u2705<code>group_norm_no_bias_scale</code> \u2705<code>group_norm_bias_scale_dynamic</code> \u2705<code>group_norm_bias_scale</code> \u2705 0.2.0 nnx.layer_norm LayerNormalization <code>layer_norm_dynamic</code> \u2705<code>layer_norm</code> \u2705<code>layer_norm_no_bias_no_scale_dynamic</code> \u2705<code>layer_norm_no_bias_no_scale</code> \u2705<code>layer_norm_bias_no_scale_dynamic</code> \u2705<code>layer_norm_bias_no_scale</code> \u2705<code>layer_norm_no_bias_scale_dynamic</code> \u2705<code>layer_norm_no_bias_scale</code> \u2705<code>layer_norm_bias_scale_dynamic</code> \u2705<code>layer_norm_bias_scale</code> \u2705<code>layer_norm_multiaxis_dynamic</code> \u2705<code>layer_norm_multiaxis</code> \u2705<code>layer_norm_symbolic_batch_dynamic</code> \u2705<code>layer_norm_symbolic_batch</code> \u2705<code>layer_norm_symbolic_batch_seq10_feat3_dynamic</code> \u2705<code>layer_norm_symbolic_batch_seq10_feat3</code> \u2705<code>layer_norm_symbolic_batch_seq10_feat3_2_dynamic</code> \u2705<code>layer_norm_symbolic_batch_seq10_feat3_2</code> \u2705<code>layer_norm_negative_axis_no_div_dynamic</code> \u2705<code>layer_norm_negative_axis_no_div</code> \u2705 0.1.0 nnx.leaky_relu LeakyRelu <code>leaky_relu</code> \u2705<code>leaky_relu_default_dynamic</code> \u2705<code>leaky_relu_default</code> \u2705<code>leaky_relu_custom</code> \u2705 0.2.0 nnx.linear CastLikeConcatGemmReshapeShapeSlice <code>linear_symbolic_batch_dynamic</code> \u2705<code>linear_symbolic_batch_dynamic_f64</code> \u2705<code>linear_symbolic_batch</code> \u2705<code>linear_symbolic_batch_f64</code> \u2705<code>linear_high_rank_dynamic</code> \u2705<code>linear_high_rank_dynamic_f64</code> \u2705<code>linear_high_rank_static</code> \u2705<code>linear_high_rank_static_f64</code> \u2705<code>linear_no_bias_dynamic</code> \u2705<code>linear_no_bias_dynamic_f64</code> \u2705<code>linear_no_bias</code> \u2705<code>linear_no_bias_f64</code> \u2705<code>linear_high_rank_no_bias_dynamic</code> \u2705<code>linear_high_rank_no_bias_dynamic_f64</code> \u2705<code>linear_high_rank_no_bias</code> \u2705<code>linear_high_rank_no_bias_f64</code> \u2705<code>linear_merge_symbolic_dim_dynamic</code> \u2705 0.1.0 nnx.linear_general CastLikeConcatGemmReshapeShapeSlice <code>linear_general_merge_symbolic_dim_dynamic</code> \u2705<code>linear_general_dynamic</code> \u2705<code>linear_general</code> \u2705<code>linear_general_2</code> \u2705<code>linear_general_3</code> \u2705<code>linear_general_4</code> \u2705<code>linear_general_abstract_eval_axes</code> \u2705<code>linear_general_abstract_eval_axes_pair</code> \u2705<code>dynamic_batch_and_feature_dims_dynamic</code> \u2705 0.1.0 nnx.log_softmax LogSoftmax <code>log_softmax</code> \u2705<code>log_softmax_f64</code> \u2705<code>log_softmax_default_axis_dynamic</code> \u2705<code>log_softmax_default_axis_dynamic_f64</code> \u2705<code>log_softmax_default_axis</code> \u2705<code>log_softmax_default_axis_f64</code> \u2705<code>log_softmax_axis0</code> \u2705<code>log_softmax_axis0_f64</code> \u2705 0.2.0 nnx.max_pool MaxPool <code>max_pool</code> \u2705<code>max_pool_same_padding</code> \u2705<code>max_pool_basic</code> \u2705<code>max_pool_same_dynamic</code> \u2705<code>max_pool_same</code> \u2705 0.2.0 nnx.relu Relu <code>relu_1d</code> \u2705<code>relu_1d_f64</code> \u2705<code>relu_4d_dynamic</code> \u2705<code>relu_4d_dynamic_f64</code> \u2705<code>relu_4d</code> \u2705<code>relu_4d_f64</code> \u2705 0.2.0 nnx.rms_norm RMSNormalization <code>rms_norm_basic</code> \u2705<code>rms_norm_use_scale_false</code> \u2705<code>rms_norm_4d_dynamic_dynamic</code> \u2705<code>rms_norm_4d_dynamic</code> \u2705<code>rms_norm_4d_dynamic_no_scale_dynamic</code> \u2705<code>rms_norm_4d_dynamic_no_scale</code> \u2705 0.2.0 nnx.sigmoid Sigmoid <code>sigmoid_dynamic</code> \u2705<code>sigmoid_dynamic_f64</code> \u2705<code>sigmoid</code> \u2705<code>sigmoid_f64</code> \u2705 0.2.0 nnx.softmax Softmax <code>softmax_dynamic</code> \u2705<code>softmax_dynamic_f64</code> \u2705<code>softmax</code> \u2705<code>softmax_f64</code> \u2705 0.1.0 nnx.softplus Softplus <code>softplus</code> \u2705 0.1.0 nnx.tanh Tanh <code>tanh</code> \u2705<code>tanh_f64</code> \u2705 0.1.0 random.random_bits CastFloorRandomUniform <code>random_bits_uint32</code> \u2705<code>random_bits_uint32_f64</code> \u2705 0.7.2 random.random_fold_in Identity <code>random_fold_in_passthrough</code> \u2705<code>random_fold_in_passthrough_f64</code> \u2705 0.2.0 random.random_seed CastConcat <code>random_seed_basic</code> \u2705<code>random_seed_basic_f64</code> \u2705 0.2.0"},{"location":"user_guide/sota_examples/maxtext/","title":"MaxText Support \ud83d\ude80","text":"<p>MaxText is a high-performance, arbitrary-scale, open-source LLM framework written in pure Python/JAX. <code>jax2onnx</code> provides a self-contained example stack to export these models to ONNX.</p> <ul> <li>MaxText (DeepSeek, Gemma, GPT-3, Kimi, Llama, Mistral, Qwen) - https://github.com/AI-Hypercomputer/maxtext</li> </ul>"},{"location":"user_guide/sota_examples/maxtext/#related-examples","title":"Related Examples","text":"<p>All supported MaxText model families (<code>DeepSeek</code>, <code>Gemma</code>, <code>Llama</code>, <code>Mistral</code>, <code>Qwen</code>, etc.) are listed with their test status in the Examples table.</p>"},{"location":"user_guide/sota_examples/maxtext/#supported-families","title":"Supported Families","text":"<p>We support exporting the following model families from the MaxText model zoo:</p> <ul> <li>DeepSeek (v2 / v3)</li> <li>Gemma (2 / 3)</li> <li>GPT-3</li> <li>Kimi (K2)</li> <li>Llama (2 / 3 / 3.1 / 4)</li> <li>Mistral</li> <li>Qwen (3 / 3-Next / Omni)</li> </ul>"},{"location":"user_guide/sota_examples/maxtext/#usage","title":"Usage","text":""},{"location":"user_guide/sota_examples/maxtext/#dependencies","title":"Dependencies","text":"<p>To run the MaxText examples, you need to install the following additional dependencies:</p> <pre><code>poetry install --with maxtext\n</code></pre> <p>Note: This installs <code>omegaconf</code>, <code>transformers</code>, <code>sentencepiece</code>, <code>tensorflow-cpu</code>, and <code>tensorboardX</code>. <code>tensorflow-cpu</code> is required because MaxText uses <code>tensorboard</code> and some TF utilities.</p>"},{"location":"user_guide/sota_examples/maxtext/#environment-configuration","title":"Environment Configuration","text":"<ul> <li><code>JAX2ONNX_MAXTEXT_SRC</code> (Optional): Path to a local clone of the MaxText repository. If not set, the system attempts to resolve it from an installed <code>MaxText</code> package.</li> <li><code>JAX2ONNX_MAXTEXT_MODELS</code> (Optional): A comma-separated list of model config names to test (e.g., <code>llama2-7b.yml</code>). If unset, it defaults to a standard set of representative models.</li> </ul>"},{"location":"user_guide/sota_examples/maxtext/#testing","title":"Testing","text":"<p>To run all the latest MaxText examples (use <code>poetry run</code> to stay in the project venv):</p> <pre><code>cd tmp\ngit clone https://github.com/AI-Hypercomputer/maxtext.git\ncd ..\nexport JAX2ONNX_MAXTEXT_SRC=tmp/maxtext\nexport JAX2ONNX_MAXTEXT_MODELS=all  # or \"gemma-2b,llama2-7b\"\npoetry install --with maxtext\npoetry run python scripts/generate_tests.py\npoetry run pytest -q tests/examples/test_maxtext.py\n</code></pre> <p>ONNX outputs land in <code>docs/onnx/examples/maxtext</code>.</p> <p>This will: 1.  Dynamically discover MaxText configs. 2.  Instantiate the models with minimal inference settings (batch_size=1, seq_len=32). 3.  Export them to ONNX and verify the graph structure.</p>"},{"location":"user_guide/sota_examples/dinov3/compare_meta_vs_jax2onnx/","title":"Comparing Meta PyTorch vs jax2onnx ONNX","text":"<p>After exporting the IR-only DINOv3 model to ONNX, you can quantify how closely it matches Meta\u2019s original PyTorch checkpoint. The repository ships a helper script that performs the comparison end to end. Our ONNX is generated from Equimo\u2019s clean-room Equinox/JAX reimplementation of the architecture described in Meta AI\u2019s DINOv3 paper; using Meta\u2019s pretrained weights remains optional and is governed by the DINOv3 license.</p>"},{"location":"user_guide/sota_examples/dinov3/compare_meta_vs_jax2onnx/#1-requirements","title":"1. Requirements","text":"<p>Ensure the optional test dependencies are installed so PyTorch and torchvision are available:</p> <pre><code>poetry install --with test\ncurl -L -o /tmp/coco_39769.jpg \\\n  http://images.cocodataset.org/val2017/000000039769.jpg\n</code></pre> <p>You\u2019ll also need:</p> <ul> <li>Meta\u2019s <code>.pth</code> checkpoint (see <code>getting_weights.md</code>)</li> <li>The mapped Equinox checkpoint (<code>~/.cache/equimo/dinov3/eqx_dinov3_vits16_mapped.eqx</code>)</li> <li>The ONNX export produced from that checkpoint</li> </ul>"},{"location":"user_guide/sota_examples/dinov3/compare_meta_vs_jax2onnx/#2-basic-comparison-meta-onnx","title":"2. Basic comparison (Meta \u21d4 ONNX)","text":"<p>The CLI reports cosine similarity and max absolute error for the CLS token and the mean pooled patch embeddings:</p> <pre><code>poetry run python scripts/compare_meta_vs_jax2onnx.py \\\n  --image /tmp/coco_39769.jpg \\\n  --variant dinov3_vits16_pretrain_lvd1689m \\\n  --weights ~/.cache/torch/hub/dinov3/weights/dinov3_vits16_pretrain_lvd1689m-08c60483.pth \\\n  --onnx ~/.cache/equimo/dinov3/eqx_dinov3_vit_S16.onnx\n</code></pre> <p>If the export is faithful, cosine \u2248 1 and max\u2006|\u0394| should sit near machine precision (\u2248 5e\u20116).</p>"},{"location":"user_guide/sota_examples/dinov3/compare_meta_vs_jax2onnx/#3-triangulate-with-the-equinox-example-meta-eqx-onnx","title":"3. Triangulate with the Equinox example (Meta \u21d4 Eqx \u21d4 ONNX)","text":"<p>To isolate where mismatches arise, include the Equinox checkpoint and enable per-block debugging:</p> <pre><code>poetry run python scripts/compare_meta_vs_jax2onnx.py \\\n  --image /tmp/coco_39769.jpg \\\n  --variant dinov3_vits16_pretrain_lvd1689m \\\n  --weights ~/.cache/torch/hub/dinov3/weights/dinov3_vits16_pretrain_lvd1689m-08c60483.pth \\\n  --onnx ~/.cache/equimo/dinov3/eqx_dinov3_vit_S16.onnx \\\n  --eqx ~/.cache/equimo/dinov3/eqx_dinov3_vits16_mapped.eqx \\\n  --block-debug\n</code></pre> <p>This prints:</p> <ul> <li>Meta \u21d4 ONNX cosine / max\u2006|\u0394|</li> <li>Meta \u21d4 Equinox cosine / max\u2006|\u0394|</li> <li>EQX \u21d4 ONNX cosine / max\u2006|\u0394|</li> <li>For each transformer block, the max absolute difference per stage (input,   attention norms, LayerScale outputs, MLP, etc.) between the Equinox example   and Meta\u2019s original model.</li> </ul> <p>The block table highlights exactly where any remaining drift originates.</p>"},{"location":"user_guide/sota_examples/dinov3/compare_meta_vs_jax2onnx/#4-interpreting-results","title":"4. Interpreting results","text":"<ul> <li>Cosine values close to 1 and max\u2006|\u0394| in the 1e\u20116 band indicate parity.</li> <li>If Meta \u21d4 Eqx is clean but Meta \u21d4 ONNX drifts, regenerate the ONNX export.</li> <li>If Meta \u21d4 Eqx diverges, inspect the highest per-block stage in the debug   table\u2014it pinpoints the first tensor that needs correction.</li> </ul>"},{"location":"user_guide/sota_examples/dinov3/getting_weights/","title":"DINOv3 Weights Workflow","text":"<p>This guide walks through the \u201cfrom scratch\u201d path for bringing Meta\u2019s official DINOv3 checkpoints into the IR-only <code>jax2onnx</code> example, and how to obtain the final ONNX export. The architecture we ship is the Equimo project\u2019s clean-room Equinox/JAX reimplementation based on Meta AI\u2019s DINOv3 paper. Using Meta\u2019s pretrained weights is optional and subject to the DINOv3 license.</p> <p>All commands assume you are at the project root with the Poetry environment available.</p>"},{"location":"user_guide/sota_examples/dinov3/getting_weights/#related-examples","title":"Related Examples","text":"<p>For a full list of DINOv3 components and their export status, see the <code>DINOv3VisionTransformer</code>, <code>FlaxDINOv3VisionTransformer</code> and related entries in the Examples table.</p>"},{"location":"user_guide/sota_examples/dinov3/getting_weights/#1-fetch-metas-pytorch-checkpoint","title":"1. Fetch Meta\u2019s PyTorch checkpoint","text":"<p>Meta publishes DINOv3 weights on Hugging Face. The commands below download the <code>dinov3_vits16_pretrain_lvd1689m</code> checkpoint directly into the cache path that Equimo-based scripts expect.</p> <ol> <li>Visit https://huggingface.co/facebook/dinov3-vitb16-pretrain-lvd1689m,    accept the model\u2019s license and request access to the gated repository. Wait    for the approval email\u2014Meta sends presigned download links that remain valid    for a limited time.</li> <li>Use one of the approved links (they start with <code>https://dinov3.llamameta.net/...</code>)    to fetch the checkpoint directly, for example:</li> </ol> <p>```bash    mkdir -p ~/.cache/equimo/dinov3</p> <p>curl -L \"https://dinov3.llamameta.net/dinov3_vits16/dinov3_vits16_pretrain_lvd1689m-08c60483.pth?...Key-Pair-Id=...\" \\      -o ~/.cache/equimo/dinov3/dinov3_vits16_pretrain_lvd1689m-08c60483.pth    ```</p> <p>(Replace the URL above with the exact link from your email; <code>wget</code> works too.    If you prefer a browser download, save the file and move it into    <code>~/.cache/equimo/dinov3/</code>.)</p> <ol> <li>Verify the file is present:</li> </ol> <p><code>bash    ls ~/.cache/equimo/dinov3/dinov3_vits16_pretrain_lvd1689m-*.pth</code></p>"},{"location":"user_guide/sota_examples/dinov3/getting_weights/#2-convert-meta-equimo-pytorch-equinox","title":"2. Convert Meta \u2192 Equimo (PyTorch \u2192 Equinox)","text":"<p>Use the helper script to convert Meta\u2019s checkpoint into an Equinox archive that matches the Equimo layout.</p> <p>Prerequisite: install the optional tooling and make a local clone of Equimo\u2019s conversion utilities (the published wheel omits <code>models/dinov3.py</code>):</p> <p><code>bash poetry install --with test git clone https://github.com/clementpoiret/Equimo.git ~/.cache/equimo/repos/Equimo poetry run pip install -e ~/.cache/equimo/repos/Equimo poetry run pip install timm torchmetrics termcolor</code></p> <pre><code>poetry run python scripts/convert_dinov3_from_equimo.py \\\n  --variant dinov3_vits16_pretrain_lvd1689m \\\n  --weights ~/.cache/equimo/dinov3/dinov3_vits16_pretrain_lvd1689m-08c60483.pth \\\n  --output ~/.cache/equimo/dinov3/dinov3_vits16_pretrain_lvd1689m\n</code></pre> <p>This produces <code>~/.cache/equimo/dinov3/dinov3_vits16_pretrain_lvd1689m.tar.lz4</code>, the Equinox checkpoint used by the rest of the tooling.</p>"},{"location":"user_guide/sota_examples/dinov3/getting_weights/#3-map-equimo-weights-into-the-ir-only-example","title":"3. Map Equimo weights into the IR-only example","text":"<p>The IR-only DINO example lives under <code>jax2onnx/plugins/examples/eqx/dino.py</code>. Use the mapping script to copy the Equimo parameter tree into that example:</p> <pre><code>poetry run python scripts/map_equimo_dino_weights.py \\\n  --variant dinov3_vits16_pretrain_lvd1689m \\\n  --weights ~/.cache/equimo/dinov3/dinov3_vits16_pretrain_lvd1689m.tar.lz4 \\\n  --output  ~/.cache/equimo/dinov3/eqx_dinov3_vits16_mapped.eqx\n</code></pre> <p>By default the register/storage tokens are preserved so the example matches Meta/Equimo semantics.</p>"},{"location":"user_guide/sota_examples/dinov3/getting_weights/#4-export-onnx-from-the-mapped-example","title":"4. Export ONNX from the mapped example","text":"<p>Both static and dynamic-batch exports are available. For the static (B=1) variant:</p> <pre><code>poetry run python scripts/export_eqx_dino_example_with_mapped_weights.py \\\n  --eqx ~/.cache/equimo/dinov3/eqx_dinov3_vits16_mapped.eqx \\\n  --output ~/.cache/equimo/dinov3/eqx_dinov3_vit_S16.onnx \\\n  --img-size 224\n</code></pre> <p>For the dynamic-batch version, add <code>--dynamic-batch</code> to the command above. The resulting ONNX files can now be used for inference or comparison.</p>"},{"location":"user_guide/sota_examples/dinov3/getting_weights/#5-verify-against-metas-release","title":"5. Verify against Meta\u2019s release","text":"<p>Use the comparison helper to confirm the ONNX export matches Meta\u2019s PyTorch checkpoint (numerical drift should be on the order of 1e-6):</p> <pre><code>curl -L -o /tmp/coco_39769.jpg \\\n  http://images.cocodataset.org/val2017/000000039769.jpg\n\npoetry run python scripts/compare_meta_vs_jax2onnx.py \\\n  --image /tmp/coco_39769.jpg \\\n  --variant dinov3_vits16_pretrain_lvd1689m \\\n  --weights ~/.cache/torch/hub/dinov3/weights/dinov3_vits16_pretrain_lvd1689m-08c60483.pth \\\n  --onnx ~/.cache/equimo/dinov3/eqx_dinov3_vit_S16.onnx\n</code></pre> <p>Expect cosine similarity close to 1.0 and maximum absolute differences below <code>1e-5</code> for both CLS and pooled patch features.</p>"},{"location":"user_guide/sota_examples/dinov3/nnx_exports/","title":"Flax/NNX DINOv3 ONNX exports (local-only)","text":"<p>This note mirrors the Equinox DINO docs but for the Flax/NNX example stack (<code>jax2onnx/plugins/examples/nnx/dinov3.py</code>). Per the repo guardrail, <code>.onnx</code> / <code>.onnx.data</code> artifacts are not committed; generate them locally and keep them in your workspace (e.g., under <code>docs/onnx/examples/nnx_dino/</code> which is <code>.gitignore</code>d).</p>"},{"location":"user_guide/sota_examples/dinov3/nnx_exports/#quick-export-commands","title":"Quick export commands","text":"<p>Exports use random init (no pretrained weights). Adjust shapes/variants as needed.</p>"},{"location":"user_guide/sota_examples/dinov3/nnx_exports/#static-input","title":"Static input","text":"<pre><code>poetry run python - &lt;&lt;'PY'\nfrom flax import nnx\nfrom jax2onnx import to_onnx\nfrom jax2onnx.plugins.examples.nnx.dinov3 import VisionTransformer\n\nmodel = VisionTransformer(\n    img_size=224,\n    patch_size=14,\n    embed_dim=384,\n    depth=12,\n    num_heads=6,\n    num_storage_tokens=0,\n    rngs=nnx.Rngs(0),\n)\n\nto_onnx(\n    model,\n    input_shapes=[(1, 3, 224, 224)],\n    return_mode=\"file\",\n    output_path=\"docs/onnx/examples/nnx_dino/nnx_dinov3_vit_S14.onnx\",\n)\nPY\n</code></pre>"},{"location":"user_guide/sota_examples/dinov3/nnx_exports/#dynamic-batch","title":"Dynamic batch","text":"<pre><code>poetry run python - &lt;&lt;'PY'\nfrom flax import nnx\nfrom jax2onnx import to_onnx\nfrom jax2onnx.plugins.examples.nnx.dinov3 import VisionTransformer\n\nmodel = VisionTransformer(\n    img_size=224,\n    patch_size=14,\n    embed_dim=384,\n    depth=12,\n    num_heads=6,\n    num_storage_tokens=0,\n    rngs=nnx.Rngs(0),\n)\n\nto_onnx(\n    model,\n    input_shapes=[(\"B\", 3, 224, 224)],\n    return_mode=\"file\",\n    output_path=\"docs/onnx/examples/nnx_dino/nnx_dinov3_vit_S14_dynamic.onnx\",\n)\nPY\n</code></pre>"},{"location":"user_guide/sota_examples/dinov3/nnx_exports/#notes","title":"Notes","text":"<ul> <li>The example registry now keys by <code>context::component</code>, and the NNX DINO components use explicit names (<code>NnxDinoPatchEmbed</code>, <code>NnxDinoAttentionCore</code>, <code>NnxDinoAttention</code>, <code>NnxDinoBlock</code>, <code>FlaxDINOv3VisionTransformer</code>) so generated test files and ONNX artifacts stay unambiguous.</li> <li>To capture the submodules (PatchEmbed/Attention/Block) alongside the full ViT, run <code>poetry run pytest tests/examples/test_nnx_dino.py</code> and pick up the emitted models under <code>docs/onnx/examples/nnx_dino/</code>.</li> <li>Parity with the Equinox DINO example is covered by <code>tests/extra_tests/examples/test_nnx_dino_parity.py</code> (weight copy + forward check).</li> <li>Keep generated artifacts local; <code>docs/onnx/examples/nnx_dino/.gitignore</code> prevents accidental commits.</li> </ul>"},{"location":"user_guide/sota_examples/gpt_oss/getting_weights/","title":"GPT-OSS Weights Workflow","text":"<p>The GPT-OSS examples in this repo come in two flavors:</p> <ul> <li>Flax/NNX (<code>jax2onnx/plugins/examples/nnx/gpt_oss_flax.py</code>) \u2013 this is the   path backed by the routing parity harness, staged checkpoint exporter, and the   <code>FlaxTransformer</code> expect-graph tests.</li> <li>Equinox (<code>jax2onnx/plugins/examples/eqx/gpt_oss.py</code>) \u2013 kept for historical   comparison and still covered by the Equinox parity tests.</li> </ul> <p>Unless you specifically need the Equinox version, follow Sections 2\u20134 below for Flax/NNX. The Equinox workflow now lives in Sections 5\u20136.</p> <p>All commands assume you are at the project root with the Poetry environment available. The workflow targets CPU-only tools; feel free to switch the device flags to <code>cuda:X</code> if you have GPU support.</p>"},{"location":"user_guide/sota_examples/gpt_oss/getting_weights/#related-examples","title":"Related Examples","text":"<p>For detailed component exports (like <code>GPT-OSS Attention</code>, <code>MLP</code>, <code>RMSNorm</code>), see the <code>GPT-OSS</code> entries in the Examples table.</p>"},{"location":"user_guide/sota_examples/gpt_oss/getting_weights/#1-download-a-gpt-oss-checkpoint","title":"1. Download a GPT-OSS checkpoint","text":"<p>OpenAI publishes GPT-OSS on Hugging Face under both 20B and 120B variants. The commands below fetch the original shard layout expected by <code>gpt_oss.torch.model.Transformer.from_checkpoint</code>.</p> <pre><code>mkdir -p ~/.cache/gpt_oss/gpt-oss-20b\npoetry run huggingface-cli download openai/gpt-oss-20b original \\\n  --repo-type model \\\n  --local-dir ~/.cache/gpt_oss/gpt-oss-20b \\\n  --local-dir-use-symlinks False\n</code></pre> <p><code>huggingface_hub</code> \u22651.0 no longer installs the <code>huggingface-cli</code> entry point by default, and recent releases ignore <code>--local-dir-use-symlinks</code>. If the command above fails with \u201ccommand not found\u201d or you prefer to stay within Python, download the checkpoint folder directly via <code>snapshot_download</code>:</p> <pre><code>poetry run python - &lt;&lt;'PY'\nfrom pathlib import Path\n\nfrom huggingface_hub import snapshot_download\n\nsnapshot_download(\n    repo_id=\"openai/gpt-oss-20b\",\n    repo_type=\"model\",\n    allow_patterns=[\"original/*\"],\n    local_dir=Path(\"~/.cache/gpt_oss/gpt-oss-20b\").expanduser(),\n)\nPY\n</code></pre> <p>This grabs the <code>original/</code> shard set expected by <code>Transformer.from_checkpoint</code>. Omit <code>allow_patterns</code> if you want the full repo contents (tokenizer, chat template, etc.).</p> <p>After the download finishes you should have a directory containing <code>config.json</code> and a set of <code>*.safetensors</code> shards.</p>"},{"location":"user_guide/sota_examples/gpt_oss/getting_weights/#2-stage-gpt-oss-weights-for-flaxnnx","title":"2. Stage GPT-OSS weights for Flax/NNX","text":"<p>Run the staging helper to materialize a Flax <code>.msgpack</code> bundle plus a matching <code>config.json</code>. The exporter and expect-graph tests consume this format directly.</p> <pre><code>poetry run python scripts/export_flax_gpt_oss_params.py \\\n  --checkpoint ~/.cache/gpt_oss/gpt-oss-20b/original \\\n  --output ~/.cache/gpt_oss/gpt-oss-20b/flax_params.msgpack\n</code></pre> <p>Use <code>--gpt-oss-path</code> if the helper repo lives somewhere other than the default <code>tmp/gpt-oss-jax-vs-torch-numerical-comparison</code>. The script automatically detects Orbax vs. SafeTensors checkpoints and writes <code>flax_params.msgpack.config.json</code> beside the serialized parameters.</p>"},{"location":"user_guide/sota_examples/gpt_oss/getting_weights/#3-export-the-flaxnnx-transformer-to-onnx","title":"3. Export the Flax/NNX transformer to ONNX","text":"<p>With staged params in place, call the ONNX exporter. It instantiates the <code>examples.nnx_gpt_oss.FlaxTransformer</code> module, loads the staged parameters via <code>nnx.Param</code> assignments, and traces the full embedding \u2192 blocks \u2192 norm \u2192 head pipeline.</p> <pre><code>poetry run python scripts/export_flax_gpt_oss_to_onnx.py \\\n  --params ~/.cache/gpt_oss/gpt-oss-20b/flax_params.msgpack \\\n  --output artifacts/gpt_oss_flax.onnx \\\n  --sequence-length 256\n</code></pre> <p>Notes:</p> <ul> <li><code>--sequence-length</code> controls both the tracing inputs and the rotary/mask   tables. Start small (e.g. 128) while verifying the workflow, then bump the   length to your deployment target.</li> <li>Pass <code>--config /path/to/config.json</code> if the staging script\u2019s JSON lives   elsewhere.</li> <li>The exporter mirrors the exact callable covered by   <code>tests/examples/test_nnx_gpt_oss.py::Test_FlaxTransformer</code>. Run that test (or   the whole file) to sanity-check ONNX numeric validation locally:</li> </ul> <p><code>bash   poetry run pytest tests/examples/test_nnx_gpt_oss.py::Test_FlaxTransformer -q</code></p> <p>Tip: When iterating on the exporter it can be helpful to trim the staged checkpoint down to a couple of layers. The snippet below keeps only the first two Transformer blocks while preserving the original hidden size/head layout, producing a much smaller bundle that exports quickly:</p> <p>```bash poetry run python - &lt;&lt;'PY' from pathlib import Path import json import flax.serialization as serialization</p> <p>root = Path(\"~/.cache/gpt_oss/gpt-oss-20b\").expanduser() params = serialization.msgpack_restore((root / \"flax_params.msgpack\").read_bytes()) keep = {k: params[k] for k in [\"embedding\", \"norm\", \"unembedding\"]} for idx in range(2):     keep[f\"block_{idx}\"] = params[f\"block_{idx}\"] (root / \"flax_params_2layers.msgpack\").write_bytes(serialization.to_bytes(keep))</p> <p>config = json.loads((root / \"flax_params.config.json\").read_text()) config[\"num_hidden_layers\"] = 2 (root / \"flax_params_2layers.config.json\").write_text(json.dumps(config, indent=2)) print(\"Wrote 2-layer bundle under\", root) PY ```</p> <p>Export with <code>--params .../flax_params_2layers.msgpack --config .../flax_params_2layers.config.json</code> to keep traces under a few minutes.</p>"},{"location":"user_guide/sota_examples/gpt_oss/getting_weights/#4-flaxnnx-routing-parity-harness","title":"4. Flax/NNX routing parity harness","text":"<p>The parity harness from PR #217 verifies that the staged Flax/NNX model makes identical expert choices to the PyTorch reference. There is an optional slow smoke test in <code>tests/extra_tests/test_flax_routing_parity.py</code> that runs the harness with <code>--max-layers 4 --max-tokens 2</code> on CPU whenever checkpoints are present.</p> <p>To run the harness manually (e.g. with longer prompts or more layers):</p> <pre><code>JAX_PLATFORM_NAME=cpu poetry run python scripts/gpt_oss_routing_parity.py \\\n  --gpt-oss-path tmp/gpt-oss-jax-vs-torch-numerical-comparison \\\n  --jax-checkpoint ~/.cache/gpt_oss/gpt-oss-20b/original \\\n  --torch-checkpoint ~/.cache/gpt_oss/gpt-oss-20b/original \\\n  --prompt \"What is the capital of France?\" \\\n  --max-layers 24 \\\n  --max-tokens 4 \\\n  --torch-device cpu \\\n  --output-dir artifacts/gpt_oss_routing/flax\n</code></pre> <p>The harness writes <code>artifacts/gpt_oss_routing/flax/&lt;timestamp&gt;_summary.md</code> containing per-layer match rates and gate diffs. Adjust <code>--max-layers</code> and <code>--max-tokens</code> to keep runs developer-friendly, and prefer <code>--torch-device cpu</code> to avoid CUDA OOMs during PyTorch checkpoint loading.</p>"},{"location":"user_guide/sota_examples/gpt_oss/getting_weights/#baseline-parity-snapshot-baseline5-2-layer-slice","title":"Baseline parity snapshot (Baseline5, 2-layer slice)","text":"<ul> <li>Instrumentation: <code>export_flax_gpt_oss_to_onnx.py</code> supports <code>--emit-hidden-states</code> and <code>--emit-block-debug</code>; <code>run_flax_gpt_oss_onnx.py</code> compares hidden states and block-debug tensors (attention I/O, MoE norms/gates/experts, fused outputs).</li> <li>Parity (2-layer checkpoint, seq_len=32, debug export): logits max |\u0394| \u2248 1.9e-05; hidden states \u2264 1.5e-04; MoE debug tensors \u2264 4.5e-04.</li> <li>Routing evidence: <code>scripts/gpt_oss_routing_parity.py</code> captures both a 2-layer slice (exact match) and a full 24-layer run (22/24 layers match; remaining layers differ by \u2264 4e-03 gate deltas). See <code>docs/onnx/examples/nnx_gpt_oss/baseline5_parity.md</code>.</li> <li>Artifacts: debug export at <code>/tmp/gpt_oss_transformer_flax_debug.onnx</code> (with <code>.data</code>). The committed Baseline5 artifact lives under <code>docs/onnx/examples/nnx_gpt_oss/</code> with external data named <code>gpt_oss_transformer_flax_baseline5.onnx.data</code>.</li> </ul>"},{"location":"user_guide/sota_examples/gpt_oss/getting_weights/#reproduce-the-baseline5-debug-export-and-parity-check","title":"Reproduce the Baseline5 debug export and parity check","text":"<pre><code>JAX_PLATFORM_NAME=cpu ORT_LOG_SEVERITY_LEVEL=4 poetry run python scripts/export_flax_gpt_oss_to_onnx.py \\\n  --params ~/.cache/gpt_oss/gpt-oss-20b/flax_params_2layers.msgpack \\\n  --config ~/.cache/gpt_oss/gpt-oss-20b/flax_params_2layers.config.json \\\n  --output /tmp/gpt_oss_transformer_flax_debug.onnx \\\n  --sequence-length 16 \\\n  --emit-hidden-states \\\n  --emit-block-debug \\\n  --skip-validation\n\nJAX_PLATFORM_NAME=cpu ORT_LOG_SEVERITY_LEVEL=4 poetry run python scripts/run_flax_gpt_oss_onnx.py \\\n  --prompt \"What is the capital of France?\" \\\n  --params ~/.cache/gpt_oss/gpt-oss-20b/flax_params_2layers.msgpack \\\n  --config ~/.cache/gpt_oss/gpt-oss-20b/flax_params_2layers.config.json \\\n  --onnx /tmp/gpt_oss_transformer_flax_debug.onnx \\\n  --sequence-length 16 \\\n  --compare-hidden-states \\\n  --compare-block-debug\n</code></pre>"},{"location":"user_guide/sota_examples/gpt_oss/getting_weights/#original-torch-flax-parity-checklist","title":"Original (Torch) \u2194 Flax parity checklist","text":"<p>Prove the staged Flax bundle matches the Torch checkpoint before shipping ONNX:</p> <pre><code>JAX_PLATFORM_NAME=cpu \\\npoetry run python scripts/probe_flax_gpt_oss_parity.py \\\n  --prompt \"France capital? Answer:\" \\\n  --params ~/.cache/gpt_oss/gpt-oss-20b/flax_params_2layers.msgpack \\\n  --config ~/.cache/gpt_oss/gpt-oss-20b/flax_params_2layers.config.json \\\n  --torch-checkpoint ~/.cache/gpt_oss/gpt-oss-20b/original \\\n  --sequence-length 16 \\\n  --gpt-oss-path tmp/gpt-oss-jax-vs-torch-numerical-comparison \\\n  --torch-device cpu \\\n  --torch-max-layers 2\n</code></pre> <p>The script tokenizes the prompt, runs both frameworks, and reports logits/stage tensor deltas. Store the transcript next to the promoted ONNX (e.g., <code>docs/onnx/examples/nnx_gpt_oss/baseline5_parity.md</code>).</p>"},{"location":"user_guide/sota_examples/gpt_oss/getting_weights/#onnx-only-smoke-test-tokenizer-generation","title":"ONNX-only smoke test (tokenizer + generation)","text":"<p>For a short-window ONNX-only run (no JAX/Torch in memory):</p> <pre><code>JAX_PLATFORM_NAME=cpu \\\npoetry run python scripts/export_flax_gpt_oss_to_onnx.py \\\n  --params ~/.cache/gpt_oss/gpt-oss-20b/flax_params.msgpack \\\n  --config ~/.cache/gpt_oss/gpt-oss-20b/flax_params.config.json \\\n  --output /tmp/gpt_oss_transformer_flax_seq16.onnx \\\n  --sequence-length 16 \\\n  --skip-validation\n\nmkdir -p artifacts/gpt_oss_full_seq16\nmv /tmp/gpt_oss_transformer_flax_seq16.onnx artifacts/gpt_oss_full_seq16/\nmv /tmp/gpt_oss_transformer_flax_seq16.onnx.data artifacts/gpt_oss_full_seq16/\n\nLD_PRELOAD=/usr/lib/x86_64-linux-gnu/libjemalloc.so.2 \\\npoetry run python scripts/run_onnx_only.py \\\n  --onnx artifacts/gpt_oss_full_seq16/gpt_oss_transformer_flax_seq16.onnx \\\n  --config ~/.cache/gpt_oss/gpt-oss-20b/flax_params.config.json \\\n  --prompt \"France capital? Answer:\" \\\n  --sequence-length 16 \\\n  --generate-steps 8 \\\n  --expand-functions \\\n  --runtime ort\n</code></pre> <p>For longer prompts/responses, re-export with a larger <code>--sequence-length</code> or add KV cache support to avoid re-running the full window.</p>"},{"location":"user_guide/sota_examples/gpt_oss/getting_weights/#5-legacy-export-the-equinox-example-to-onnx","title":"5. (Legacy) Export the Equinox example to ONNX","text":"<p>Use the helper script to load the checkpoint, mirror it into the IR-only Equinox modules, and emit an ONNX graph. The script preserves the exact callable used by our tests so structural expectations continue to hold. On memory constrained systems it helps to run the export in two stages:</p> <ol> <li>Stage the Equinox weights (reads SafeTensors \u2192 writes <code>.eqx</code>, no ONNX yet):</li> </ol> <p><code>bash    poetry run python scripts/export_eqx_gpt_oss_example_with_mapped_weights.py \\      --checkpoint ~/.cache/gpt_oss/gpt-oss-20b/original \\      --save-eqx ~/.cache/gpt_oss/gpt-oss-20b/eqx_gpt_oss_transformer.eqx \\      --seq-len 256 \\      --dynamic-b \\      --skip-onnx</code></p> <ol> <li>Convert the cached Equinox model to ONNX (no PyTorch in memory):</li> </ol> <pre><code>poetry run python scripts/export_eqx_gpt_oss_example_with_mapped_weights.py \\\n  --eqx ~/.cache/gpt_oss/gpt-oss-20b/eqx_gpt_oss_transformer.eqx \\\n  --output ~/.cache/gpt_oss/gpt-oss-20b/eqx_gpt_oss_transformer.onnx \\\n  --seq-len 256 \\\n  --dynamic-b \n</code></pre> <ul> <li><code>--dynamic-b</code> emits a symbolic batch axis (<code>B</code>) that matches the example tests.</li> <li>Omit <code>--dynamic-b</code> and/or add <code>--dynamic-seq</code> to tailor the exported shapes.</li> <li><code>--save-eqx</code> keeps the mapped Equinox parameters around for future exports.</li> <li>Pass a higher <code>--seq-len</code> (e.g. 512) once the 256-token run succeeds; longer   sequences raise memory pressure while tracing the attention blocks.</li> </ul>"},{"location":"user_guide/sota_examples/gpt_oss/getting_weights/#6-legacy-validate-equinox-parity-optional","title":"6. (Legacy) Validate Equinox parity (optional)","text":"<p>Numerical comparisons between the PyTorch and ONNX/JAX paths are covered by the regression tests in <code>tests/extra_tests/test_eqx_gpt_oss_parity.py</code>. When the optional dependencies above are installed, this test asserts the Equinox model tracks the PyTorch reference to within a small tolerance (absolute differences stay below <code>~1e0</code> when working in bfloat16).</p> <p>Run a focused check with:</p> <pre><code>poetry run pytest -q tests/extra_tests/test_eqx_gpt_oss_parity.py\n</code></pre>"}]}